---
title: "The Conjugate Gradient Method for Linear Equations"
date: 2026-01-11
slug: conj-gradient
draft: false
katex: true
description: "Using linear algebra to improve upon gradient-descent for solving linear equations"
tags: ["optimization", "linear-equations", "gradient-descent"]
categories: ["Optimization", "Gradient Descent"]
---
> [!info] Prerequisites
> *   **Linear Algebra:** Eigenvalues, eigenvectors, positive (semi-)definite (PSD) matrices, and the notion of orthogonality.
> *   **Calculus:** Gradients and basic convexity.
> *   **Optimization:** Gradient Descent (GD) basics.

One of the most common tasks in numerical algorithms is to solve a linear equationâ€”that is, find $x$ for which
$$Ax=b$$
for a given matrix $A$ and vector $b$. This can be solved via Gaussian elimination, which generally has a high runtime ($O(n^3)$). We will show how to improve upon this using optimization ideas. This is one instance of a problem for which we can find an **approximate** solution much faster using **calculus** tools, compared with using a close-form exact algebraic solution.

We will make the following assumption: $A$ is **symmetric** and **positive definite**. This means $A$ has eigenvalues $0<\lambda_{1}\le \ldots \le \lambda_{n}$. In particular, $A$ is square, $A\in \mathbb{R}^{n\times n}$, and $x,b\in\mathbb{R}^{n}$.

## As an Optimization Problem

Solving $Ax=b$ is equivalent to minimizing the "distance" $Ax-b$. However, $Ax-b$ is a vector, so it cannot be minimized directly. To transform $Ax-b$ into a meaningful scalar, several approaches are possible. We could minimize $\left\lVert Ax-b \right\rVert_{2}$ (the **least-squares** problem), or any other norm for that matter. Instead, we choose to take the inner product with $x$, leading to the quadratic form:
$$f(x):=\frac{1}{2}x^{\top}Ax -x^{\top}b.$$
The gradient of $f$ is $\nabla f(x)=Ax-b$, while the Hessian matrix is $\nabla^2 f(x)=A$. It is a general fact that **quadratic** functions with a **positive-definite** Hessian matrix are strongly convex. We thus conclude:

> [!important] Lemma
> $x$ is a minimizer of $f$ if and only if $Ax=b$. Moreover, $f$ is strongly convex, so it has a unique global minimum, $x^{\*}$.

**Proof.**
As noted above, $\nabla f(x)=Ax-b$; hence $Ax=b$ iff $\nabla f(x)=0$. We know $\nabla f(x)=0$ is a necessary condition for $x$ to be a minimizer. However, it is also sufficient because $\nabla^{2}f=A$ is **positive definite**, implying $\nabla f$ can have at most a **single** root. That root must be a minimizer, as the curvature of $f$ implies there are no saddle points. $\blacksquare$

Since $f$ is convex, we can use a general solver, like Gradient Descent (GD), to find a minimizer. GD requires computing gradients, which in our case is just $\nabla f(x)=Ax-b$. Therefore, each GD iteration requires time $T_{A}$, which is the time needed to compute the **matrix-vector** product $Ax$ (this dominates the computation). Hopefully, if the number of iterations is small, we need a small number of such products. Put explicitly, each iteration in the GD method is:
$$x_{t+1}=x_{t}- \eta_{t}\cdot \nabla f(x_{t})$$
where $\eta_{t}$ is called the step size at time $t$. The following theorem is a general result about the rate of convergence for gradient descent:

> [!tip] Theorem
> With a proper choice of step sizes, for every $\varepsilon>0$, the GD algorithm finds a vector $x$ such that
> $$\sqrt{(x-A^{-1}b)^{\top}A(x-A^{-1}b)}\le \varepsilon \sqrt{(A^{-1}b)^{\top}A(A^{-1}b)},$$
> in time $O(T_{A}\cdot \kappa(A)\cdot \log (1/\varepsilon)))$, where $\kappa(A)=\frac{\lambda_{n}}{\lambda_{1}}$ is called the **condition number** of $A$.

To understand the meaning of the bounds, note that $A^{-1}b=x^\*$, and so $x-A^{-1}b=x-x^\*$, i.e., the difference between the solution and the optimal solution. Moreover, note that $$
\begin{aligned}
(x-A^{-1}b)^{\top}A(x-A^{-1}b)&= x^{\top} Ax - x^{\top}AA^{-1}b - b^{\top}(A^{-1})^{\top}Ax + b^{\top}(A^{-1})^{\top}AA^{-1}b 
\\\\ &= x^{\top}Ax- 2x^{\top} b+ b^{\top}x^\* \\\\
&= 2f(x)-2f(x^\*)
\end{aligned}
$$
where we used the fact $A^{-1}$ must also by symmetric and that $f(x^\*)=-\frac{1}{2}b^{\top}x^{\*}$. In other words, the error bound in the theorem is just (divided by $2$): $$(f(x)-f(x^\*))\le \varepsilon^2\cdot f(x^\*)$$
A proof can be found in [[5]](#references).

## Viewing GD with Krylov Subspaces
}\_
> [!caution] Definition
> Let $M$ denote a matrix of size $n\times n$ and $y$ a vector of size $n$. Then the **Krylov subspace** of order $t$ generated by $M,y$ is
> $$\mathrm{Span}\set{y,\ My,\ \ldots ,M^{t-1}y}.$$
> In other words, we repeatedly apply $M$ to obtain a sequence of $t$ vectors starting from $y$.

Suppose we initiate $x_{0}=0$. Then
$$\nabla f(x_{0})=Ax_{0}-b=-b.$$
Consequently,
$$x_{1}=x_{0}-\eta_{0}\nabla f(x_{0})=\eta_{0}b,$$
and similarly,
$$x_{2}=x_{1}-\eta_{1}(Ax_{1}-b)=\eta_{0}\cdot b-\eta_{1}\eta_{0}\cdot Ab+\eta_{1}\cdot b=(\eta_{0}+\eta_{1})b-\eta_{1}\eta_{0} Ab.$$
Therefore, a simple inductive argument shows that
$$x_{t}\in \mathrm{Span}\set{b,Ab,\ldots ,A^{t-1}b}.$$
Denote the $t$-th order Krylov subspace for $A,b$ by $\mathcal{K}\_{t}$. Then we see GD moves $x_{1},\ldots,x_{t}$ in $\mathcal{K}\_{1},\ldots,\mathcal{K}\_{t}$. In other words, $x_i\in \mathcal{K}\_i$. 

However, GD does not guarantee that $x_{t}$ is the minimizer of $f$ in $\mathcal{K}\_{t}$. The idea of the **Conjugate Gradient (CG)** method is, at each step, to minimize over $\mathcal{K}\_{t}$. In particular, $x^{\*}$, the true minimizer, is the minimizer over $\mathcal{K}\_{n}$.

## Finding the Minimizer in a Krylov Subspace

**Question.** How can we quickly find the minimizer of $f$ in $\mathcal{K}\_{t}$?

To answer this question, we start with the following observation:
> [!important] Observation
> We can assume without loss of generality that $\dim \mathcal{K}\_{t}=t$. Otherwise, suppose $\dim \mathcal{K}\_{t}=t-1$; then $A^{t-1}b$ is linearly dependent on $\mathcal{K}\_{t-1}$, which means $A^{k}b$ is also linearly dependent on $\mathcal{K}\_{t-1}$ for every $k\ge t$. In particular, $\mathcal{K}\_{t-1}=\mathcal{K}\_{n}$, and so by minimizing over $\mathcal{K}\_{t-1}$ we are done.

Suppose we were given a basis $\set{p_{0},\ldots,p_{t-1}}$ for $\mathcal{K}\_{t}$, with the following **separability property**:
$$f\left(\sum_{i=0}^{t-1} \beta_{i}p_{i}\right)=\sum_{i=0}^{t-1}f(\beta_{i}p_{i}),$$
for every $\beta_{0},\ldots,\beta_{t-1}$. Then, to minimize $f$ over $\mathcal{K}\_{t}$, it suffices to find $\alpha_{i}$ for all $i$, where $\alpha_i$ is the scalar minimizing $f(\alpha p_{i})$. In particular, when moving from $\mathcal{K}\_{t-1}$ to $\mathcal{K}\_{t}$, to minimize $f$ over $\mathcal{K}\_{t}$ we only need to add $\alpha_{t-1}p_{t-1}$ to the previous minimizer.

This separability property clearly holds for linear functions, so for our objective function $f(x)=\frac{1}{2}x^{\top}Ax+b^{\top}x$ (ignoring the constant), only the quadratic part $x^{\top}Ax$ might be the problem. Note that the left hand side is
$$\left(\sum_{i=0}^{t-1}\beta_{i}p_{i}\right)^{\top}A\left(\sum_{i=0}^{t-1}\beta_{i}p_{i}\right)=\sum_{0\le i,j\le t-1}\beta_{i}\beta_{j} p_{i}^{\top}Ap_{j}$$
while the right hand side is $\sum_{i=0}^{t-1}(\beta_{i}p_{i})^{\top}A(\beta_{i}p_{i})=\sum_{i} \beta_{i}^{2}p_{i}^{\top}Ap_{i}$. Hence, for equality to hold, we need the basis $\set{p_{0},\ldots,p_{t-1}}$ to be **$A$-orthonormal**:

> [!caution] Definition
> Given a symmetric matrix $A$, a set of vectors $v_{1},\ldots,v_{t}$ is **$A$-orthonormal** (or $A$-conjugate) if
> $$v_{i}^{\top}Av_{j}=0\quad \forall i\not=j.$$

### Computing an $A$-Orthonormal Basis
#### Gram-Schmidt Iterations

We wish to iteratively compute an $A$-orthonormal basis $\set{p_{0},\ldots,p_{t-1}}$ for $\mathcal{K}\_{t}$. One way to do this is via **Gram-Schmidt** iterations:
1.  Given a new vector $v\notin \mathcal{K}\_{t}$ (specifically $v=A^{t}b$),
2.  Define
    $$p_{t}=v-\sum_{i\le t-1}\frac{v^{\top}Ap_{i}}{p_{i}^{\top}Ap_{i}}p_{i}.$$

Since $p_{i}^{\top}Ap_{j}=0$ when $i\not=j$, we obtain that
$$p_{t}^{\top}Ap_{j}=v^{\top}A p_{j}-\sum_{i\le t-1}\frac{v^{\top}Ap_{i}}{p_{i}^{\top}Ap_{i}}p_{i}^{\top}Ap_{j}=v^{\top}Ap_{j}-\frac{v^{\top}Ap_{j}}{\cancel{p_{j}^{\top}Ap_{j}}}\cdot \cancel{ p_{j}^{\top}Ap_{j}}=0$$
for every $j\le t-1$. However, to compute $p_{t}$ in this manner we need $O(t)$ matrix-vector products. Moreover, we haven't really used the fact that we are computing a basis for the Krylov subspace generated by $A$, which gives us extra structure to work with.

#### A Slight Adjustment
**The ida**: If we start with $p_0=b$ and choose $v$ to be $Ap_{t-1}$, the Gram-Schmidt iterations simplify and are faster to compute, involving only a *constant* number of matrix-vector products.

Formally, let $p_{0}=b$ be the first vector, and assume we have constructed an $A$-orthonormal basis for $\mathcal{K}\_{t}$, denoted by $\set{p_{0},\ldots,p_{t-1}}$, where the induction hypothesis states
$$\mathcal{K}\_{i}=\mathrm{Span}\set{p_{0},\ldots,p_{i-1}}$$
for every $i=1,\ldots,t-1$. In particular, $Ap_{i-1}\in \mathcal{K}\_{i+1}$. We want to construct a vector $p_{t}$ so that the statement above is true for $i=t+1$.

As discussed in the observation above, if $Ap_{t-1}\in \mathcal{K}\_{t}$, then $\mathcal{K}\_{t}=\mathcal{K}\_{n}$ and so we have reached the end of the iteration. Therefore we may assume without loss of generality that $Ap_{t-1}\notin \mathcal{K}\_{t-1}$. Define $$p_{t}=A p_{t-1}-\sum_{i\le t-1}\frac{(Ap_{t-1})^{\top} Ap_{i}}{p_{i}^{\top}Ap_{i}}p_{i}.$$

By definition, $Ap_{t-1}\in \mathcal{K}\_{t+1}$, and also $\mathcal{K}\_{t+1}=\mathrm{Span}\set{p_{0},\ldots,p_{t}}$ (using the induction hypothesis). By the induction hypothesis, for every $0\le j<t-1$ it holds that $Ap_{j}\in \mathcal{K}\_{j+2}$. This implies $Ap_j$ is a linear combination of $p_{0},\ldots,p_{j+1}$, and in particular there are scalars $c_{0},\ldots,c_{j+1}$ such that $Ap_{j}=\sum_{i=0}^{j+1}c_{i}p_{i}$. Hence:
$$(Ap_{t-1})^{\top}Ap_{j}=p_{t-1}A^{\top}Ap_{j}=p_{t-1}A^{\top}\left(\sum_{i=0}^{j+1}c_{i}p_{i}\right)=\sum_{i=0}^{j+1}c_{i}\cdot p_{t-1}^{\top}Ap_{i}.$$
Due to $A$-orthogonality, $p_{t-1}^{\top}Ap_{i} = 0$ unless $i = t-1$. Thus:
$$(Ap_{t-1})^{\top}Ap_{j} = \begin{cases}
0 & j<t-2, \\\\
c_{t-1}p_{t-1}^{\top}Ap_{t-1} & j=t-2.
\end{cases}$$
Therefore, the definition of $p_{t}$ is essentially a sum of **three** elements only. In other words, the definition simplifies to a three-term recurrence:
$$p_{t}=Ap_{t-1}-\frac{p_{t-1}^{\top}A^{2}p_{t-1}}{p_{t-1}^{\top}Ap_{t-1}}p_{t-1}-\frac{p_{t-1}^{\top}A^{2}p_{t-2}}{p_{t-2}^{\top}Ap_{t-2}}p_{t-2},$$
which implies we need a **constant** number of matrix-vector products to compute the next element of the basis.

### The Conjugate Gradient Algorithm

> [!caution] Algorithm: Conjugate Gradient
> *   **Input:** A symmetric, positive definite matrix $A$, vector $b$, and iteration count $T$.
> *   **Output:** A vector $x_{T}$ obtained after $T$ iterations.
>
> 1.  Set $r_{0}\gets b$, and $p_{0}\gets b$.
> 2.  Set $x_0 \gets 0$.
> 3.  For $t=0,\ldots,T-1$:
>     1.  Set $\alpha_{t}=\frac{r_{t}^{\top}p_{t}}{p_{t}^{\top}Ap_{t}}$.
>     2.  Set $x_{t+1}=x_{t}+\alpha_{t}p_{t}$.
>     3.  Set $r_{t+1}=b-Ax_{t+1}$.
>     4.  Compute $p_{t+1}$ to be $A$-orthogonal to $p_t$ and $p_{t-1}$ using the recurrence derived above.
> 4.  Return $x_{T}$.

The choice of $\alpha_{t}$ is done to minimize $f(\alpha p_{t})=\frac{1}{2}\alpha^{2}p_{t}^{\top}Ap_{t}- \alpha b^{\top}p_{t}$, viewed as a function $\mathbb{R}\to \mathbb{R}$. The number of iterations needed for a certain prescribed precision is therefore dependent on the convergence of the conjugate gradient algorithm (which is always better than the gradient descent algorithm).

## Polynomial Minimization

Let $x^{\*}$ denote the minimizer, satisfying $Ax^{\*}=b$. Then
$$f(x^{\*})=\frac{1}{2}(x^{\*})^{\top}Ax^{\*}-(x^{\*})^{\top}Ax^{\*}=-\frac{1}{2}(x^{\*})^{\top}Ax^{\*}.$$
Thus at iteration $t$ we have
$$f(x_{t})-f(x^{\*})=\frac{1}{2}(x_{t}^{\top}Ax_{t}+(x^{\*})^{\top}Ax^{\*}-2x_{t}^{\top}Ax^{\*})=\frac{1}{2}(x_{t}-x^{\*})^{\top}A(x_{t}-x^{\*}),$$
using the **symmetry** of $A$. This is the derivation as in the error of the gradient descent theorem.

> [!caution] Definition
> Define the $A$-norm as $\left\lVert v \right\rVert_{A}=\sqrt{v^{\top}Av}$. It is a well-defined norm because $A$ is symmetric and positive definite.

> [!important] Observation
> Let $x_t\in \mathcal{K}\_{t}$; then it can be written as the linear combination $\gamma_{0},\ldots,\gamma_{t-1}$ of $b,Ab,\ldots,A^{t-1}b$, i.e., $x_{t}=\sum_{i=0}^{t-1}\gamma_{i}A^{i}b$. Define $p(X)=\sum_{i=0}^{t-1}\gamma_{i}X^{i}$. Then
> $$x_t=p(A)b=p(A)(Ax^{\*}).$$
> Hence
> $$x_t-x^{\*}=p(A)(Ax^{\*})-x^{\*}=(p(A)A-I)x^{\*}.$$
> This establishes a **bijection** between points in $\mathcal{K}\_{t}$ and degree $t-1$ polynomials.

We note that evaluating a polynomial at a matrix gives a new matrix which is the weighted sum of matrix powers. Here $A^{0}=I$.

Let $q(X)=p(X)\cdot X-1$. Then $q(A)=(p(A)A-I)$ is a degree $t$ polynomial. Moreover, there is a **bijection** between
$$\mathcal{P}\_{t} =\set{P(X)\mid \deg P\le t-1}\leftrightarrow \set{Q(X)\mid \deg Q\le t, Q(0)=-1}=\mathcal{Q}\_{t}.$$
This bijection is given by the transformation of $P\mapsto Q$, i.e., a polynomial $P$ is mapped to $Q(X)=X\cdot P(X)-1$.

> [!important] Observation
> We know $x_{t}$ is the minimizer of $f(x)$ over $\mathcal{K}\_{t}$, hence it minimizes $f(x)-f(x^{\*})$ over $\mathcal{K}\_{t}$. Thus,
> $$\frac{1}{2}\left\lVert x_{t}-x^{\*} \right\rVert_{A}^{2}=\min_{x\in \mathcal{K}\_{t}}\frac{1}{2}\left\lVert x -x^{\*} \right\rVert_{A}^{2}=\min_{p(X)\in \mathcal{P}\_{t}}\frac{1}{2}\left\lVert (p(A)A-I)x^{\*} \right\rVert_{A}^{2}=\min_{q(X)\in \mathcal{Q}\_{t}}\frac{1}{2} \left\lVert q(A)x^{\*} \right\rVert_{A}^{2}.$$

Another way to view polynomials of matrices is through their diagonalization. Since $A$ is symmetric, it is orthogonally diagonalizable, meaning that $A=U \Lambda U^{\top}$ for a diagonal matrix $\Lambda$ (with the eigenvalues on the diagonal) and $U$ an orthogonal matrix ($UU^{\top}=U^{\top}U=I$). In this case,
$$A^{2}=(U\Lambda U^{\top})^{2}=U \Lambda U^{\top}U\Lambda U^{\top}=U \Lambda^{2}U^{\top}.$$
Since $\Lambda$ is diagonal with $\set{\lambda_{1},\ldots,\lambda_{n}}$, $\Lambda^{2}$ is just the element-wise product of the diagonals. By induction, this is true for every $k$. Therefore,
$$p(A)=\sum_{i}\gamma_{i}A^{i}=\sum_{i}\gamma_{i}U \mathrm{diag}(\lambda_{1}^{i},\ldots,\lambda_{n}^{i}) U^{\top}=U\left(\sum_{i}\gamma_{i} \mathrm{diag}(\lambda_{1}^{i},\ldots, \lambda_{n}^{i})\right)U^{\top}$$
which is equal to $U \mathrm{diag}(p(\lambda_{1}),\ldots,p(\lambda_{n}))U^\top$. In words, we apply the polynomial on the eigenvalues to obtain a new diagonal matrix.

> [!important] Lemma
> For a symmetric matrix $M$ with non-negative eigenvalues $\lambda_{1},\ldots,\lambda_{n}$, and any polynomial $p(X)$ and vector $v$, it holds that
> $$(p(M)v)^{\top}M (p(M)v)\le v^{\top}Mv\cdot \max_{i}\left|p(\lambda_{i})\right|^{2}.$$

**Proof.**
Write $M=U\Lambda U^{\top}$ as above. Note that diagonal matrices commute, hence
$$v^{\top}(p(M))^{\top}Mp(M)v=v^{\top}Up(\Lambda)U^{\top}U\Lambda U^{\top}Up(\Lambda)U^{\top}v=v^{\top}U(\Lambda\cdot p^{2}(\Lambda ))U^{\top}v.$$
Writing $v=\sum_{i}c_{i}u_{i}$, where $u_{i}$ are the columns of $U$ (the eigenvectors basis), we have
$$v^{\top}p(M)^{\top}Mp(M)v=\sum_{i,j}c_{i}c_{j}u_{i}^{\top}U(\Lambda\cdot p^{2}(\Lambda))U^{\top}u_{j}=\sum_{i}c_{i}^{2} \lambda_{i}p(\lambda_{i})^{2},$$
where we used the orthogonality. On the other hand, $v^{\top}Mv=\sum_{i}c_{i}^{2}\lambda_{i}$. The claim follows because all the eigenvalues are non-negative. $\blacksquare$

Therefore:
$$\left\lVert x_{t}-x^{\*} \right\rVert_{A}^{2}=\min_{q(X)\in \mathcal{Q}\_{t}}\left\lVert q(A)x^{\*} \right\rVert_{A}^{2}\le \min_{q(X)\in \mathcal{Q}\_{t}}\max_{i=1}^{n}\left|q(\lambda_{i})\right|^{2}\cdot  \left\lVert x^{\*} \right\rVert_{A}^{2}.$$
More specifically, we have shown that (by extending the set on which we maximize):
$$\left\lVert x_{t}-x^{\*} \right\rVert_{A}^{2}\le \left\lVert x^{\*} \right\rVert_{A}^{2}\cdot \min_{q(X)\in \mathcal{Q}\_{t}}\max_{\lambda\in [\lambda_{1},\lambda_{n}]}\left|q(\lambda)\right|^{2}.$$
In particular, **every polynomial** $q \in \mathcal{Q}\_t$ gives an upper bound. For example, take
$$q(X)=-\left(1-\frac{2X}{\lambda_{1}+\lambda_{n}}\right)^{t},$$
and note $q(0)=-1$ so $q(X)\in \mathcal{Q}\_{t}$, while $|q(X)|^{2}$ is maximized over $[\lambda_{1},\lambda_{n}]$ when $\lambda=\lambda_{1}$. Thus
$$q(\lambda_{1})^{2}=\left(1-\frac{2\lambda_{1}}{\lambda_{1}+\lambda_{n}}\right)^{2t}=\left(\frac{\lambda_{n}-\lambda_{1}}{\lambda_{1}+\lambda_{n}}\right)^{2t}=\left(\frac{\kappa-1}{\kappa+1}\right)^{2t},$$
which leads to a slightly better convergence rate compared to gradient descent.

## Polynomial Minimax Evaluation

We have essentially arrived at the **minimax** problem of polynomial evaluation:
$$\min_{q(X)\in \mathcal{Q}\_{t}}\max_{\lambda\in [\lambda_{1},\lambda_{n}]}\left|q(\lambda)\right|^{2}.$$
The solution to a more general version of this problem is given by the Chebyshev polynomials, a central result in approximation theory.

> [!caution] Definition
> For an integer $n\ge 1$ the $n$-th **Chebyshev polynomial** $T_{n}(x)$ is defined recursively to be a degree $n$ polynomial as follows:
> $$T_{0}(x)=1,\quad T_{1}(x)=x,\quad T_{n}(x)=2x\cdot T_{n-1}(x)-T_{n-2}(x).$$

> [!important] Proposition
> It holds that $T_{n}(\cos \theta)=\cos (n \theta)$. In particular, $T_{n}(x)\in [-1,1]$ for every $x\in [-1,1]$.

The minimization property of the Chebyshev polynomials is:
> [!tip] Theorem
> For any integer $n\ge 1$, the normalized $n$-th Chebyshev polynomial $f(x)=\frac{1}{2^{n-1}}T_{n}(x)$ minimizes the term
> $$\max_{x\in [-1,1]}\left|g(x)\right|,$$
> over all non-zero polynomials $g$ with degree at most $n$ and leading coefficient $1$. The maximal value for $f$ is $\frac{1}{2^{n-1}}$.

For proof and background, see [[6]](#references).

Thus, Chebyshev polynomials offer good candidates to obtain the upper bound for the convergence rate. In particular, note that minimizing the square magnitude is equivalent to minimizing the magnitude. All we have to deal with is mapping polynomials evaluated on $[\lambda_{1},\lambda_{n}]$ to $[-1,1]$. This is done easily by mapping an interval
$$[a,b]\to [-1,1]\quad \text{via}\quad x\mapsto \frac{a+b-2x}{b-a}.$$
Note that $a\mapsto \frac{b-a}{b-a}=1$ while $b\mapsto \frac{a-b}{b-a}=-1$, and by linearity, every $a<x<b$ is mapped inside the interval. Define
$$Q_{a,b,n}(x)=\frac{T_{n}\left(\frac{a+b-2x}{b-a}\right)}{T_{n}\left(\frac{a+b}{b-a}\right)}.$$
It is a degree $n$ polynomial which satisfies
$$Q_{a,b,n}(0)=\frac{T_{n}(\frac{a+b}{b-a})}{T_{n}(\frac{a+b}{b-a})}=1.$$
Therefore $Q_{a,b,t} \in \mathcal{Q}\_{t}$ (up to a sign flip, which does not affect magnitude). Using the fact the numerator has magnitude $\le 1$ on the interval gives us the estimate
$$|Q_{\lambda_{1},\lambda_{n},t}(x)|\le \frac{1}{T_{t}(\frac{\lambda_{1}+\lambda_{n}}{\lambda_{n}-\lambda_{1}})}=T_{t}\left(\frac{\lambda_{n}/\lambda_{1}+1}{\lambda_{n}/\lambda_{1}-1}\right)^{-1}.$$
By a few more manipulations, we see that
$$T_{t}\left(\frac{\kappa+1}{\kappa-1}\right)\ge \frac{1}{2}\left(\frac{\sqrt{\kappa}+1}{\sqrt{\kappa}-1}\right)^{t},$$
hence
$$\left|Q_{\lambda_{1},\lambda_{n},t}(\lambda)\right|\le 2 \cdot \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^{t}.$$
This proves the following theorem (to obtain the theorem, a few more algebraic steps are required, similar to the omitted proof of the GD theorem), which is essentially a quadratic improvement over gradient descent ($\sqrt{\kappa(A)}$ instead of $\kappa(A)$):

> [!tip] Theorem
> The Conjugate Gradient algorithm achieves error
> $$\left\lVert x_{t}-A^{-1}b \right\rVert_{A}\le \varepsilon \left\lVert A^{-1}b \right\rVert_{A}$$
> after $t=O(\sqrt{\kappa(A)}\cdot \log (1/\varepsilon))$ iterations.

### Clustered Eigenvalues

Consider the following scenario: $\lambda_{n}$ is very large, $\lambda_{1}$ is very small, and most of the eigenvalues lie concentrated in the interval $[a,b]$ where $a$ and $b$ are moderately sized. The condition number $\lambda_n/\lambda_1$ is enormous, so the convergence bound derived above may be poor. However, it turns out that by a nice algebraic trick, the number of iterations required turns out to be small.

> [!important] Lemma
> If all but $c$ eigenvalues lie in are smaller than $b$, then after $t=\Omega(\sqrt{b/\lambda_1}\cdot \log (1/\varepsilon))$ iterations the error is bounded by $\varepsilon$ (plus a small constant overhead for the outliers).

**Proof.**
Let $a=\lambda_1$. Using the polynomials $Q_{a,b,t}$ we can define a composite polynomial:
$$q(x)=Q_{a,b,t}(x)\cdot \prod_{i=1}^{c} \left(1 - \frac{x}{\mu_i}\right)$$
where $\mu_{1},\ldots,\mu_{c}$ are the "outlier" eigenvalues outside of $[a,b]$.
Note that $q(0)=1 \cdot \prod 1 = 1$, so $q \in \mathcal{Q}\_{t+c}$. Furthermore:
1.  For any outlier eigenvalue $\mu_i$, the term $(1 - \mu_i/\mu_i) = 0$, so $q(\mu_i)=0$.
2.  For any $\lambda \in [a,b]$, the term $|1 - \lambda/\mu_i| \le  1$.
3.  The term $|Q_{a,b,t}(\lambda)|$ is bounded by the Chebyshev decay rate determined by the ratio $b/a$ (the "effective" condition number).

Thus, the error of the algorithm is at most $\max_{j}\left|q(\lambda_{j})\right| \le \max_{\lambda\in [a,b]}\left|Q_{a,b,t}(\lambda)\right|$. Setting $t=O(\sqrt{b/a}\cdot \log(1/\varepsilon))$, we obtain $\varepsilon$-error. In particular, $q(x)$ has degree $t+c$, so we need $t$ iterations for the bulk spectrum plus $c$ iterations to "kill" the outliers. $\blacksquare$

This lemma can be extended to outliers outside of $[a,b]$ for general $a$ (i.e., $a>\lambda_1$). In this case one has to deal with the fact $(1-x/\mu)$ may be large in magnitude for small $\mu$. This might add a logarithmic (in the condition number) number of iterations.

## References
1. **Hestenes, M. R., & Stiefel, E.** (1952), [*Methods of Conjugate Gradients for Solving Linear Systems*](https://doi.org/10.6028/jres.049.044).
2.  **Shewchuk, J. R.** (1994), [*An Introduction to the Conjugate Gradient Method Without the Agonizing Pain*](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf).
3.  **Trefethen, L. N., & Bau III, D.** (1997), [*Numerical Linear Algebra*](https://epubs.siam.org/doi/book/10.1137/1.9780898719574).
4.  **Nocedal, J., & Wright, S.** (2006), [*Numerical Optimization*](https://link.springer.com/book/10.1007/978-0-387-40065-5).
5. **Vishnoi, N.K.** (2013), [*$Lx=b$ Laplacian Solvers and Their Algorithmic Applications*](https://theory.epfl.ch/vishnoi/Lxb-Web.pdf).
6. **Mudde, M.H.** (2017), [*Chebyshev approximation*](https://fse.studenttheses.ub.rug.nl/15406/1/Marieke_Mudde_2017_EC.pdf).