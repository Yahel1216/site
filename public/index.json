{"categories":[{"link":"/categories/algebraic-algorithms/","name":"Algebraic Algorithms","slug":"Algebraic Algorithms"},{"link":"/categories/complexity/","name":"Complexity","slug":"Complexity"},{"link":"/categories/gradient-descent/","name":"Gradient Descent","slug":"Gradient Descent"},{"link":"/categories/kernels/","name":"Kernels","slug":"Kernels"},{"link":"/categories/machine-learning/","name":"Machine Learning","slug":"Machine Learning"},{"link":"/categories/math/","name":"Math","slug":"Math"},{"link":"/categories/matrix-multiplication/","name":"Matrix Multiplication","slug":"Matrix Multiplication"},{"link":"/categories/optimization/","name":"Optimization","slug":"Optimization"},{"link":"/categories/theory/","name":"Theory","slug":"Theory"}],"pages":[],"posts":[{"link":"/posts/kernel-4/","text":" Prerequisites Linear Algebra: Eigen-decompositions, positive definite matrices, rank. Functional Analysis: Hilbert spaces, $L^2$ spaces, operators. Kernel Methods: Previous posts in the series. Continuing our series on kernel methods, recall that these methods allow us to operate in high-dimensional spaces using the \u0026ldquo;kernel trick\u0026rdquo;. However, they suffer from a major computational bottleneck: constructing and manipulating the Gram matrix requires $O(N^2)$ memory and $O(N^3)$ time for operations like inversion or eigen decomposition, where $N$ is the dataset size. When $N$ reaches hundreds of thousands, exact computation becomes infeasible. In this post, I will explore the Nystrom method, a powerful technique for constructing low-rank approximations of these matrices. Rather than treating it merely as a linear algebra heuristic, I want to derive it from first principles: starting with the spectral properties of integral operators on Hilbert spaces and showing how the discretization of these operators naturally leads to the matrix approximation formulas we use in practice.\nLet $K:X\\times X\\to \\mathbb{R}$ denote a symmetric positive definite kernel, with $X=\\mathbb{R}^{d}$. Recall that by the Moore-Aronszajn theorem, there exists a feature map $\\varphi:X\\to \\mathcal{H}$, where $\\mathcal{H}$ is a Hilbert space, such that: $$K(x,y)=\\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle_{\\mathcal{H}}.$$\nSome Analysis Recall a remark I\u0026rsquo;ve written in a previous post, saying that a kernel can be used to define an integral transform. Let $L^{2}(X)$ denote the space of functions $f:X\\to \\mathbb{C}$ which are square integrable, i.e., $$\\int_{X}^{}{\\left|f(x)\\right|^{2}}\\ \\mathrm{d}{x}\u0026lt;\\infty.$$ This is a Hilbert space, with the inner product $$\\left\\langle f,g \\right\\rangle=\\int_{X}^{}{f(x)\\cdot \\overline{g(x)}}\\ \\mathrm{d}{x}.$$ Then $K$ can be used to define an operator $T_{K}:L^{2}(X)\\to L^{2}(X)$, taking in functions and spitting out new functions (like the Fourier transform), by letting: $$[T_{K}(f)] (x)=\\int_{X}^{}{K(x,y)\\cdot f(y)}\\ \\mathrm{d}{y}.$$ By linearity of the integral, it is clear that $T_{K}$ is linear.\nProperties of the Integral Transform Lemma A linear operator is continuous if and only if the operator norm, defined by $$\\left\\lVert T \\right\\rVert:=\\sup_{f\\in L^{2}(X),\\left\\lVert f \\right\\rVert=1}\\left\\lVert T(f) \\right\\rVert$$ is finite (bounded). Here $\\left\\lVert f \\right\\rVert=\\left(\\int_{X}^{}{\\left|f(x)\\right|^{2}}\\ \\mathrm{d}{x}\\right)^{1/2}$ is the norm induced from the inner product.\nProof. $f_{n}\\to f$ in $L^{2}(X)$ if and only if $\\left\\lVert f_{n}-f \\right\\rVert\\to 0$. Hence: $$\\left\\lVert T(f_{n})-T(f) \\right\\rVert= \\left\\lVert T(f_{n}-f) \\right\\rVert\\le \\left\\lVert T \\right\\rVert\\cdot \\left\\lVert f_{n}-f \\right\\rVert\\to 0,$$ implying $Tf_{n}\\to Tf$ when $T$ is bounded. Conversely, if $T$ is continuous, it is continuous at $0$. Hence for every $\\varepsilon\u0026gt;0$ there is $\\delta\u0026gt;0$ such that every $\\left\\lVert f \\right\\rVert\\le \\delta$ satisfies $\\left\\lVert Tf \\right\\rVert\\le \\varepsilon$. Consequently, setting $\\varepsilon=1$, let $\\delta\u0026gt;0$ be the respective value. For every $\\left\\lVert f \\right\\rVert= 1$ we have $\\left\\lVert \\delta f \\right\\rVert=\\delta\\le \\delta$ and so $\\delta \\left\\lVert Tf \\right\\rVert= \\left\\lVert T(\\delta f) \\right\\rVert\\le 1$, which implies $\\left\\lVert Tf \\right\\rVert\\le 1/\\delta$. Since $\\delta$ is constant, $\\left\\lVert T \\right\\rVert$ is bounded. $\\blacksquare$\nLemma If $$\\left\\lVert K \\right\\rVert_{L^{2}(X\\times X)}^{2}=\\int_{X \\times X}^{}{\\left|K(x,y)\\right|^{2}}\\ \\mathrm{d}{(x,y)}\u0026lt;\\infty$$ then $T_{K}$ is continuous.\nProof. By Cauchy-Schwarz inequality, $$|T_{K}f(x)|=\\left|\\left\\langle K(x,\\cdot ), \\overline{f} \\right\\rangle\\right|\\le \\left\\lVert K(x,\\cdot) \\right\\rVert\\cdot \\left\\lVert f \\right\\rVert.$$ Therefore, $$\\left\\lVert T_{K}f \\right\\rVert^{2}=\\int_{X}^{}{\\left|T_{K}f(x)\\right|^{2}}\\ \\mathrm{d}{x}\\le \\int_{X}^{}{\\left\\lVert K(x,\\cdot ) \\right\\rVert^{2}\\cdot \\left\\lVert f \\right\\rVert^{2}}\\ \\mathrm{d}{x}=\\left\\lVert f \\right\\rVert^{2}\\cdot \\int_{X}^{}{\\int_{X}^{}{\\left|K(x,y)\\right|^{2}}\\ \\mathrm{d}{y}}\\ \\mathrm{d}{x},$$ where we can take $\\left\\lVert f \\right\\rVert^{2}$ out because it is a scalar (independent of $x$). The last integral is just $\\int_{X\\times X}^{}{\\left|K(x,y)\\right|^{2}}\\ \\mathrm{d}{(x,y)}$ by Fubini\u0026rsquo;s theorem, and so $$\\left\\lVert T_{K}f \\right\\rVert^{2}\\le \\left\\lVert f \\right\\rVert^{2}\\cdot \\left\\lVert K \\right\\rVert_{L^{2}(X\\times X)}^{2}\\implies \\left\\lVert T_{K} \\right\\rVert\\le \\left\\lVert K \\right\\rVert_{L^{2}(X\\times X)},$$ and in particular it is bounded, thus continuous. $\\blacksquare$\nDefinition A linear operator is self-adjoint if $T=T^{*}$, where $T^{*}$ is the unique linear operator satisfying $$\\left\\langle Tf,g \\right\\rangle=\\left\\langle f,T^{*}g \\right\\rangle$$ for every $f,g\\in L^{2}(X)$.\nLemma If $K$ is real-valued then $T_{K}$ is self-adjoint.\nProof. It holds that $$\\begin{aligned} \\left\\langle T_{K}f,g \\right\\rangle \u0026amp; =\\int_{X}^{}{T_{K}f(x) \\overline{g(x)}}\\ \\mathrm{d}{x}=\\int_{X}^{}{\\int_{X}^{}{K(x,y)\\cdot f(y)\\cdot \\overline{g(x)}}\\ \\mathrm{d}{y}}\\ \\mathrm{d}{x} \\\\ \u0026amp;=\\int_{X}^{}{\\int_{X}^{}{f(y)\\cdot \\overline{K(y,x)\\cdot g(x)}}\\ \\mathrm{d}{x}}\\ \\mathrm{d}{y}=\\int_{X}^{}{f(y)\\cdot \\overline{T_{K}g(y)}}\\ \\mathrm{d}{y}= \\left\\langle f,T_{K}g \\right\\rangle \\end{aligned}$$ using Fubini\u0026rsquo;s theorem and the fact $\\overline{r}=r$ for $r\\in\\mathbb{R}$ (and symmetry $K(x,y)=K(y,x)$). $\\blacksquare$\nDefinition A linear operator is called compact if for every bounded sequence $(f_{n})_{n=1}^{\\infty}$, the sequence $(Tf_{n})_{n=1}^{\\infty}$ has a convergent sub-sequence.\nEquivalently, $T$ is compact if and only if it is the limit (in operator norm) of a sequence $T_{n}$ of operators for which $\\mathrm{Im}(T_{n})$ is a finite dimensional subspace.\nLemma If $\\left\\lVert K \\right\\rVert_{L^{2}(X\\times X)}\u0026lt;\\infty$ then $T_{K}$ is compact.\nProof. Fix some orthonormal basis for $L^{2}(X)$, denoted $e_{1},e_{2},\\ldots$. Take $P_{n}$ to be the projection operator onto $\\mathrm{Span}{e_{1},\\ldots,e_{n}}$. Define $T_{n}=T_{K}\\circ P_{n}$ (composition). Note that the image of $T_{n}$ is the image of $T_{K}$ on a finite dimensional space, hence it is also finite dimensional. Let $\\left\\lVert f \\right\\rVert=1$ and expand it with the orthonormal basis $f=\\sum_{i=1}^{\\infty}\\alpha_{i}e_{i}$ (where $\\alpha_{i}\\in\\mathbb{C}$). Note that $\\sum |\\alpha_i|^2 \\le 1$. Then: $$\\begin{aligned} \\left\\lVert (T_{K}-T_{n})f \\right\\rVert^{2}\u0026amp;=\\left\\lVert \\sum_{i=n+1}^{\\infty}\\alpha_{i}T_{K}e_{i} \\right\\rVert^{2}\\le \\left(\\sum_{i=n+1}^{\\infty}|\\alpha_{i}| \\left\\lVert T_{K}e_{i} \\right\\rVert\\right)^{2} \\\\ \u0026amp;\\le \\left(\\sum_{i=n+1}^\\infty |\\alpha_i|^2\\right)\\left(\\sum_{i=n+1}^\\infty |T_K e_i|^2\\right) \\le \\sum_{i=n+1}^{\\infty}\\left\\lVert T_{K}e_{i} \\right\\rVert^{2} \\end{aligned}$$ We are left showing $$\\sum_{i=1}^{\\infty}\\left\\lVert T_{K}e_{i} \\right\\rVert^{2}\u0026lt;\\infty,$$ which would imply the tail $\\sum_{i=n+1}^{\\infty}\\left\\lVert T_{K}e_{i} \\right\\rVert^{2}\\to 0$ vanishes, hence $\\left\\lVert T_{K}-T_{n} \\right\\rVert\\to 0$, and so $T_{K}$ is compact. By Parseval\u0026rsquo;s identity, we know that for every fixed $x$, the function $K(x,\\cdot)$ satisfies $$\\int_{X}^{}{\\left|K(x,y)\\right|^{2}}\\ \\mathrm{d}{y}=\\left\\lVert K(x,\\cdot) \\right\\rVert^{2}=\\sum_{i=1}^{\\infty}|\\left\\langle K(x,\\cdot),e_{i} \\right\\rangle|^{2}=\\sum_{i=1}^{\\infty}\\left|\\int_{X}^{}{K(x,y)e_{i}(y)}\\ \\mathrm{d}{y}\\right|^{2},$$ and therefore $$\\begin{aligned} \\sum_{i=1}^{\\infty}\\left\\lVert T_{K}e_{i} \\right\\rVert^{2}\u0026amp;=\\sum_{i=1}^{\\infty}\\int_{X}^{}{\\left|\\int_{X}^{}{K(x,y)e_{i}(y)}\\ \\mathrm{d}{y}\\right|^{2}}\\ \\mathrm{d}{x}=\\int_{X}^{}{\\sum_{i=1}^{\\infty}\\left|\\int_{X}^{}{K(x,y)e_{i}}\\ \\mathrm{d}{y}\\right|^{2}}\\ \\mathrm{d}{x} \\\\ \u0026amp;=\\int_{X}^{}{\\int_{X}^{}{\\left|K(x,y)\\right|^{2}}\\ \\mathrm{d}{y}}\\ \\mathrm{d}{x}=\\left\\lVert K \\right\\rVert_{L^{2}(X\\times X)}^{2}\u0026lt; \\infty\\end{aligned}$$ $\\blacksquare$\nUsing the Spectral Theorem The following theorem is a fundamental result in spectral analysis:\nTheorem: The Spectral Theorem If $T:\\mathcal{H}\\to \\mathcal{H}$ is a self-adjoint, compact, and continuous linear operator on a (separable) Hilbert space $\\mathcal{H}$, there exists an orthonormal basis for $\\mathcal{H}$ consisting of eigenfunctions of $T$, denoted by $(\\psi_{n})_{n=1}^{\\infty}$, corresponding to eigenvalues $(\\lambda_{n})_{n=1}^{\\infty}$, where the sequence of eigenvalues is monotone decreasing and vanishes at infinity.\nThis is a generalization of the spectral theorem for self-adjoint matrices in finite-dimensions: a linear operator $T$ on a finite dimensional space can be represented by a matrix $M$, and the operator is self-adjoint iff the matrix is self-adjoint, $\\overline{M^{\\top}}=M$. On finite dimensions, linear operators are always compact and continuous, and so we obtain the existence of a sequence $(\\lambda_{n})_{n=1}^{d}$ (where $d$ is the dimension) and orthonormal vectors $(v_{n})_{n=1}^{d}$ such that $Mv_{n}=\\lambda v_{n}$, concluding that $M=V \\Lambda \\overline{V^{\\top}}$ with $V=(v_{1},\\ldots,v_{d})$ and $\\Lambda =\\mathrm{diag}(\\lambda_{1},\\ldots,\\lambda_{d})$.\nIn particular, for $T_{K}$ we conclude that:\nCorollary (Mercer\u0026rsquo;s Theorem) The operator $T_{K}$ has eigenvalues $(\\lambda_{n})$ and eigenfunctions $(\\psi_{n})$, which form an orthonormal basis. It holds $$K(x,y)=\\sum_{n}\\lambda_{n}\\psi_{n}(x)\\psi_{n}(y).$$\nProof Sketch. Using the expansion in the eigenfunction basis, we write $f=\\sum_{n}\\left\\langle f,\\psi_{n} \\right\\rangle \\cdot \\psi_{n}$. Hence: $$T_{K}f(x)=\\left(T_{K}\\left[\\sum_{n}\\left\\langle f,\\psi_{n} \\right\\rangle \\cdot \\psi_{n}\\right]\\right)(x)=\\sum_{n} \\lambda_{n} \\left\\langle f,\\psi_{n} \\right\\rangle\\cdot \\psi_{n}(x).$$ Expanding the inner product and switching the sum and integration: $$\\sum_{n}\\lambda_{n}\\left\\langle f,\\psi_{n} \\right\\rangle\\cdot \\psi_{n}(x)=\\sum_{n} \\lambda_{n}\\psi_{n}(x)\\cdot \\int_{X}^{}{f(y) \\cdot \\overline{\\psi_{n}(y)}}\\ \\mathrm{d}{y}=\\int_{X}^{}{\\left(\\sum_{n}\\lambda_{n}\\psi_{n}(x) \\overline{\\psi_{n}(y)}\\right) f(y)}\\ \\mathrm{d}{y}.$$ Write $\\widetilde{K}(x,y)=\\sum_{n}\\lambda_{n}\\psi_{n}(x)\\overline{\\psi_{n}(y)}$, and note that $T_{K}=T_{\\widetilde{K}}$ by the above identity. This implies $K=\\widetilde{K}$, otherwise we could have found a function $f$ for which the integrals would have been different. Since $K$ is real valued, the complex conjugation can be removed. $\\blacksquare$\nDealing With Other Measures The values in the dataset, $x_{1},\\ldots,x_{N}$ come from some probability distribution, representing the data. In the discussion above, we implicitly used the Lebesgue measure which is not a probability distribution. However, all the arguments carry over to the case where we switch the definition of integration to be against some probability measure $\\mu$. So the $L^{2}(X)$ space has the inner product $$\\left\\langle f,g \\right\\rangle=\\int_{X}^{}{f(x)\\overline{g(x)}}\\ \\mathrm{d}{\\mu(x)},$$ and $T_{K}$ is given by $$T_{K}f(x)=\\int_{X}^{}{K(x,y)f(y)}\\ \\mathrm{d}{\\mu(x)}.$$ The orthonormal basis is then orthonormal with respect to this measure.\nIn fact, proving $T_K$ is compact becomes much more easier when the probability measure is supported on a compact subset of $\\mathbb{R}^d$. All the proofs we gave above can be simplified to conclude that $T_K$ is compact, continuous and self-adjoint. It is crucial to take the measure into account. For example, the Gaussian kernel (radial basis function) we discussed before does not induce a compact operator on $\\mathbb{R}^d$ with the standard measure, or even a non-compactly supported measure.\nCompactly supported measures are very often in practice, simply because we cannot really sample from an unbounded distribution, due to machine restrictions.\nApproximating the Kernel Recall that our goal is to compute a fast approximation to $K(x_{i},x_{j})$ over a dataset of points $x_{1},\\ldots,x_{N}\\sim \\mu$. The previous discussion suggests that we should approximate $$\\lambda_{n}\\psi_{n}(x_{i})\\psi_{n}(x_{j})$$ for the largest eigenvalues (recall $\\lambda_{1}\\ge \\lambda_{2}\\ge \\ldots$), as they are likely to contribute the most the sum. Moreover, computing $\\lambda_{n}\\psi_{n}(y)$ is the same as computing $$\\lambda_{n}\\psi_{n}(y)=T_{K}\\psi_{n}(y)=\\int_{X}^{}{K(y,x) \\psi_{n}(x)}\\ \\mathrm{d}{\\mu(x)}.$$ By randomly sampling $\\widehat{x}_{1},\\ldots,\\widehat{x}_{q}$ from $X$ (according to the probability measure $\\mu$), we can approximate the integral using numerical integration: $$\\frac{1}{q}\\sum_{i=1}^{q}K(y,\\widehat{x}_{i})\\cdot \\psi_{n}(\\widehat{x}_{i})\\approx \\int_{X}^{}{K(y,x)\\psi_{n}(x)}\\ \\mathrm{d}{x}=\\lambda_{n}\\psi_{n}(y).$$ The orthonormality of $\\psi_{n}$ and $\\psi_{m}$ can be empirically checked by $$\\frac{1}{q}\\sum_{i=1}^{q}\\psi_{n}(\\widehat{x}_{i})\\psi_{m}(\\widehat{x}_{i})\\approx \\int_{X}^{}{\\psi_{n}(x)\\psi_{m}(x)}\\ \\mathrm{d}{x}=\\delta_{n,m}.$$ Note that numerical integration does not require explicit dealing with the probability measure $\\mu$, because it is implicit in the sampling process.\nRemark. Numerical integration works in expectation even for $q=1$, and taking $q$ to be large enough we can use concentration inequalities that guarantee a very good approximation with high probability, as we did in the post about random Fourier Features. Although we\u0026rsquo;ll not quantify this here, it can be shown to be a very accurate approximation with high probability, uniformly on compact domains.\nAs a Matrix Eigenproblem Define $$K^{(q)}\\in \\mathbb{R}^{q \\times q}:\\quad (K^{(q)})_{i,j}=K(\\widehat{x}_{i},\\widehat{x}_{j})$$ and also $$U^{(q)}\\in\\mathbb{R}^{q \\times q}:\\quad (U^{(q)})_{i,j}=\\frac{1}{\\sqrt{q}}\\psi_{j}(\\widehat{x}_{i}).$$ Furthermore, let $\\Lambda^{(q)}$ be the diagonal $q\\times q$ matrix with values $\\lambda_{1}^{(q)}\\ge \\ldots \\ge \\lambda^{(q)}_{q}\\ge 0$, given by $\\lambda_{i}^{(q)}=q\\cdot \\lambda_{i}$.\nNote that this definition satisfies the equation $$K^{(q)}U^{(q)}\\approx U^{(q)}\\Lambda^{(q)},$$ as the $i,j$-th element on the left is just the numerical integration expression $$\\frac{1}{\\sqrt{q}}\\sum_{t=1}^{q}K(\\widehat{x}_{i},\\widehat{x}_{t})\\cdot \\psi_{j}(\\widehat{x}_{t})\\approx \\sqrt{q} \\lambda_{j} \\psi_{j}(\\widehat{x}_{i}),$$ while the $i,j$-th element on the right is the approximated quantity $$\\frac{1}{\\sqrt{q}}\\cdot q \\cdot \\lambda_{j} \\cdot \\psi_{j}(\\widehat{x}_{i})=\\sqrt{q}\\lambda_{j}\\psi_{j}(\\widehat{x}_{i}).$$ Moreover, $U^{(q)}$ has orthonormal columns, hence it is invertible, and therefore $$K^{(q)}\\approx U^{(q)}\\Lambda^{(q)}(U^{(q)})^{\\top}.$$ Of course, equality would mean that our integration is exact, thus we can view the numerical integration problem as the matrix eigen-decomposition problem of $K^{(q)}$.\nEstimating the Gram Matrix We now arrive at the core of the Nyström method: bridging the small sample $q$ to the full dataset $N$. Define $$\\mathbf{K}\\in\\mathbb{R}^{N\\times N}:\\quad \\mathbf{K}_{i,j}=K(x_{i},x_{j}),$$ as the Gram matrix of the input dataset. By re-ordering the order of the points, we may assume the first $q$ points are our sample $\\widehat{x}_{1}, \\dots, \\widehat{x}_{q}$, and the remaining $N-q$ points are the rest of the dataset. In other words, $\\widehat{x}_i = x_i$ for $1\\le i\\le q$. Thus, we can write $\\mathbf{K}$ in block form: $$\\mathbf{K}=\\begin{bmatrix}K^{(q)} \u0026amp; K_{q,N-q} \\\\ K_{N-q,q} \u0026amp; K_{N-q,N-q}\\end{bmatrix}.$$ Since the Gram matrix is symmetric, it must hold $K_{q,N-q}=K_{N-q,q}^{\\top}$. Denote $K_{q,q}=K^{(q)}$. Moreover, denote $$K_{N,q}=\\begin{bmatrix}K^{(q)} \\\\ K_{N-q,q} \\end{bmatrix},\\quad K_{q,N}=\\begin{bmatrix}K^{(q)} \u0026amp; K_{q,N-q}\\end{bmatrix}$$ By symmetry, $K_{N,q}=K_{q,N}^{\\top}$. If $N$ is large, then setting $q=N$ and computing the eigen decomposition of $\\mathbf{K}$ is very expensive. So the question becomes: How can we approximate $\\mathbf{K}$ using the eigen decomposition of the small block $K^{(q)}$?\nThis is the method\u0026rsquo;s claim to fame \u0026ndash; because we are dealing with kernels, we can use the decomposition of $K^{(q)}$ to estimate the other values of $\\mathbf{K}$.\nThe Nystrom Extension The key insight comes from our integral approximation earlier. Recall that for an eigenfunction $\\psi_j$ and eigenvalue $\\lambda_j$, we have: $$\\lambda_{j}\\psi_{j}(y) \\approx \\frac{1}{q}\\sum_{i=1}^{q}K(y,x_i)\\cdot \\psi_{j}(x_{i}).$$ Rearranging this allows us to extend the value of the eigenfunction to any new point $y \\in X$ given its values on the set $x_1,\\ldots ,x_q$ (approximately): $$ \\psi_{j}(y) \\approx \\frac{1}{q\\lambda_n} \\sum_{i=1}^{q}K(y,x_{i})\\cdot \\psi_{j}({x}_{i}). $$ Using our matrix definitions $\\lambda_j^{(q)} = q\\lambda_j$ and $(U^{(q)})_{i,j} = \\frac{1}{\\sqrt{q}}\\psi_j({x}_i)$, we can rewrite the extension formula as: $$ \\psi_{j}(y) \\approx \\frac{1}{\\lambda_j^{(q)}} \\sum_{i=1}^{q}K(y,{x}_{i}) \\cdot \\sqrt{q}(U^{(q)})_{i,j}. $$ In particular, for $i\u0026gt;q$, we have $$\\psi_j(x_i)\\approx \\frac{\\sqrt{q}}{\\lambda_j^{(q)}}\\sum_{t=1}^q K(x_i,x_t)\\cdot (U^{(q)})_{t,j}=\\frac{\\sqrt{q}}{\\lambda_j^{(q)}}(K_{N,q} U^{(q)})_{i,j}.$$\nDefine $\\tilde{u}^{(j)}\\in\\mathbb{R}^N$ denote the vector of approximations to $v_j=(\\psi_j(x_1),\\ldots, \\psi_j(x_N))$, defined by the formula above. Then $$\\tilde{u}^{(j)}=\\frac{\\sqrt{q}}{\\lambda_j^{(q)}} [K_{N,q}U^{(q)}]_{*,j}=\\sqrt{q}\\cdot [K_{N,q}U^{(q)}(\\Lambda^{(q)})^{-1}]_{*,j}$$that is, the $j$-column of the scaled matrix product. Define $$\\tilde{U}=\\begin{bmatrix}\\tilde{u}^{(1)} \u0026amp; \\cdots \u0026amp; \\tilde{u}^{(q)}\\end{bmatrix}\\in \\mathbb{R}^{N\\times q}.$$\nArriving at a Low-Rank Approximation Recall that we\u0026rsquo;ve shown $$K(x,y)=\\sum_n \\lambda_n \\psi_n(x)\\psi_n(y)$$ The best $q$-rank approximation is given by taking the top $q$ eigenvalues. Therefore, $$\\mathbf{K}_{i,j}=K(x_i,x_j)\\approx \\sum_{n=1}^q \\lambda_n \\psi_n(x_i)\\psi_n(x_j)$$ Plugging in the notation from the previous section: $$\\mathbf{K}_{i,j}\\approx \\sum_{n=1}^q \\lambda_n v_n(i)v_n(j)=\\sum_{n=1}^q \\lambda_n (v_n \\otimes v_n)_{i,j}$$ where $\\otimes$ denotes the Kronecker product of vectors (as we\u0026rsquo;ve encountered in the previous post). Therefore the best $q$-rank approximation for $\\mathbf{K}$ is $\\sum_{n=1}^q \\lambda_n (v_n \\otimes v_n)$. We approximate this by using $\\tilde{u}^{(n)}$ instead of $v_n$: $$\\mathbf{K}\\approx \\sum_{n=1}^q\\lambda_n (\\tilde{u}^{(n)}\\otimes \\tilde{u}^{(n)}) = \\sum_{n=1}^q \\lambda_n \\tilde{u}^{(n)}(\\tilde{u}^{(n)})^{\\top}$$ where we use the fact $x\\otimes y= xy^{\\top}$ is an equivalent way to define the Kronecker product of vectors.\nLemma Let $A,B$ be matrices of size $n\\times m,m\\times n$ respectively. Let $a_1,\\ldots,a_m\\in \\mathbb{R}^n$ denote the columns of $A$ and $b_1,\\ldots,b_m\\in\\mathbb{R}^n$ denote the rows of $B$. Then $$AB=\\sum_{i=1}^m a_i b_i^{\\top}.$$\nProof. The $(k,j)$-th element of $AB$ is by definition $\\sum_{i=1}^m A_{k,i}B_{i,j}=\\sum_{i=1}^m a_i(k)\\cdot b_i(j)$. On the other hand, the $(k,j)$-th element of $a_ib_i^{\\top}$ is $a_i(k)\\cdot b_i(j)$ by definition, and the equality follows. $\\blacksquare$\nReturning to the approximation of $\\mathbf{K}$, note that scaling the $n$-th column of $\\tilde{U}$ by $\\lambda_n$ is the same as multiplying by a diagonal matrix from the right. Using the lemma, the sum can be re-written as $$\\mathbf{K}\\approx \\frac{1}{q} \\tilde{U}\\Lambda^{(q)}\\tilde{U}^{\\top}$$ where the extra $1/q$ factor sets off the definition of $\\Lambda^{(q)}$. Substituting the definition of $\\tilde{U}$: $$ \\begin{aligned} \\mathbf{K} \u0026amp;\\approx \\left( K_{N,q} U^{(q)} (\\Lambda^{(q)})^{-1} \\right) \\Lambda^{(q)} \\left( K_{N,q} U^{(q)} (\\Lambda^{(q)})^{-1} \\right)^\\top \\\\ \u0026amp;= K_{N,q} U^{(q)} (\\Lambda^{(q)})^{-1} \\underbrace{\\Lambda^{(q)} (\\Lambda^{(q)})^{-1}}_{I} (U^{(q)})^\\top K_{q,N} \\\\ \u0026amp;= K_{N,q} \\underbrace{U^{(q)} (\\Lambda^{(q)})^{-1} (U^{(q)})^\\top}_{(K^{(q)})^{-1}} K_{q,N}\\\\ \u0026amp; = K_{N,q} (K^{(q)})^{-1} K_{q,N} \\end{aligned} $$\nIn the last step, we used the fact that $K^{(q)} = U^{(q)} \\Lambda^{(q)} (U^{(q)})^\\top$, which implies its inverse (or pseudo-inverse) is $U^{(q)} (\\Lambda^{(q)})^{-1} (U^{(q)})^\\top$.\nTheorem: The Nystrom Approximation The Nyström method approximates the full Gram matrix $\\mathbf{K}$ by: $$ \\widetilde{K} = K_{N,q} (K^{(q)})^{-1} K_{q,N}. $$ In block form, this results in: $$ \\widetilde{K} = \\begin{bmatrix} K^{(q)} \u0026amp; K_{q, N-q} \\\\ K_{N-q, q} \u0026amp; K_{N-q, q} (K^{(q)})^{-1} K_{q, N-q} \\end{bmatrix}. $$\nMoreover, the term $K_{N-q, q} (K^{(q)})^{-1} K_{q, N-q}$ is precisely the Schur complement of $K^{(q)}$ in $\\widetilde{K}$ being set to zero (meaning $\\widetilde{K}$ has rank $q$). I hope to write a blog on Schur complements soon enough, but for now I won\u0026rsquo;t go into details here.\nRemark. The only block which is approximated is the bottom right block $K_{N-q,N-q}$. The others are computed exactly.\nRemark 2. Low-rank approximations are useful because the operations cost proportionally to the rank. For example, inverting the matrix $K^{(q)}$ is done in time $O(q^3)$ instead of $O(N^3)$! Computing the product of $\\tilde{K}$ by a vector can be done in time $O(Nq)$ instead of $O(N^2)$.\nConclusions The Nystrom method gives an alternative to the direct eigen decomposition of the Gram matrix, resulting in an approximate best low rank approximation to $\\mathbf{K}$ (the best low-rank approximation requires the eigen decomposition). The core idea is to use the fact numerical integration on a small number of points yields a good approximation of the eigenfunctions on new points too. Thus computing the top eigenfunctions on a small number of points (equivalent to decomposing the small matrix $K^{(q)}$) suffices to obtain good approximations of all the other values.\nThere have been many innovations and adaptations based on this basic method, that propose different sampling methods, reconstruction methods and numerical stability improvements. However, the core ideas are the same.\nReferences Williams, C. K., \u0026amp; Seeger, M. (2001). Using the Nyström method to speed up kernel machines . Drineas, P., \u0026amp; Mahoney, M. W. (2005). On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning . Kumar, S., Mohri, M., \u0026amp; Talwalkar, A. (2012). Sampling methods for the Nyström method . Gittens, A., \u0026amp; Mahoney, M. W. (2016). Revisiting the Nyström Method for Improved Large-Scale Machine Learning . Bach, F. (2013). Sharp analysis of low-rank kernel matrix approximations . Nyström, E. J. (1930). Über die Praktische Auflösung von Integralgleichungen with Anwendungen auf Randwertaufgaben . ","title":"The Nystrom Method: Spectral Action"},{"link":"/posts/fast-integer-mult/","text":" Prerequisites Ring Theory: Homomorphisms, Ideals, Quotient Rings, Chinese Remainder Theorem (see Appendix for definitions). Polynomials: Polynomial multiplication, division, convolutions. Algorithm Analysis: Basic recursive complexity and $O$-notation. Given two integers $a,b\\in \\mathbb{N}$, how fast can we multiply them?\nLet\u0026rsquo;s recall the elementary school algorithm for integer multiplication:\nGiven two decimal numbers $a,b$ of length $n$ and $m$ (assume $n \\ge m$). For every $i \\in [n]$ and $j \\in [m]$: Compute the product of the $i$-th digit of $a$ and the $j$-th digit of $b$. Shift the result by $i+j$ positions. Add to the accumulator. Return the accumulated number. Overall we perform $O(n\\cdot m)$ multiplications of one-digit numbers, which can be done in constant time just by writing down the multiplication table. We also perform $O(nm)$ additions. By carefully ordering the addition operations, we can ensure there are $O(n)$ additions of $O(m)$-long numbers. Since addition requires linear time, we obtain a total runtime of $O(nm)$.\nNote that by moving to binary instead of decimal, the runtime remains asymptotically equivalent, because the number of digits needed to represent a number $a$ in base $B$ is roughly $\\log_B (a)$ and $\\log_{B\u0026rsquo;}(a)=\\frac{\\log_B(a)}{\\log_{B}(B\u0026rsquo;)}$ showing $\\log_B(a)=\\Theta(\\log_{B\u0026rsquo;}(a))$ for any other base $B\u0026rsquo;$.\nFor many years, up to Karatsuba\u0026rsquo;s discovery in the 60\u0026rsquo;s, it was the best known method. In this post, we discuss the deep ideas behind fast algorithms for integer multiplication, building up the fastest algorithms known up to a few years ago.\nWhile often treated as bit-manipulation tricks, these algorithms are fundamentally algebraic. What do we mean by an Algebraic Algorithm?\nOne way to define algebraic algorithms, also called symbolic computation, is that we limit the algorithm to algebraic manipulations of symbolic expressions (like polynomials). In particular, we do not allow numerical approximation, and the algorithm must output an exact result. We often treat ring operations as constant time operations, and they are the basic building block for the algorithms.\nThe algorithms we\u0026rsquo;ll cover in this post are all algebraic, but it should be noted that the recent state-of-the-art algorithms for integer multiplication rely analytic bounds and numerical approximations.\nThe Algebraic Framework Before diving into specific algorithms, we distill the common techniques they all share. These tools allow us to transport a multiplication problem from one ring to another where it might be easier to solve.\nTool 1: Mapping Definition: Mapping Let $R, A$ be rings and $f: R \\to A$ be a ring homomorphism. Given $r, s \\in R$, we can compute $r \\cdot s$ by computing $f(r) \\cdot f(s)$ in $A$. If $f$ is injective (or we have a way to invert specific elements), we can recover $r \\cdot s$.\nExample: Modular Arithmetic. If $a,b\\in\\mathbb{N}$ are integers which are both (strictly) smaller than $m$, then we can compute $a\\cdot b$ by first reducing $a,b$ modulo $m^2-1$, computing the product of the reduced elements, and return that product as the result. This works because $\\mathbb{Z}\\to \\mathbb{Z}_{m^2-1}$ given by $x\\mapsto x\\mod (m^2-1)$ is a homomorphism, and $\\mathbb{Z}_{m^2-1}\\to \\mathbb{Z}$ given by $x\\mapsto x$ inverts the homomorphism for $a,b$ which are small enough.\nExample: Modular Arithmetic for Polynomials. Fix a monic polynomial $p \\in R[x]$ with $\\deg(p) \u0026gt; 2n$. If $r, s \\in R[x]$ have degree at most $n$, their product has degree at most $2n$. Thus, we can recover $r \\cdot s$ perfectly from its image in the quotient ring $R[x]/(p)$. Formally, we use the homomorphism $f: R[x] \\to R[x]/(p)$ given by $q(x) \\mapsto q(x) \\pmod{p(x)}$.\nIf $\\deg(p) = 2n$, we might lose the highest degree coefficient ($x^{2n}$). We can fix this missing data by performing a single scalar multiplication of the leading coefficients $r_n s_n$ and adding $r_n s_n \\cdot p(x)$ back to result. This is often called evaluating at $\\infty$.\nTool 2: Lifting Definition: Lifting Let $I \\lhd R$ be an ideal. The quotient ring $R/I$ can be \u0026ldquo;lifted\u0026rdquo; back to $R$. If we have an equation $ab = c$ in $R/I$, we can choose representatives $\\hat{a}, \\hat{b} \\in R$ and compute $\\hat{a}\\hat{b}$. The result will be congruent to $c$ modulo $I$.\nA crucial application of lifting is Clumping (or Substitution).\n1. Base-$B$ Clumping (Integers $\\to$ Polynomials) To multiply integers $a, b$, we can view them as polynomials evaluated at a base $B$.\nWrite $a = \\sum a_i B^i$ and $b = \\sum b_i B^i$ where $0 \\le a_i, b_i \u0026lt; B$. Lift $a \\to a(y) = \\sum a_i y^i$ and $b\\to b(y)=\\sum b_i y^i$ in $\\mathbb{Z}[y]$. Compute $p(y) = a(y) \\cdot b(y)$ and recover the result $a\\cdot b$ from $p$. Formally, we have mapped $\\mathbb{Z}\\to \\mathbb{Z}[y]/(y-B)$ (view the numbers as polynomials evaluated at $B$), and then lifted $\\mathbb{Z}[y]/(y-B)\\to \\mathbb{Z}[y]$, forgetting the evaluation at $B$. We know that $p(y)\\equiv a(y)b(y)\\mod (y-B)$, which means $p(B)=ab$. Note that computing $a(y)\\cdot b(y)$ can be now done using integer multiplications of size $B$, while $a,b$ are potentially much larger than $B$.\n2. Polynomial Clumping This is a useful way to control the degrees of polynomials. Formally, we map $R[x]\\to R[x][y]/(x^n-y)$, replacing powers of $x^n$ by powers of $y$. Then, we lift $R[x][y]/(x^n-y)\\to R[x][y]$, forgetting the relationship between $x,y$. Doing this to a polynomial $p(x)$ with degree $kn$, we obtain a new polynomial with $x$-degree $n$ and $y$-degree $k$. Another way to view this is \u0026ndash; doing this process we obtain a new polynomial $P(y)$ whose coefficients are polynomials in $x$. The degree of $P(y)$ is $k$.\nTool 3: Chinese Remainder Theorem (CRT) Theorem (Chinese Remainder Theorem) Let $I, J$ be co-prime ideals in $R$ (meaning $I+J=R$). Then: $$ R/(IJ) \\cong (R/I) \\times (R/J) $$ The isomorphism is given by $z \\mapsto (z \\bmod I, z \\bmod J)$.\nThis allows us to perform arithmetic in the product ring $(R/I) \\times (R/J)$—which effectively parallelizes the computation—and reconstruct the result in $R/(IJ)$.\nRemark. Note that $I,J$ are co-prime iff $\\exists u\\in I,v\\in J$ such that $u+v=1$. The inverse CRT map $(R/I)\\times (R/J)\\to R/IJ$ is given by $(x,y)\\mapsto vx+uy$. Note the order of $v,u$ in the map is crucial! We mostly work over principal ideal domains, which are rings in which there are no-zero divisors and that every ideal $I$ is generated by a single element, $I=(a)=Ra$ for some $a\\in R$. Thus assuming $I=(a),J=(b)$, we have that $I+J=R$ iff and there are $r,s\\in R$ such that $ra+sb=1$. The elements $r,s$ are called Bezout\u0026rsquo;s coefficients.\nExample: Remainder integer arithmetic. To multiply in $\\mathbb{Z}_{(p-1)(p+1)}$ we can map to $\\mathbb{Z}_{p-1}\\times \\mathbb{Z}_{p+1}$, compute the product in the product ring and invert by the map $$(x,y)\\mapsto a(p+1)x+b(p-1)y,$$where $a,b$ are the unique numbers in $\\mathbb{Z}_{(p-1)(p+1)}$ satisfying $$a(p+1)+b(p-1)=1.$$\nExample: Polynomial remainder arithmetic. Consider the ring $R[x]/(x^{2}-x)$. We can factor $(x^{2}-x)=(x)(x-1)$, and these are co-prime ideals because taking $u=x\\in (x)$ and $v=-(x-1)=-x+1\\in (x-1)$ we see that $$u+v=x-x+1=1\\in R[x]/(x^{2}-x).$$Hence the inverse map for the isomorphism $z\\mapsto (z\\pmod{x},z\\pmod{(x-1)})$ is given by $$(a,b)\\mapsto va+ub=-ax+a+b=a+(b-a)x.$$ The formula suggests that inverting the CRT in this setup requires $2$ multiplications and $1$ addition.\nAlgorithms We now present the major algorithms using the framework above.\n1. Karatsuba\u0026rsquo;s Algorithm Consider multiplying two linear polynomials $a(x) = a_0 + a_1x$ and $b(x) = b_0 + b_1x$ in $R[x]$. The standard product requires 4 multiplications ($a_0b_0, a_0b_1, a_1b_0, a_1b_1$).\nKaratsuba\u0026rsquo;s trick reduces this to 3 using the CRT with ideals $(x)$ and $(x-1)$:\nMap: Reduce modulo $x^2-x$, $R[x]\\to R[x]/(x^2-x)$. For $a(x),b(x)$ this doesn\u0026rsquo;t do anything. CRT Projection: Map to $R[x]/(x^2-x)\\to (R[x]/x) \\times (R[x]/(x-1))$. $a(x) \\mapsto (a_0, a_0+a_1)$ $b(x) \\mapsto (b_0, b_0+b_1)$ Multiply: Compute pair-wise products: $P_0 = a_0 b_0$ $P_1 = (a_0+a_1)(b_0+b_1)$ Invert CRT: Reconstruct the polynomial in $R[x]/(x^2-x)$. The linear term coefficient is $P_1 -P_0$. Fix: Evaluate at $\\infty$, by computing $P_2=a_1b_1$ directly and returning $$P_0 +(P_1-P_0) x+ P_2 \\cdot (x^2-x)=P_0+(P_1-P_0-P_2)x+ P_2 x.$$ This requires only $3$ multiplications ($P_0, P_1, P_2$). It also requires $4$ addition and subtraction operations.\nVariation: Knuth\u0026rsquo;s Trick Instead of $x(x-1)$, we can use $x(x+1)$, mapping to evaluations at $0$ and $-1$. $$ (a_0, a_0-a_1) \\cdot (b_0, b_0-b_1) $$ The reconstruction logic is symmetric.\nApplication: Complex Multiplication We can identify $\\mathbb{C}$ with $\\mathbb{R}[i]/(i^2+1)$. Standard multiplication $(a+bi)(c+di)$ takes 4 real multiplications. Using a variation of the trick above (evaluating at $\\infty$ and $1$, essentially), we can do it in 3: compute $P_0=ac,P_1=(a+b)(c+d)$ and $P_2=bd$ and return $$ (ac-bd) + i[(a+b)(c+d) - ac - bd] =(P_0 -P_2)+ i(P_1-P_0 -P_2)$$\nApplication: Integer Multiplication Given two integers $a,b\\in\\mathbb{Z}$, which have $n$-digit long $B$-basis representation, we can:\nPerform base $B^{n/2}$-clumping to obtain two polynomials $$p(x)=a_{0}+a_{1}x\\quad,\\quad q(x)=b_{0}+b_{1}x$$such that $a=a_{0}+a_{1}B^{n/2}$ and $b=b_{0}+b_{1}B^{n/2}$. The numbers $a_0,a_1,b_0,b_1$ are all $n/2$-digit long (in basis $B$). Computing $p(x)\\cdot q(x)$ using Karatsuba\u0026rsquo;s trick, requires $3$ multiplications and $4$ additions of $n/2$-long numbers. Note that $$ab=a_{0}b_{0}+a_{1}b_{0}B^{n/2}+a_{0}b_{1}B^{n/2}+a_{1}b_{1}B^{n},$$so we can recover the result $ab$ by shifting the coefficients of the product polynomial by a correct number of digits, and sum up the results. This requires $3$ additions of numbers $2n$-long numbers (at most). By recursively applying this method, we see that the time to compute $n$-long multiplication is $$T(n)=3T(n/2)+O(n)$$ Using the master theorem, this leads to $T(n)=O(n^{\\log_2 3})$.\n2. Toom-Cook (Toom-$k$) Toom-Cook generalizes Karatsuba. Instead of splitting a number into 2 parts (degree 1 polynomial), we split it into $k$ parts (degree $k-1$ polynomial). Note that in $R[x]$, the ideals generated by $(x-\\alpha)$ and $(x-\\beta)$ (linear polynomials) are co-prime whenever $\\alpha\\not=\\beta$.\nProcedure:\nSetup: Choose $2k-1$ distinct points $\\alpha_1,\\ldots,\\alpha_{2k-1}\\in R$. CRT: Map $R[x] \\to \\prod_{i=1}^{2k-1} R[x]/(x - \\alpha_i)$. This is equivalent to evaluating the polynomial at these points. Multiply: Perform $2k-1$ recursive multiplications of the values. Invert CRT: Recover the product polynomial (degree $2k-2$) from the $2k-1$ point-value pairs. This step is often called interpolation. Runtime Analysis: We perform $2k-1$ recursive calls on inputs of size $n/k$. $$ T(n) = (2k-1)T(n/k) + O(n) $$ The solution is $T(n) = O(n^{\\log_k(2k-1)})$.\nFor $k=2$, $\\log_2 3 \\approx 1.58$. (Karatsuba) For $k=3$, $\\log_3 5 \\approx 1.46$. (Toom-3) As $k \\to \\infty$, the exponent approaches 1. 3. Fast Fourier Transform (FFT) The overhead of CRT and interpolation (inverting the CRT) in Toom-Cook grows rapidly with $k$. The FFT is essentially Toom-Cook where the evaluation points are roots of unity. The special structure and algebraic properties of roots of unity give a much faster algorithm for computing both the CRT and the inverse CRT.\nDefinition A primitive $n$-th root of unity is an element $\\zeta\\in R$ such that $\\zeta^n=1$ and for every $1\\le k\u0026lt;n$ it holds $\\zeta^k\\not=1$.\nFor example, in $\\mathbb{C}$, we have the complex exponent $\\exp(2\\pi i / n)$, which is a primitive $n$-th root of unity. Note that $\\zeta^k\\not=\\zeta^j$ for every $j\\not=k$ in $[1,n]$, otherwise that would mean (by exponent rules) $\\zeta^{k-j}=1$, contradicting the definition.\nTherefore, by CRT we have $$R[x]/(x^{n}-1)\\cong \\prod_{k=1}^n R[x]/(x-\\zeta^k).$$\nThe Procedure: Given two polynomials $p(x),q(x)\\in R[x]$ with degree at most $n-1$,\nMap: $R[x]\\to R[x]/(x^{2n}-1)$. This does nothing to $p,q$. CRT: $R[x]/(x^{2n}-1)\\to \\prod_{k=1}^{2n} R[x]/(x-\\zeta^k)$ assuming $\\zeta$ is a primitive $2n$-th root of unity. Multiply: in each factor separately, i.e., compute $p(\\zeta^k)\\cdot q(\\zeta^k)$ for every $k$. Inverse CRT: Interpolate using the inverse CRT map. Recover $p(x)\\cdot q(x)\\in R[x]/(x^{2n}-1)$. The Trick: The CRT map in this case is called the Discrete Fourier Transform and it can be computed fast, using the Cooley-Tukey algorithm, in time $O(n\\log n)$. Let us sketch the algorithm: Let $\\alpha\\in R$ denote an invertible element, and note $$R[x]/(x^{2n}-\\alpha^2)\\cong R[x]/(x^n-\\alpha)\\times R[x]/(x^n +\\alpha).$$ Note that $\\frac{1}{2\\alpha}(x^n+\\alpha)-\\frac{1}{2\\alpha}(x^n-\\alpha)=1$ and so the inverse CRT map is given by $$(p(x),q(x))\\mapsto \\frac{1}{2\\alpha}\\cdot [(x^n+\\alpha)\\cdot p(x)+(x^n-\\alpha)\\cdot q(x)]$$ The product $x^n\\cdot p(x)$ is just shifting a degree $\\le n-1$ polynomial by $n$ powers. So overall to compute the inverse CRT map we need $n$ multiplications in $R$ (namely multiplying $1/2\\alpha$ by the coefficients of $p,q$) and $2n$ additions of elements in $R$ (multiplied by $1/2$). If $\\alpha=\\zeta^{n}$ and $n$ is a power of two, we can apply this recursively to obtain a total operation count of $O(n\\log n)$ to invert the CRT. A similar approach gives the same runtime for the CRT map. This type of algorithm for computing the DFT in time $O(n\\log n)$ is called a Fast Fourier transform (FFT).\nThe case where $n$ is not a power of $2$ can be dealt with using Bluestein\u0026rsquo;s trick, which encodes the DFT as a polynomial multiplication, which can be computed using slightly larger power of $2$ FFTs. So the result of $O(n\\log n)$ carries over to the general case of $n$ not being a power of $2$.\nTo use the FFT for computing integer multiplication, we need a root of unity of the relevant order. Without it, the runtime is the same as in the Toom-Cook method. The next algorithm deals with this issue.\n4. Schönhage-Strassen Algorithm Standard FFT requires roots of unity. The ring of integers $\\mathbb{Z}$ does not have them. We must \u0026ldquo;manufacture\u0026rdquo; a ring that does.\nProcedure:\nBase $2^m$-Clumping: Break $N$-bit numbers into blocks of size $m$. Treat them as polynomials of degree $n-1$ where $N = mn$. Formally, $\\mathbb{Z}\\to \\mathbb{Z}[x]/(x-2^m)\\to \\mathbb{Z}[x]$. By construction, the coefficients of the polynomial are $m$-bit numbers, at most $2^m-1$. Map: Reduce modulo $(x^n +1)$, i.e., $\\mathbb{Z}[x]\\to \\mathbb{Z}[x]/(x^n+1)$. Map: Reduce modulo $(2^{nk}+1)$ where $k$ is some positive integer. In other words, identify $2^{nk}$ with $-1$. Formally, we reduce the coefficients of the polynomials $$\\mathbb{Z}[x]/(x^n +1)\\to (\\mathbb{Z}/(2^{nk}+1))[x]/(x^n+1).$$ Apply the FFT trick: Let $R=\\mathbb{Z}/(2^{nk}+1)$, then we want to compute a product in the ring $R[x]/(x^n+1)$. Note that $\\zeta=2^k$ is a primitive root of unity of order $2n$ in $R$, therefore we can apply the FFT trick to compute the product in $O(n\\log n)$ operations in $R$. Recursion: Apply the procedure recursively to compute products in $R$, which are $nk$-bit integers. When can we recover the result? In other words, when is this invertible?\nNote that polynomial multiplication modulo $x^n +1$ is just the nega-cyclic convolution of the coefficient vectors. In other words, if $p(x)=\\sum_{i=0}^{n-1}a_i x^i$ and $q(x)=\\sum_{i=0}^{n-1}b_i y^i$, then $$ \\begin{aligned} p(x)\\cdot q(x)\\mod (x^n+1)\u0026amp; =\\sum_{j=0}^{2n-2}\\left(\\sum_{i=0}^{j} a_i b_{j-i} \\right)x^j\\mod (x^n+1) \\\\ \u0026amp;=\\sum_{j=0}^{n-1}\\left(\\sum_{i=0}^{j} a_i b_{j-i} - \\sum_{i=j+1}^{n-1} a_{i} b_{(j-i)\\mod n}\\right) x^j \\end{aligned}$$ where we interpret indices out of range as $0$. In particular, the coefficient of $x^j$ is the sum of $n$ integer products where each product involves two numbers smaller than $2^m-1$. Therefore, the coefficients of this product polynomials are strictly smaller than $2^{2m}n$. Hence to ensure we can recover the correct result we must have $2^{nk}\\ge 2^{2m}n$. This implies $k\\ge \\frac{\\log_2(n)+2m}{n}$, and when the ratio of $m/n$ is fixed, we can pick $k$ to be a small constant. Note that the second and third steps don\u0026rsquo;t really change anything about the polynomials, but only move them between rings with different multiplication rules. To actually implement this, we need to choose a balance between $n,m$ and $k$.\nRuntime Analysis: Let $T(N)$ be the bit-complexity of multiplying two $N$-bit numbers. Choosing $n,m= \\sqrt{N}$ (without loss of generality assume $N$ has a square root), we reduce the problem to $\\sqrt{N}$ sub-problems of size $\\sqrt{N}$. In this setup, we can choose $k=3$ (assuming $n$ is large enough, this satisfies the condition above).\nSince multiplication by the root of unity ($2^k$) is just a shift of the binary representation, it can be done in linear time. Therefore the FFT requires $O(n\\log n)$ operations which are all linear (recall that FFT only uses multiplications by the root of unity and division by $2$, which is also a shift), i.e., cost $O(nk)$. Thus, the FFT runs in $$O(nk\\cdot n\\log n)=O(N\\log N)$$ Hence the runtime complexity satisfies the recurrence: $$ T(N) = \\sqrt{N} \\cdot T(\\sqrt{N}) + O(N \\log N) $$ This resolves to $T(N)=O(N\\log N\\log\\log N)$.\n5. Nussbaumer\u0026rsquo;s Trick When dealing with multi-variate polynomials, we can define the Multi-dimensional DFT, which is the CRT map: $$ R[x_{1},\\ldots,x_{d}]/(x_{1}^{t_{1}}-1,\\ldots,x_{d}^{t_{d}}-1)\\to \\prod_{(i_1, \\dots, i_d)} R[x_{1},\\ldots,x_{d}]/(x_{1}-\\xi_{1}^{i_{1}},\\ldots,x_{d}-\\xi_{d}^{i_{d}}) $$ where $\\xi_{j}$ is a primitive root of unity of order $t_{j}$. This is equivalent to computing the FFT along one axis (variable), and then by a second, and so on. If we choose $\\xi_{j}$ to be ring elements in $R$, the total number of ring multiplications is $\\Theta(t^{d}\\log (t^{d}))$. As we observed before, the multiplications done in the FFT are just multiplications by a power of $\\xi$ (and division by $2$, but this can be done at the end of the recursion for cheaper).\nHowever, suppose $t_{1}=\\ldots=t_{d}=t$. Note that $x_{1}$ is a suitable choice for $\\xi_{2},\\ldots,\\xi_{d}$, since it satisfies $x_1^{t}=1$. Suppose we start by applying the FFT along the last variable $x_{d}$, viewing the polynomial as having coefficients which are themselves polynomials in $x_1,\\ldots ,x_{d-1}$. Multiplying a polynomial in $x_1,\\ldots,x_{d-1}$ by by $x_{1}=\\xi_{d}$ is essentially a shift of the coefficients. Therefore, there are no ring multiplications when applying the FFT trick, on the last $d-1$ axes. The only ring multiplications happen at the base level of the recursion, which is $R[x_{1}]/(x_{1}^{t}-1)$.\nAt the base level of the recursion, there are $O(t\\log t)$ ring multiplications. Since every application of the FFT produces $t$ products in the ring, there are $t^{d-1}$ leaves in the recursion tree. Hence the total number of ring multiplications is: $$ O(t^{d}\\log t)=O\\left(\\frac{t^{d}\\log (t^{d})}{d}\\right) $$ This works even when $t_{1},\\ldots,t_{d}$ are not equal but close enough to each other. The implication is that we saved a factor of $d$ in the number of ring multiplications. If we are working on an integer multiplication algorithm, this saves a factor $d$ in the number of recursive calls to the algorithm.\nSince additions and shifts are much cheaper than ring multiplications (at least in the case of integers), this $d$-factor may be instrumental for the runtime. In the newest state-of-the-art algorithm, this trick is exploited.\nConclusion \u0026amp; Outlook We have journeyed from the $O(n^2)$ grade-school method to the $O(n \\log n \\log \\log n)$ of Schönhage-Strassen using purely algebraic insights: mapping, lifting, and the Chinese Remainder Theorem.\nFor decades, Schönhage-Strassen was the champion. In 2007, Martin Fürer introduced an algorithm running in $O(n \\log n \\cdot 2^{O(\\log^* n)})$. Finally, in 2019, Harvey and van der Hoeven achieved the holy grail: $O(n \\log n)$. While these modern algorithms rely on heavy analytic machinery (and valid over $\\mathbb{Z}$ specifically), the algebraic pillars we discussed here remain the foundation of computer algebra, and are just beautiful ideas!\nAppendix: Algebraic Definitions Definition: Ring A ring $R$ is a set equipped with two binary operations, addition ($+$) and multiplication ($\\cdot$), satisfying the following axioms:\n$(R, +)$ is an abelian group (associative, commutative, has an identity $0$, and every element has an additive inverse $-r$). Multiplication is associative: $(a\\cdot b)\\cdot c = a \\cdot (b \\cdot c)$. Distributivity holds: $a(b+c) = ab + ac$ and $(a+b)c = ac + bc$. If multiplication is commutative ($ab=ba$), $R$ is a commutative ring. If there exists an element $1 \\in R$ such that $1 \\cdot r = r \\cdot 1 = r$ for all $r$, $R$ is a ring with identity. In this post, we assume all rings are commutative with identity.\nDefinition: Field A field $\\mathbb{F}$ is a commutative ring with identity where every non-zero element $r \\in \\mathbb{F}$ has a multiplicative inverse $r^{-1}$ such that $r \\cdot r^{-1} = 1$. Examples include $\\mathbb{Q}, \\mathbb{R}, \\mathbb{C}$, and finite fields $\\mathbb{Z}_p$ for prime $p$.\nDefinition: Polynomial Ring Given a ring $R$, the polynomial ring $R[x]$ consists of formal sums $\\sum_{i=0}^n a_i x^i$ with coefficients $a_i \\in R$.\nThe degree of a polynomial, denoted $\\deg(p)$, is the largest $k$ such that $a_k \\neq 0$. A polynomial is monic if its leading coefficient $a_n$ is $1$. Definition: Homomorphism \u0026amp; Isomorphism A map $\\phi: R \\to S$ between rings is a ring homomorphism if it preserves the structure: $$ \\phi(a+b) = \\phi(a) + \\phi(b) \\quad \\text{and} \\quad \\phi(ab) = \\phi(a)\\phi(b) $$ If $\\phi$ is a bijection (one-to-one and onto), it is an isomorphism, denoted $R \\cong S$.\nDefinition: Ideals An ideal $I \\subseteq R$ is a subset that forms a subgroup under addition and is \u0026ldquo;sticky\u0026rdquo; under multiplication: for any $r \\in R$ and $x \\in I$, the product $rx \\in I$.\nPrincipal Ideal: An ideal generated by a single element $a$, denoted $(a) = {ra : r \\in R}$. Coprime Ideals: Two ideals $I, J$ are coprime if their sum is the whole ring, i.e., $I + J = R$. This implies there exist $u \\in I, v \\in J$ such that $u+v=1$. Definition: Quotient Ring Given an ideal $I \\subseteq R$, the quotient ring $R/I$ is the set of equivalence classes modulo $I$. The elements are of the form $a + I$, with operations: $$ (a+I) + (b+I) = (a+b) + I $$ $$ (a+I) \\cdot (b+I) = (ab) + I $$\nDefinition: Roots of Unity An element $\\omega \\in R$ is an $n$-th root of unity if $\\omega^n = 1$. It is a primitive $n$-th root of unity if it generates the cyclic subgroup of order $n$ under multiplication (often defined requiring $\\omega^k \\neq 1$ for all $1 \\le k \u0026lt; n$ and that $n$ is invertible in $R$).\nReferences Karatsuba, A. and Ofman, Y. (1962). Multiplication of Many-Digital Numbers by Automatic Computers . Toom, A. L. (1963). The Complexity of a Scheme of Functional Elements Realizing the Multiplication of Integers . Schönhage, A. and Strassen, V. (1971). Schnelle Multiplikation großer Zahlen . Harvey, D. and van der Hoeven, J. (2021). Integer multiplication in time $O(n \\log n)$ . ","title":"Algebraic Techniques for Fast Integer Multiplication"},{"link":"/posts/fft-finite-field/","text":" Prerequisites Ring Theory: Basic definitions (Rings, Fields, Polynomials). Group Theory: Cyclic groups and generators. Basic FFT: Familiarity with the standard divide-and-conquer strategy on $\\mathbb{C}$ is helpful but not required. The Fast Fourier Transform (FFT) is one of the most important algorithms in history. Usually, it is introduced over the field of complex numbers $\\mathbb{C}$, relying on geometric intuition about roots of unity lying on the unit circle.\nHowever, in Computer Science—particularly in Cryptography and Coding Theory—we often care about Finite Fields $\\mathbb{F}_q$. The geometric intuition fades, but the algebraic structure remains. In this post, we will explore how to perform efficient Fourier Transforms over any ring, leading us to advanced techniques like Bluestein\u0026rsquo;s trick and the Schönhage-Strassen algorithm for polynomial\nThroughout the post $R$ will denote an arbitrary ring with a unit, $\\mathbb{F}p$ a field of prime size $p$, and $\\mathbb{F}{q}$ a finite size (we know $q=p^k$ for some prime $p$).\nFoundations: Roots of Unity There are many ways to derive the Discrete Fourier Transform (DFT). We will use a formulation that keeps a distance from representation-theoretic roots and works for any ring.\nDefinition: Principal Root of Unity Let $R$ be any ring and $n \\ge 1$ an integer. We say $\\alpha \\in R$ is a principal $n$-th root of unity if:\n$\\alpha^n = 1$. $\\sum_{j=0}^{n-1} \\alpha^{jk} = 0$ for every $1 \\le k \u0026lt; n$. Another definition is that of a primitive root of unity \u0026ndash; if $\\alpha^n=1$ and $\\alpha^k\\not=1$ for every $1\\le k\u0026lt;n$.\nIn fields, a primitive root of unity is always a principal root of unity (so it\u0026rsquo;s a stronger definition).\nDefinition: The DFT Let $\\alpha\\in R$ be a principal $n$-th root of unity. Given a tuple $(v_0, \\dots, v_{n-1})$ of elements in $R$, the Discrete Fourier Transform is the tuple $(f_0, \\dots, f_{n-1})$ where: $$ f_k = \\sum_{j=0}^{n-1} v_j \\alpha^{jk} $$ Equivalently, if $P(x) = \\sum v_j x^j$, the DFT is the evaluation tuple $(P(\\alpha^0), \\dots, P(\\alpha^{n-1}))$.\nIn fields, a primitive root of unity is always a principal root of unity. To work over finite fields $\\mathbb{F}_q$, we rely on the following structural fact:\nFact: Structure of $\\mathbb{F}_q^\\times$ Let $\\mathbb{F}_q^{\\times}$ denote the set $\\mathbb{F}_q\\setminus \\set{0}$. Since this is a field, every non-zero element is invertible, so this is a group under the multiplication operation. The crucial fact is that $\\mathbb{F}_q^{\\times}$ is a cyclic group of order $q-1$. This means there exists a generator $\\gamma \\in \\mathbb{F}_q^\\times$ such that $\\gamma^{q-1} = 1$, but $\\gamma^j \\neq 1$ for any $1 \\le j \u0026lt; q-1$.\nThus $\\mathbb{F}_q$ contains a primitive $n$-th root of unity if and only if $n$ divides $q-1$.\nThe \u0026ldquo;Easy\u0026rdquo; Case: Radix-2 FFT If the ring $R$ has a primitive root of unity of order $2^k$, then we can compute the DFT fast, using a divide-and-conquer approach which is almost identical to the complex case. In fact, it is the same algorithm, but we abstract away details about $\\mathbb{C}$. Note that $\\mathbb{C}$ has primitive roots of unity of any order. For a finite field $\\mathbb{F}_q$ to support this algorithm we need $2^k\\mid q-1$ ($2^k$ divides $q-1$), using the observations above.\nTheorem (Radix-2 FFT) Let $R$ be a ring with a $2^k$-th principal root of unity $\\alpha \\in R$. Then the DFT of size $n=2^k$ for a polynomial $P \\in R[x]$ (degree $\u0026lt; n$) can be computed in time $O(n \\log n)$.\nProof Sketch: The idea is the standard Cooley-Tukey algorithm. We decompose the evaluation of $P(x)=\\sum a_i x^i$ into evaluations of its even and odd parts: $$ P(x) = P_{\\text{even}}(x^2) + x \\cdot P_{\\text{odd}}(x^2) $$ where $P_{\\text{even}}(y) = \\sum a_{2i} y^i$ and $P_{\\text{odd}}(y) = \\sum a_{2i+1} y^i$. Since evaluating at ${1, \\alpha, \\dots, \\alpha^{n-1}}$ involves squaring the points, the set of evaluation points for the sub-problems reduces to ${1, \\alpha^2, \\dots, \\alpha^{2(n/2)-1}}$, which is exactly the set of powers of $\\alpha^2$, a principal root of unity of order $n/2=2^{k-1}$. This follows from the fact $$(\\alpha^j)^2=\\alpha^{2j}=\\begin{cases} \\alpha^{2j} \u0026amp; j\u0026lt; n/2, \\\\ \\alpha^{2j-n}\\alpha^n=\\alpha^{2j -n} \u0026amp; j\\ge n/2,\\end{cases}$$ using $\\alpha^n=1$. Hence we can recover the DFT by computing two $n/2$ DFTs for $P_{\\text{even}},P_{\\text{odd}}$ and combining the results in linear time. The runtime recurrence is $T(n) = 2T(n/2) + O(n)$, yielding $T(n) = O(n \\log n)$. $\\blacksquare$\nThis algorithm is called an FFT (fast Fourier transform). A similar algorithm works in the inverse direction, yielding the inverse FFT algorithm. Using the famous Convolution Theorem, this allows us to multiply polynomials efficiently.\nCorollary Let $R$ be a ring with a $2^k$-th primitive root of unity. Given polynomials $f, g \\in R[x]$ with degree $\u0026lt; n = 2^k$, we can compute their product $h \\equiv f \\cdot g \\pmod{x^n - 1}$ in time $O(n \\log n)$ using two forward FFTs and one Inverse FFT.\nProof Sketch: Every polynomial of degree $\u0026lt;n$ is uniquely determined by its values on $n$ distinct points. Moreover, the value of the product polynomial $f\\cdot g$ at a point $\\xi\\in R$ is just $f(\\xi)\\cdot g(\\xi)$. Let $h=f\\cdot g\\mod (x^n-1)$, and note it can be written as $h(x)=(f(x)\\cdot g(x)) - (x^n -1)\\cdot q(x)$ where $q$ is the quotient polynomial (and $h$ is the remainder). Note that $h$ has degree $\u0026lt;n$. The value of $h$ on a $n$-th root of unity $\\alpha$ (not a principal necessarily) is $$h(\\alpha)=(f(\\alpha)\\cdot g(\\alpha))- (\\alpha^n -1)\\cdot q(\\alpha)=f(\\alpha)\\cdot g(\\alpha)$$ since $\\alpha^n-1=0$ as $\\alpha$ is a root of unity. Therefore $h$ is uniquely determined by the values $$(f(1)\\cdot g(1), f(\\alpha)\\cdot g(\\alpha),\\ldots , f(\\alpha^{n-1})\\cdot g(\\alpha^{n-1}))$$ where $\\alpha$ is a principal $n$-th root of unity. Computing the values requires computing the DFT of $f$ and $g$, taking the element-wise products (in linear time), and then applying the inverse DFT to obtain $h$ (this is interpolation). Overall, this takes $O(n\\log n)$ time. $\\blacksquare$\nFFT of General Size What if we want to compute the DFT of size $n$ where $n$ is not a power of $2$? Then the Radix-2 FFT cannot be used directly. Bluestein\u0026rsquo;s trick is a way to solve this problem, by encoding the DFT of size $n$ as polynomial multiplication, which can be computed fast by DFTs of slightly larger size, which is a power of $2$.\nTheorem: Bluestein\u0026rsquo;s Trick Let $\\alpha\\in R$ denote a principal $2n$-th root of unity. Given a polynomial $P(x)\\in R[x]$ with degree $\u0026lt;n$, write $P(x)=\\sum_{i=0}^{n-1}a_i x^i$. Then the DFT of $P$, denoted $f=(P(1),P(\\alpha^2),\\ldots,P(\\alpha^{2(n-1)}))$, can be computed via the multiplication of $$p(y)=\\sum_{i=0}^{n-1}a_i \\alpha^{i^2}\\cdot y^i\\quad \\text{by} \\quad q(y)=\\sum_{i=1}^{2n-1}\\alpha^{-(n-i)^2}y^i$$ In particular, $f_i=P(\\alpha^{2i})=\\alpha^{i^2}\\cdot b_i$ where $b_i$ is the coefficient of $y^i$ in $p(y)\\cdot q(y)$.\nProof. By definition $$p(y)\\cdot q(y)=\\sum_{k=0}^{3n-2} \\left(\\sum_{i=0}^{n-1}a_i \\alpha^{i^2}\\cdot \\alpha^{-(n-(k-i))^2}\\right)y^k$$Noting that $i^2 - (n-(k-i))^2=i^2 -n^2 +2n(k-i)-k^2+2ki-i^2 = n(2(k-i)-n) -k^2 +2ki$ and since $\\alpha^n=1$, the sum simplifies to $$p(y)\\cdot q(y)=\\sum_{k=0}^{3n-2} \\alpha^{-k^2} \\cdot \\left(\\sum_{i=0}^{n-1}a_i (\\alpha^{2k})^{i}\\right)y^k=\\sum_{k=0}^{n-1} \\alpha^{-k^2}\\cdot P(\\alpha^{2k})\\cdot y^k +\\sum_{k=n}^{3n-2}(\\ldots)y^k,$$and so the coefficient of $y^k$ for $k\u0026lt;n$ is just $\\alpha^{-k^2}\\cdot f_k$. $\\blacksquare$\nCorollary If $R$ has a $2n$-th principal root of unity and also a $2^k$-th principal root of unity, where $2^k\\ge 3n-1$, the DFT of size $n$ can be computed in time $O(2^k k)=O(n\\log n)$.\nProof. It reduces to computing a polynomial multiplication of two polynomials whose product degree is at most $3n-2$. It can be computed using two FFTs of size $2^k$ and one inverse FFT. We can choose $k$ to be minimal, because if $R$ has a $2^k$-th root of unity $\\zeta$, then $\\zeta^2$ is a $2^{k-1}$-th root of unity. Hence $2^k\\le 2(3n-1)=O(n)$. $\\blacksquare$\nRings without Roots of Unity What if the ring $R$ doesn\u0026rsquo;t have the required root of unity to perform a DFT? To solve this we use the Schönhage-Strassen trick.\nTo multiply polynomials efficiently without a native root of unity, we construct a ring extension that has one. In the polynomial ring $R[x]$, we can \u0026ldquo;manufacture\u0026rdquo; a root of unity of order $2n$ by working in the quotient ring $R[x] / (x^{2n}-1)$. In this ring, $x^{2n} \\equiv 1$.\nTheorem (Schönhage-Strassen) The product of two polynomials $f,g\\in R[x]$, of degree $\u0026lt;n$, where $n=2^k$ can be computed in time $O(n \\log n \\log \\log n)$, even if $R$ doesn\u0026rsquo;t contain a root of unity of order $2n$.\nProof Sketch: We use a recursive technique involving Kronecker Substitution (also called segmentation). Note that $\\deg(f\\cdot g)\\le 2n-2$, so computing $f \\cdot g \\pmod{x^{2n}-1}$ yields the exact product. Let $m = 2^{\\lfloor k/2 \\rfloor}$ and $t = n/m$. Note that $n = m \\cdot t$. (Also, $t=2m$ if $k$ is odd, while $t=m$ if $k$ is even).\nSubstitution: We introduce a new variable $y$ and identify it with $x^m$. Explicitly, we map indices $i \\to (u, v)$ such that $i = u + v \\cdot m$ (where $0\\le u\u0026lt;m$). This maps a univariate polynomial $f(x)$ to a bivariate polynomial $f\u0026rsquo;(x, y)$ such that $f(x) = f\u0026rsquo;(x, x^m)$. The resulting polynomials have $x$-degree $\u0026lt; m$ and $y$-degree $\u0026lt; t$.\nThe Recursive Ring: Define the ring $D = R[x] / (x^{2m} + 1)$. This ring contains a $4m$-th root of unity (the element $x$, since $x^{2m} \\equiv -1$). We view $f\u0026rsquo;, g\u0026rsquo;$ as polynomials in $y$ with coefficients in $D$.\nNote: The coefficients of $f\u0026rsquo;, g\u0026rsquo;$ are polynomials in $x$ with degree $\u0026lt;m$. Their product has degree $\u0026lt; 2m$. Thus, multiplication in $D$ (modulo $x^{2m}+1$) yields the exact product of coefficients, as no wrap-around occurs. Choosing a Root: The ring $D$ contains a $2t$-th primitive root of unity, which we denote $\\eta$:\nIf $t = 2m$, take $\\eta = x$ (order $4m = 2t$). If $t = m$, take $\\eta = x^2$ (order $2m = 2t$). FFT in the Extension: Compute the convolution of $f\u0026rsquo;$ and $g\u0026rsquo;$ modulo $(y^{2t} - 1)$. Since we have a root $\\eta$ of order $2t$, we can use the Radix-2 FFT (recall $t$ is a power of $2$). This gives a polynomial $h^*(x, y)$ with $\\deg_y h^* \u0026lt; 2t$ such that: $$ f\u0026rsquo; \\cdot g\u0026rsquo; \\equiv h^* \\pmod{(y^{2t} - 1)} $$\nReconstruction: Lift $h^*$ to a univariate polynomial $h \\in R[x]/(x^{2n}-1)$ by substituting $y=x^m$.\nThe correctness relies on the \u0026ldquo;spacing\u0026rdquo; provided by the substitution:\nNo Aliasing in $x$: We viewed the coefficients as elements of $D = R[x]/(x^{2m}+1)$. Since the input coefficients had degree $\u0026lt; m$, their point-wise product has degree $\u0026lt; 2m$. The modulus $x^{2m}+1$ is large enough to prevent these products from wrapping around and mixing with each other incorrectly. No Aliasing in $y$: Since the $y$-degrees are at most $t-1$, their products are $2t-2\u0026lt;2t$ and so by computing the product modulo $y^{2t}-1$ we have essentially computed the exact product (similar to the previous point). Thus, the evaluation $h^*(x, x^m)=f\u0026rsquo;(x,x^m)\\cdot g\u0026rsquo;(x,x^m)=f(x)\\cdot g(x)$, and the reconstruction gives $h$ for which $h(x)=h^{*}(x,x^m)$. Complexity: The runtime $T(k)$ satisfies the recurrence: $$ T(k) \\le 2^{\\lceil k/2 \\rceil} T(\\lfloor k/2 \\rfloor + 1) + O(2^k \\cdot k) $$ The first term accounts for the recursive multiplications, and the second for the FFT additions/shifts at the current level. This solves to $T(n) = O(n \\log n \\log \\log n)$. $\\blacksquare$\nCorollary The DFT of size $n$ for a polynomial $f\\in R[x]$ can be computed even in time $O(n\\log n\\log\\log n)$ even if $R$ doesn\u0026rsquo;t have a root of unity of order $n$ (or $2n$).\nProof. Use Bluestein\u0026rsquo;s trick to reduce the problem to a polynomial multiplication, which can be computed fast using the Schonhage-Strassen algorithm. $\\blacksquare$\nApplication: Multipoint Evaluation We can combine these tools to solve a classic problem: evaluating a polynomial at every point in a finite field $\\mathbb{F}_p$. Recall that Fermat\u0026rsquo;s little theorem says that $$a^{p-1}\\equiv 1\\pmod{p}$$for every $a\\not=0$ in $\\mathbb{F}_p$. Therefore, the polynomial $x^p-x$ is equivalent to $0$ (as a function) over $\\mathbb{F}_p$.\nUnivariate Evaluation Corollary Given a polynomial $P \\in \\mathbb{F}_p[x]$ of degree $d$, we can evaluate $P$ on every $\\alpha \\in \\mathbb{F}_p$ in time $O(d + p \\cdot \\text{poly}(\\log p))$.\nProof:\nReduction: In $\\mathbb{F}_p$, $x^p - x \\equiv 0$ for all elements (Fermat\u0026rsquo;s Little Theorem). We first reduce $P$ to $\\widetilde{P} \\equiv P \\pmod{x^{p-1}-1}$. This takes $O(d)$ time by folding coefficients: $a_i$ contributes to $a_{i \\pmod{p-1}}$. Evaluation via DFT: The values $\\widetilde{P}(1), \\dots, \\widetilde{P}(p-1)$ correspond exactly to the DFT of the coefficient vector of $\\widetilde{P}$ over the cyclic group $\\mathbb{F}_p^{\\times}$ (we take the generator of the group which is a $p-1$-th primitive root of unity). Even if $p-1$ is not a power of $2$, we can use Bluestein\u0026rsquo;s Trick combined with Schönhage-Strassen to compute this DFT in $O(p \\log p \\log \\log p)$ time. Zero Case: $\\widetilde{P}(0)$ is simply the constant term $a_0$, computed in $O(1)$. $\\blacksquare$ Multivariate Evaluation This result extends powerfully to multivariate polynomials.\nCorollary Given $P \\in \\mathbb{F}_p[x_1, \\dots, x_m]$ with individual degrees $\u0026lt; d$, we can evaluate $P$ on every $\\alpha \\in \\mathbb{F}_p^m$ in time $O(d^m + m \\cdot p^m \\cdot \\text{poly}(\\log p))$.\nProof: The first step is reduce the polynomial modulo $x_i^p -x_i$ for every $i$. This gives a new polynomial $\\tilde{P}$ with individual degrees $\u0026lt;p$. This reduction takes $O(d^m)$ because it requires folding the $O(d^m)$ coefficients onto $p^m$ coefficients. Therefore, we assume $P=\\tilde{P}$ has individual degrees $\u0026lt;p$.\nWe proceed by induction on $m$. Base Case ($m=1$): the previous corollary.\nInductive Step: Let $R = \\mathbb{F}_p[x_1, \\dots, x_{m-1}]$. We can write $P$ as a polynomial in $x_m$ with coefficients in $R$: $$ P(x_1, \\dots, x_m) = \\sum_{i=0}^{p-1} Q_i(x_1, \\dots, x_{m-1}) \\cdot x_m^i $$\nEvaluate Coefficients: By the induction hypothesis, evaluate each $Q_i$ on all points in $\\mathbb{F}_p^{m-1}$.\nThere are $p$ such polynomials ($Q_0, \\dots, Q_{p-1}$). Cost: $p \\times O((m-1) p^{m-1} \\text{poly}(\\log p)) = O((m-1) p^m \\text{poly}(\\log p))$. Fix the Prefix: For every fixed tuple $\\vec{\\alpha} = (\\alpha_1, \\dots, \\alpha_{m-1}) \\in \\mathbb{F}_p^{m-1}$, we have a univariate polynomial in $x_m$: $$ P_{\\vec{\\alpha}}(x_m) = \\sum_{i=0}^{p-1} Q_i(\\vec{\\alpha}) \\cdot x_m^i $$ The coefficients $Q_i(\\vec{\\alpha})$ were computed in step 1.\nUnivariate Sweep: For each of the $p^{m-1}$ vectors $\\vec{\\alpha}$, we evaluate $P_{\\vec{\\alpha}}$ on all $p$ values of $x_m$ using the fast univariate algorithm.\nCost: $p^{m-1} \\times O(p \\cdot \\text{poly}(\\log p)) = O(p^m \\text{poly}(\\log p))$. Total Time: Summing the steps yields $O(m \\cdot p^m \\cdot \\text{poly}(\\log p))$. $\\blacksquare$\nReferences Cooley, J. W., \u0026amp; Tukey, J. W. (1965). An algorithm for the machine calculation of complex Fourier series . Bluestein, L. (1970). A linear filtering approach to the computation of discrete Fourier transform . Schönhage, A., \u0026amp; Strassen, V. (1971). Schnelle Multiplikation großer Zahlen . von zur Gathen, J. \u0026amp; Gerhard, J. (2013). Modern Computer Algebra . ","title":"Fast Fourier Transform over Finite Fields"},{"link":"/posts/fmm-4/","text":" Prerequisites Part 3: Familiarity with Tensor Rank, Direct Sums, and Tensor Products. Combinatorics: Basic understanding of the Multinomial Theorem. Asymptotic Analysis: Limits and roots of polynomials. In this post, we will formalize the tools needed to compare different tensor algorithms and prove the powerful $\\tau$-Theorem (also known as the Asymptotic Sum Inequality). This theorem is the engine behind many modern improvements in the exponent of matrix multiplication, allowing us to derive bounds on $\\omega$ from sums of disparate tensors.\nTensor Similarity In linear algebra, linear operators are abstract objects. To represent an operator using a matrix, one must fix a basis. Consequently, we have the notion of matrix similarity: two matrices represent the same operator if they differ only by a change of basis ($A = PBP^{-1}$). This can be extended to rectangular matrices as $A=PBQ^{-1}$ where $P,Q$ are squares of different sizes.\nWe need an analogous notion for tensors.\nDefinition Two tensors $\\mathcal{T}$ and $\\mathcal{T}\u0026rsquo;$ over vector spaces $U,V,W$ are called similar (denoted $\\mathcal{T}\\cong \\mathcal{T}\u0026rsquo;$) if there exist invertible linear maps $f_{U}:U\\to U$, $f_{V}:V\\to V$, and $f_{W}:W\\to W$ such that: $$ (f_{U} \\otimes f_{V} \\otimes f_{W})(\\mathcal{T}) = \\mathcal{T}\u0026rsquo; $$\nUsing the notation from the previous post, this implies both $\\mathcal{T}\\le \\mathcal{T}\u0026rsquo;$ and $\\mathcal{T}\u0026rsquo;\\le \\mathcal{T}$. Since rank is preserved under restriction:\nCorollary If $\\mathcal{T}\\cong \\mathcal{T}\u0026rsquo;$, then $R(\\mathcal{T})=R(\\mathcal{T}\u0026rsquo;)$.\nWe can extend this definition to include more general transformations. This is often referred to as De Groote Equivalence.\nDefinition (De Groote Equivalence) Two bilinear algorithms $\\langle \\mathbf{U}_{1},\\mathbf{V}_{1},\\mathbf{W}_{1} \\rangle$ and $\\langle \\mathbf{U}_{2},\\mathbf{V}_2,\\mathbf{W}_{2} \\rangle$ with the same inner dimension $r$ are equivalent if one can be obtained from the other by a sequence of:\nScaling: $\\langle \\mathbf{U}_{1},\\mathbf{V}_{1},\\mathbf{W}_{1} \\rangle = \\langle D_{1} \\mathbf{U}_{2}, D_{2} \\mathbf{V}_{2}, D_{3}\\mathbf{W}_{2} \\rangle$ for diagonal matrices $D_i$ satisfying $D_1 D_2 D_3 = I_r$. Permutation: Permuting the rows of the matrices (reordering the $r$ multiplications). Change of Basis: Applying the tensor similarity transformations defined above. It is easy to show why this is an equivalence relation. It generalizes the tensor equivalence definition, because it allows applying a simple transformation on the tensors, instead of on each separate component. More specifically, change of basis operates on the input spaces $U,V,W$, which correspond to the columns of the algorithm matrices. De-Groote equivalence allows to operate on the rows of the algorithms, by scaling or permutation. This equivalence relation allows researchers to search for algorithms with \u0026ldquo;nicer\u0026rdquo; properties (e.g., sparsity, integer coefficients) without changing the rank.\nDirect Sum and Tensor Product How do the direct sum ($\\oplus$) and tensor product ($\\otimes$) interact?\nLemma (Distributivity) $$ (\\mathcal{T}_{1}\\oplus \\mathcal{T}_{2})\\otimes \\mathcal{T}_{3} \\cong (\\mathcal{T}_{1}\\otimes \\mathcal{T}_{3})\\oplus (\\mathcal{T}_{2}\\otimes \\mathcal{T}_{3}) $$ $$ \\mathcal{T}_{1}\\otimes (\\mathcal{T}_{2}\\oplus \\mathcal{T}_{3}) \\cong (\\mathcal{T}_{1}\\otimes \\mathcal{T}_{2})\\oplus (\\mathcal{T}_{1}\\otimes \\mathcal{T}_{3}) $$\nProof. Consider the right distributivity. Let the variables (bases) for $\\mathcal{T}_{1}$ be ${X_{i},Y_j,Z_\\ell}$, for $\\mathcal{T}_{2}$ be ${X^{\\prime}_{i\u0026rsquo;},Y^{\\prime}_{j\u0026rsquo;},Z^{\\prime}_{\\ell\u0026rsquo;}}$, and for $\\mathcal{T}_{3}$ be ${A_{k},B_{n},C_{m}}$. The direct sum $\\mathcal{T}_{1}\\oplus \\mathcal{T}_{2}$ acts on the disjoint union of the first two variable sets. When we tensor with $\\mathcal{T}_3$, the indices of the resulting tensor are combinations of $(\\text{Index}_{\\mathcal{T}_1 \\oplus \\mathcal{T}_2}, \\text{Index}_{\\mathcal{T}_3})$. Because the indices of $\\mathcal{T}_1$ and $\\mathcal{T}_2$ never mix in the direct sum, the resulting cross-terms with $\\mathcal{T}_3$ also separate perfectly into two disjoint blocks. This is exactly the definition of the direct sum on the right-hand side. $\\blacksquare$\nWe also need the concept of a Unit Tensor.\nDefinition Let $k \\in \\mathbb{N}$. Define the unit tensor $\\langle k \\rangle$ over $\\mathbb{F}^{k}\\otimes \\mathbb{F}^{k}\\otimes \\mathbb{F}^k$ as the tensor with $1$ on the main diagonal and $0$ elsewhere: $$ T_{i,j,\\ell} = \\delta_{i,j} \\delta_{j,\\ell} $$ Explicitly: $\\langle k \\rangle = \\sum_{i=1}^k e_i \\otimes e_i \\otimes e_i$.\nLemma $R(\\mathcal{T}) \\le r$ if and only if $\\mathcal{T} \\le \\langle r \\rangle$.\nProof. Forward ($\\Rightarrow$): If $R(\\mathcal{T}) \\le r$, then $\\mathcal{T} = \\sum_{i=1}^r u_i \\otimes v_i \\otimes w_i$. Define the map $f_U: \\mathbb{F}^r \\to U$ by $e_i \\mapsto u_i$, and similarly for $V, W$. Then $\\mathcal{T} = (f_U \\otimes f_V \\otimes f_W)(\\langle r \\rangle)$, which means $\\mathcal{T} \\le \\langle r \\rangle$.\nBackward ($\\Leftarrow$): Since restriction implies $R(\\mathcal{T}) \\le R(\\langle r \\rangle)$, it suffices to prove $R(\\langle r \\rangle) = r$. Clearly $R(\\langle r\\rangle)\\le r$ so we are left proving $R(\\langle r\\rangle)\\ge r$. Suppose $\\langle r \\rangle = \\sum_{j=1}^s u_j \\otimes v_j \\otimes w_j$. For $k=1,\\ldots ,r$, define $f_k:\\mathbb{F}^k\\to \\mathbb{F}^k$ by extending $f_k(e_j)=\\delta_{k,j}$ linearly (so it is $1$ for $e_k$ and $0$ for the other standard basis vectors). Applying $\\mathrm{Id}\\otimes f_k \\otimes f_k$ on $\\langle r\\rangle$ gives $$(\\mathrm{Id}\\otimes f_k\\otimes f_k)(\\langle r\\rangle)=\\sum_{j=1}^r e_j \\otimes f_k (e_j )\\otimes f_k (e_j)=e_k\\otimes e_k \\otimes e_k$$ On the other hand, applying the map to $u_j \\otimes v_j\\otimes w_j$ gives $\\alpha_j \\beta_j \\cdot u_j \\otimes e_k \\otimes e_k$ where $\\alpha_j,\\beta_j$ are the $k$-th coordinates of $v_j,w_j$ respectively. Therefore, applying the map to $\\langle r\\rangle$ gives the equation $$\\sum_{j=1}^s \\alpha_j\\beta_j\\cdot u_j\\otimes e_k \\otimes e_k = \\left(\\sum_{j=1}^s \\alpha_j\\beta_j u_j\\right) \\otimes e_k \\otimes e_k =e_k \\otimes e_k \\otimes e_k$$ In particular, $e_k$ is in the linear span of $u_1,\\ldots,u_s$, and this is true for every $k$, concluding that $s\\ge r$ by dimension considerations. $\\blacksquare$\nThe $\\tau$-Theorem We now arrive at the main theorem of this post. Previously, we bounded $\\omega$ by analyzing a single matrix multiplication tensor. But what if we have an algorithm that computes multiple, independent matrix multiplications simultaneously?\nThe tensor $\\bigoplus_{i=1}^{k}\\langle n_{i},m_{i},p_{i} \\rangle$ corresponds to computing $k$ independent matrix products $(A_1 B_1, \\dots, A_k B_k)$.\nTheorem (The $\\tau$-Theorem / Asymptotic Sum Inequality) Suppose there exists a bilinear algorithm for the direct sum of matrix multiplication tensors such that: $$ R\\left(\\bigoplus_{i=1}^{k} \\langle n_{i},m_{i},p_{i} \\rangle\\right) \\le r $$ Then $\\omega$ satisfies the inequality: $$ \\sum_{i=1}^{k} (n_{i} m_{i} p_{i})^{\\omega/3} \\le r $$\nThis implies that if $\\tau$ is the solution to $\\sum_{i} (n_i m_i p_i)^{\\tau} = r$, then $\\omega \\le 3\\tau$.\nThe Simple Case Let\u0026rsquo;s first prove it for the case where all blocks have equal size: $n_i=n, m_i=m, p_i=p$. By distributivity, the direct sum is a tensor product with a unit tensor: $$ \\bigoplus_{i=1}^{k}\\langle n,m,p \\rangle \\cong \\langle k \\rangle \\otimes \\langle n,m,p \\rangle $$ The theorem claims that if $R(\\langle k \\rangle \\otimes \\langle n,m,p \\rangle) \\le r$, then $k (nmp)^{\\omega/3} \\le r$.\nLemma If $R(\\langle k \\rangle \\otimes \\langle n,m,p \\rangle) \\le r$, then for every integer $t \\ge 1$: $$ R(\\langle k \\rangle \\otimes \\langle n^{t},m^{t},p^{t} \\rangle) \\le k \\cdot \\lceil r/k \\rceil^{t} $$\nProof. The proof is by induction on $t$. The base case $t=1$ holds trivially, because $r\\le k\\cdot \\lceil r/k\\rceil$. For the inductive step, assume the assertion holds for $t-1$ and we wish to prove for $t$ By distributivity: $$ \\langle k \\rangle \\otimes \\langle n^{t},m^{t},p^{t} \\rangle \\cong (\\langle k \\rangle \\otimes \\langle n,m,p \\rangle) \\otimes \\langle n^{t-1},m^{t-1},p^{t-1} \\rangle \\le \\langle r \\rangle \\otimes \\langle n^{t-1},m^{t-1},p^{t-1} \\rangle $$ since $\\langle k\\rangle \\otimes \\langle n,m,p\\rangle \\le \\langle r \\rangle$. Ovserve that $\\langle r \\rangle \\le \\langle k \\lceil r/k \\rceil \\rangle \\cong \\langle \\lceil r/k \\rceil \\rangle \\otimes \\langle k \\rangle$. By the induction hypothesis we conclude $$ R(\\langle k\\rangle\\otimes \\langle n^t,m^t,p^t\\rangle)\\le R( \\langle \\lceil r/k \\rceil \\rangle \\otimes \\langle k \\rangle \\otimes \\langle n^{t-1},m^{t-1},p^{t-1} \\rangle) \\le \\lceil r/k \\rceil \\cdot \\left( k \\cdot \\lceil r/k \\rceil^{t-1} \\right) = k \\cdot \\lceil r/k \\rceil^t $$ $\\blacksquare$\nCorollary If $R(\\left\\langle k \\right\\rangle\\otimes \\left\\langle n,m,p \\right\\rangle)\\le r$ then $\\omega\\le \\frac{3\\log \\left\\lceil r/k\\right\\rceil}{\\log nmp}$.\nProof. Recall a previous result we\u0026rsquo;ve proved in earlier posts: $$R(\\langle a,b,c\\rangle)\\le r \\implies \\omega\\le \\log_{abc}(r^3)$$For this we used symmetrization. From the Lemma above, we have an algorithm for size $(n^t, m^t, p^t)$—repeated $k$ times—with rank roughly $(r/k)^t \\cdot k$. Since we can restrict to a single copy, we obtain $$\\langle n^t,m^t,p^t\\rangle\\le \\langle k\\rangle \\otimes \\langle n^t,m^t,p^t\\rangle\\implies R(\\langle n^t,m^t,p^t\\rangle)\\le \\lceil r/k\\rceil^t\\cdot k$$ concluding that $$ \\omega \\le \\frac{3\\log(k \\cdot \\lceil r/k \\rceil^t)}{\\log(n^t m^t p^t)} = 3\\frac{t \\log \\lceil r/k \\rceil + \\log k}{t \\log (nmp)} $$ Taking the limit as $t \\to \\infty$, the $\\log k$ term vanishes, yielding: $$ \\omega \\le \\frac{3 \\log(\\lceil r/k\\rceil)}{\\log(nmp)}. $$ $\\blacksquare$\nGeneral Case We now want to obtain the general $\\tau$-theorem from the simple case. The idea is roughly the following:\nInstead of working with a direct sum, we can generalize to work with a weighted direct sum: $$\\bigoplus_{i=1}^{k}\\langle k_i\\rangle \\otimes \\langle n_i,m_i,p_i\\rangle$$ So instead of the $i$-th element showing up once, we now add multiplicity (given by $k_i$). Notice that for any fixed $j$, we have $\\langle k_{j} \\rangle \\otimes \\langle n_{j},m_{j},p_{j} \\rangle \\le \\bigoplus_{i=1}^{k}\\langle k_{i} \\rangle \\otimes \\langle n_{i},m_{i},p_{i} \\rangle$. This is done simply by zeroing out all variables not associated with the $j$-th element. Therefore, $R(\\langle k_{j} \\rangle \\otimes \\langle n_{j},m_{j},p_{j} \\rangle) \\le R(\\bigoplus_{i=1}^{k} \\langle k_{i} \\rangle \\otimes \\langle n_{i},m_{i},p_{i} \\rangle)$. If we have an upper bound on the right-hand side, we can apply the simple case. But how can we ensure that choosing a specific $j$ will not be too lossy? In the $\\tau$-theorem, we are given $k_{i}=1$, so the best we can do is choose the largest individual tensor. However, if we take high tensor powers first, we can do much better.\nBy distributivity, the binomial formula (or rather, multinomial formula) holds for tensors. $$ \\begin{aligned} \\left(\\bigoplus_{i=1}^{k}\\langle n_{i},m_{i},p_{i} \\rangle\\right)^{\\otimes s} \u0026amp;= \\bigoplus_{a_{1}+\\dots+a_{k}=s} \\langle \\mathcal{M}_{\\vec{a}} \\rangle \\otimes \\left(\\bigotimes_{i=1}^{k}\\langle n_{i},m_{i},p_{i} \\rangle^{\\otimes a_{i}}\\right) \\\\ \u0026amp;= \\bigoplus_{a_{1}+\\dots+a_{k}=s} \\langle \\mathcal{M}_{\\vec{a}} \\rangle \\otimes \\left\\langle \\prod_{i=1}^{k}n_{i}^{a_{i}},\\prod_{i=1}^{k} m_{i}^{a_{i}},\\prod_{i=1}^{k}p_{i}^{a_{i}} \\right\\rangle \\end{aligned} $$ Here $\\mathcal{M}_{\\vec{a}} = \\binom{s}{a_{1},\\dots,a_{k}}$ is the multinomial coefficient.\nThis derivation works because the binomial formula relies on just two algebraic properties, both of which hold for tensors:\nDistributivity: $(\\mathcal{A} \\oplus \\mathcal{B}) \\otimes \\mathcal{C} \\cong (\\mathcal{A} \\otimes \\mathcal{C}) \\oplus (\\mathcal{B} \\otimes \\mathcal{C})$. Scalar Multiplication: Adding a tensor $\\mathcal{T}$ to itself $t$ times in a direct sum is equivalent to $\\langle t \\rangle \\otimes \\mathcal{T}$. Intuition: The Multinomial Peak Why does this help? The highlight of the proof is that the binomial formula is uneven. As we take higher powers ($s \\to \\infty$), the multinomial coefficients deviate significantly in size.\nThink of the multinomial distribution. Although the probability mass shrinks (due to normalization), the \u0026ldquo;peak\u0026rdquo; of the distribution becomes much higher relative to the \u0026ldquo;surroundings\u0026rdquo; (the tails).\nThis is visualized in the following plot, where we normalize the probability of the multinomial distribution over $3$ variables that sum to $n$. We vary the values of $n$ so the concentration becomes apparent. By taking $s$ to be large, we ensure there is a choice of a single tensor in the sum that captures the \u0026ldquo;bulk\u0026rdquo; of the complexity, allowing us to apply the simple case without \u0026ldquo;losing too much rank\u0026rdquo;.\nProof of the $\\tau$-Theorem. Denote $\\mathcal{T}=\\bigoplus_{i=1}^{k}\\langle n_{i},m_{i},p_{i} \\rangle$. We are given $R(\\mathcal{T}) \\le r$. Consider the $s$-th power $\\mathcal{T}^{\\otimes s}$. By the sub-multiplicativity of rank, $R(\\mathcal{T}^{\\otimes s})\\le r^{s}$.\nUsing the expansion derived above, for any fixed choice of exponents $\\vec{a}=(a_{1},\\dots,a_{k})$ summing to $s$, the specific term in the direct sum is a restriction of the whole. Thus: $$ R\\left (\\langle \\mathcal{M}_{\\vec{a}} \\rangle \\otimes \\left\\langle \\prod_{i=1}^{k}n_{i}^{a_{i}}, \\prod_{i=1}^{k}m_{i}^{a_{i}},\\prod_{i=1}^{k}p_{i}^{a_{i}} \\right\\rangle\\right) \\le R(\\mathcal{T}^{\\otimes s}) \\le r^{s} $$ This is exactly the setup for the Simple Case of the $\\tau$-theorem (where the \u0026ldquo;multiplicity\u0026rdquo; is the multinomial coefficient). Applying the simple case result: $$ \\mathcal{M}_{\\vec{a}} \\cdot \\left(\\prod_{i=1}^{k}(n_{i}m_{i}p_{i})^{a_{i}}\\right)^{\\omega/3} \\le r^{s} $$ This inequality holds for every valid sequence $\\vec{a}$. Summing over all possible choices of $\\vec {a}$, and by the scalar multinomial theorem, we obtain: $$ \\sum_{a_{1}+\\dots+a_{k}=s} \\binom{s}{a_{1},\\dots,a_{k}} \\cdot \\prod_{i=1}^{k}\\left((n_{i}m_{i}p_{i})^{\\omega/3}\\right)^{a_{i}} = \\left(\\sum_{i=1}^{k}(n_{i}m_{i}p_{i})^{\\omega/3}\\right)^{s} $$\nThe number of terms in the sum is equal to the number of solutions to $a_1 + \\dots + a_k = s$, which is $\\binom{k+s-1}{s-1}$. Thus, by averaging, there must exist at least one choice of $\\vec{a}$ for which the term is at least the average value: $$ \\text{Max Term} \\ge \\frac{\\text{Sum}}{\\text{Count}} \\implies r^s \\ge \\text{Max Term} \\ge \\frac{\\left(\\sum_{i=1}^{k}(n_{i}m_{i}p_{i})^{\\omega/3}\\right)^{s}}{\\binom{k+s-1}{s-1}} $$ Re-arranging and taking the $s$-th root, we see $$ \\sum_{i=1}^{k}(n_{i}m_{i}p_{i})^{\\omega/3} \\le r \\cdot \\sqrt[s]{\\binom{k+s-1}{s-1}} $$ Note that $\\binom{k+s-1}{s-1}$ is a polynomial in $s$ of degree $k-1$. Specifically, it is roughly $s^{k-1}/(k-1)!$. Since $k$ is constant with respect to $s$, and it holds $\\lim_{s\\to \\infty} \\sqrt[s]{\\text{Poly}_k (s)} = 1$ for any fixed polynomial of degree $k$, taking the limit of the inequality above $s \\to \\infty$ yields: $$ \\sum_{i=1}^{k}(n_{i}m_{i}p_{i})^{\\omega/3} \\le r $$ $\\blacksquare$\nReferences Bürgisser, P., Clausen, M., \u0026amp; Shokrollahi, M. A. (1997). Algebraic Complexity Theory . Bläser, M. (2013). Fast Matrix Multiplication . 3 Schönhage, A. (1981). Partial and Total Matrix Multiplication . Alman, J. Lecture Notes . ","title":"Fast Matrix Multiplication - Part 4"},{"link":"/posts/fmm-3/","text":" Prerequisites Part 2 of this series: Familiarity with the tensor product of vector spaces and bilinear algorithms. Linear Algebra: Vector spaces, Bases, Dual spaces, and Tensor Products (basic definition). In this post, we continue building the foundation for fast matrix multiplication algorithms. We will discuss essential tensor operations—product, sum, and restriction—and establish the Triple Product Condition, setting the stage for the group-theoretic approach.\nTensor Operations - Tensor Product Recall the definition of a 3-d tensor as a weighted sum of simple tensors in a space $U\\otimes V \\otimes W$: $$ \\mathcal{T}=\\sum_{i}\\sum_{j}\\sum _{\\ell}T_{i,j,\\ell}(u_{i}\\otimes v_{j} \\otimes w_{\\ell}) $$ where ${u_{i}},{v_{j}},{w_{\\ell}}$ are bases for $U,V,W$ respectively.\nDefinition The tensor product of two 3-d tensors $\\mathcal{T}_1$ (over $U_{1}\\otimes V_{1}\\otimes W_{1}$) and $\\mathcal{T}_2$ (over $U_{2}\\otimes V_{2}\\otimes W_{2}$) is a tensor $\\mathcal{T}_{1}\\otimes \\mathcal{T}_{2}$ over the space: $$ \\underbrace{(U_{1}\\otimes U_{2})}_{U}\\otimes\\underbrace{(V_{1}\\otimes V_{2})}_{V }\\otimes\\underbrace{(W_{1}\\otimes W_{2})}_{W} $$ It is defined by the formula: $$ \\mathcal{T}_{1}\\otimes \\mathcal{T}_{2}=\\sum_{\\substack{i,i\u0026rsquo;,j,j\u0026rsquo;\\ell,\\ell\u0026rsquo;}} (T_{i,j,\\ell}^{1} \\cdot T^{2}_{i\u0026rsquo;,j\u0026rsquo;,\\ell\u0026rsquo;}) \\cdot [(u_{i}\\otimes u_{i\u0026rsquo;})\\otimes (v_{j}\\otimes v_{j\u0026rsquo;})\\otimes (w_{\\ell}\\otimes w_{\\ell\u0026rsquo;})] $$ In other words, it is a simple tensor over $6$ vector spaces which we fold to a $3$-d tensor by treating each pair $U_1\\otimes U_2$, $V_1\\otimes V_2$ and $W_1\\otimes W_2$ as the underlying vector spaces (forgetting the fact they too are tensor spaces).\nWe can apply this to the specific tensors representing matrix multiplication.\nLemma Let $\\langle n,m,p \\rangle$ denote the tensor corresponding to the multiplication of $n \\times m$ and $m \\times p$ matrices. Then: $$ \\langle n_{1},m_{1},p_{1} \\rangle \\otimes \\langle n_{2},m_{2},p_{2} \\rangle \\cong \\langle n_{1}n_{2},m_{1}m_{2},p_{1}p_{2} \\rangle $$\nProof. Our vector spaces are $U_{k}=\\mathbb{F}^{n_{k}\\times m_{k}}$, $V_{k}= \\mathbb{F}^{m_{k} \\times p_{k}}$, and $W_{k}=\\mathbb{F}^{n_{k}\\times p_{k}}$ for $k=1,2$. First, observe the isomorphism of the tensor product of matrix spaces: $$ U_{1}\\otimes U_{2}=\\mathbb{F}^{n_{1}\\times m_{1}}\\otimes \\mathbb{F}^{n_{2}\\times m_{2}}\\cong \\mathbb{F}^{n_{1}n_{2}\\times m_{1}m_{2}} $$ This isomorphism is given explicitly by the Kronecker product map: $$ E_{i,k} \\otimes E_{i\u0026rsquo;,k\u0026rsquo;} \\mapsto E_{i n_2 + i\u0026rsquo;, k m_2 + k\u0026rsquo;} $$ Using double indices for the bases, and recalling the definition of the matrix multiplication tensor, we see that non-zero products of coefficients of both tensors are: $$ T^{1}_{(i,k),(k,j),(i,j)}\\cdot T^{2}_{(i\u0026rsquo;,k\u0026rsquo;),(k\u0026rsquo;,j\u0026rsquo;),(i\u0026rsquo;,j\u0026rsquo;)}=1\\cdot 1$$ Hence the non-zero coefficients of $\\langle n_1,m_1,p_1\\rangle \\otimes \\langle n_2,m_2,p_2\\rangle$ are the coefficients of $$(E_{i,k}\\otimes E_{k,j}\\otimes E_{i,j})\\otimes (E_{i\u0026rsquo;,k\u0026rsquo;}\\otimes E_{k\u0026rsquo;,j\u0026rsquo;}\\otimes E_{i\u0026rsquo;,j\u0026rsquo;})\\in (U_1\\otimes V_1\\otimes W_1)\\otimes (U_2 \\otimes V_2\\otimes W_2)$$ which is mapped to $$E_{in_2 +i\u0026rsquo;, km_2 +k\u0026rsquo;}\\otimes E_{km_2 + k\u0026rsquo;, j p_2 + j\u0026rsquo;} \\otimes E_{in_2+i\u0026rsquo;,j p_2 +j\u0026rsquo;} \\in (U_1\\otimes U_2)\\otimes (V_1\\otimes V_2)\\otimes (W_1 \\otimes W_2)$$ This is exactly the matrix multiplication tensor of size $\\langle n_1n_2 ,m_1m_2,p_1p_2\\rangle$. $\\blacksquare$\nThe most important property of the tensor product regarding complexity is:\nLemma Tensor rank is sub-multiplicative under tensor products. $$ R(\\mathcal{T}_{1}\\otimes \\mathcal{T}_{2})\\le R(\\mathcal{T}_{1})\\cdot R(\\mathcal{T}_{2}) $$\nProof. This follows directly from the definition. If $\\mathcal{T}_1 = \\sum_{r=1}^{R_1} \\mathbf{t}^1_r$ and $\\mathcal{T}_2 = \\sum_{s=1}^{R_2} \\mathbf{t}^2_s$ are optimal decompositions into simple tensors, then $\\mathcal{T}_1 \\otimes \\mathcal{T}_2 = \\sum_{r,s} \\mathbf{t}^1_r \\otimes \\mathbf{t}^2_s$ is a decomposition into $R_1 R_2$ simple tensors. $\\blacksquare$\nPermutations and Symmetrization We wish to discuss certain helpful properties of the matrix multiplication tensor. We start with the following lemma:\nLemma (Trace Formula) Consider the polynomial $P$ for the matrix multiplication tensor $\\langle n,m,p\\rangle$. Let $X$ denote the $X_{i,k}$ variables ordered as a matrix, and similarly let $Y,Z$ denote the variable matrices. Then $$\\langle n,m,p\\rangle =\\mathrm{Tr}(XYZ^{\\top})$$\nProof. By definition $$\\mathrm{Tr}(XYZ^{\\top})=\\sum_{i} [XYZ^{\\top}]_{i,i}=\\sum_{i}\\sum_{j,k} X_{i,k}Y_{k,j} Z^{\\top}_{j,i}=\\sum_{i,j,k}X_{i,k}Y_{k,j}Z_{i,j}=\\langle n,m,p\\rangle$$\nLemma (Permutation Invariance) For every permutation $\\sigma:[3]\\to [3]$ of the dimensions $(n_0, n_1, n_2)$, it holds that: $$ R(\\langle n_{0},n_{1},n_{2} \\rangle)=R(\\langle n_{\\sigma(0)},n_{\\sigma(1)},n_{\\sigma(2)} \\rangle) $$\nProof. It suffices to prove this for the generators of the permutation group $S_3$: the cyclic shift $(012)$ and the transposition $(02)$. Let $(\\mathbf{U},\\mathbf{V},\\mathbf{W})$ be an algorithm (decomposition) for $\\langle n_{0},n_{1},n_{2} \\rangle$.\nCyclic Shift $\\sigma=(012)$: We want to show $R(\\langle n_{1},n_{2},n_{0} \\rangle) = R(\\langle n_{0},n_{1},n_{2} \\rangle)$. Recall that cyclic property of the trace implies $\\text{Tr}(ABC) = \\text{Tr}(BCA)$. Thus, letting $X$ denote the $n_0\\times n_1$ variable matrix, $Y$ denote the $n_1\\times n_2$ variable matrix and $Z$ denote the $n_0\\times n_2$ variable matrix, we have by the trace formula: $$\\langle n_0,n_1,n_2\\rangle = \\mathrm{Tr}(XYZ^{\\top})=\\mathrm{Tr}(YZ^{\\top}X)$$ Note that $Y$ has shape $n_1\\times n_2$, $Z^{\\top}$ has shape $n_2\\times n_0$ and $X$ has shape $n_0\\times n_1$. Therefore by filling in the values of an input $n_1\\times n_2$ matrix $A$ into $Y$ variables, the values of the second input $n_2\\times n_0$ matrix into $Z^{\\top}$, we can read the value of $(AB)_{(i,j)}$ from the coefficient of the variable $X_{j,i}$. More specifically, the formula above computes $(AB)^{\\top}$, and moreover provides a bilinear algorithm with the same inner dimension. Note that by permuting the rows of a bilinear algorithm we can deal with a transposed input or output, which is just a change of basis. Thus we have obtained an algorithm for $\\langle n_1,n_2,n_0\\rangle$ with the same rank.\nTransposition $\\sigma=(02)$: We want to show $R(\\langle n_{2},n_{1},n_{0} \\rangle) = R(\\langle n_{0},n_{1},n_{2} \\rangle)$. This corresponds to the fact that $(AB)^\\top = B^\\top A^\\top$. Indeed, $\\langle n_2, n_1, n_0 \\rangle$ represents the multiplication of an $n_2 \\times n_1$ matrix by an $n_1 \\times n_0$ matrix. By identifying the spaces via the transpose map (swapping row/column indices in the basis), the tensor remains structurally identical. Explicitly, define $\\widetilde{\\mathbf{U}}$ by taking $\\mathbf{W}$ and re-ordering rows according to the transpose order, and $\\widetilde{\\mathbf{W}}$ by taking $\\mathbf{U}$ in transpose order. Then $(\\widetilde{\\mathbf{W}}, \\widetilde{\\mathbf{V}}, \\widetilde{\\mathbf{U}})$ is an algorithm for the permuted tensor. $\\blacksquare$\nThis leads to a powerful reduction technique:\nProposition (Symmetrization) If $R(\\langle n,m,p \\rangle) \\le r$, then: $$ \\omega \\le \\log_{nmp}(r^{3}) = \\log_{\\sqrt[3]{nmp}}(r) $$ In the square case $n=m=p$, this recovers the result $\\omega \\le \\log_n r$.\nProof. Suppose $R(\\langle n,m,p \\rangle) \\le r$. By the Lemma above, the rank is invariant under permutations, so $R(\\langle m,p,n \\rangle) \\le r$ and $R(\\langle p,n,m \\rangle) \\le r$. Using the sub-multiplicative property: $$ R( \\langle n,m,p \\rangle \\otimes \\langle m,p,n \\rangle \\otimes \\langle p,n,m \\rangle ) \\le r \\cdot r \\cdot r = r^3 $$ However, the tensor product of these three is isomorphic to $\\langle nmp, nmp, nmp \\rangle$. Thus: $$ R(\\langle nmp, nmp, nmp \\rangle) \\le r^3 $$ By a corollary from the previous post, we obtain the upper bound $$ \\omega \\le \\log_{nmp} (R(\\langle nmp, nmp, nmp \\rangle)) \\le \\log_{nmp}(r^3) $$ $\\blacksquare$\nTensor Operations - Direct Sum Definition Let $\\mathcal{T}_{1}$ (over $U_{1}\\otimes V_{1}\\otimes W_{1}$) and $\\mathcal{T}_{2}$ (over $U_{2}\\otimes V_{2} \\otimes W_{2}$) be tensors. Their direct sum $\\mathcal{T}_{1}\\oplus \\mathcal{T}_{2}$ is a tensor over $(U_{1}\\oplus U_{2})\\otimes (V_{1}\\oplus V_{2}) \\otimes (W_{1} \\oplus W_{2})$, given by: $$ \\begin{aligned} \\mathcal{T}_{1}\\oplus \\mathcal{T}_{2} \u0026amp;= \\sum_{i,j,\\ell} T_{i,j,\\ell}^{1}[(u^{1}_{i} \\oplus 0)\\otimes (v_{j}^{1} \\oplus 0) \\otimes (w_{\\ell}^{1} \\oplus 0)] \\\\ \u0026amp;+ \\sum_{i\u0026rsquo;,j\u0026rsquo;,\\ell\u0026rsquo;} T_{i\u0026rsquo;,j\u0026rsquo;,\\ell\u0026rsquo;}^{2}[(0\\oplus u_{i\u0026rsquo;}^{2})\\otimes (0\\oplus v_{j\u0026rsquo;}^{2}) \\otimes(0\\oplus w_{\\ell\u0026rsquo;}^{2})] \\end{aligned} $$\nIn words, we embed the variables of $\\mathcal{T}_{1}$ and $\\mathcal{T}_2$ into disjoint subspaces. The direct sum acts like $\\mathcal{T}_1$ on the first subspace and $\\mathcal{T}_2$ on the second, with zero interaction between them. The following lemma follows easily from definition:\nLemma Tensor rank is sub-additive under direct sums: $$ R(\\mathcal{T}_{1}\\oplus \\mathcal{T}_{2})\\le R(\\mathcal{T}_{1})+ R(\\mathcal{T}_{2}) $$\nRemark. Note that $\\langle n_{1},m_{1},p_{1} \\rangle \\oplus \\langle n_{2},m_{2},p_{2} \\rangle \\neq \\langle n_{1}+n_{2},m_{1}+m_{2},p_{1}+p_{2} \\rangle$. The direct sum corresponds to performing two independent matrix multiplications side-by-side, not multiplying two larger block matrices.\nTensor Restriction Definition We say that a tensor $\\mathcal{T}$ restricts to $\\mathcal{T}\u0026rsquo;$, denoted by $\\mathcal{T}\u0026rsquo;\\le \\mathcal{T}$, if there exist linear maps $f_{U}:U\\to U\u0026rsquo;$, $f_{V}:V\\to V\u0026rsquo;$, $f_{W}:W\\to W\u0026rsquo;$ such that: $$ (f_{U}\\otimes f_{V} \\otimes f_{W})(\\mathcal{T}) = \\mathcal{T}\u0026rsquo; $$\nRecall that given linear functions $f:U\\to X,g:V\\to Y$ we can define $f\\otimes g:U\\otimes V\\to X\\otimes Y$ by defining $$(f\\otimes g)(u,v)=(f(u))\\otimes (g(v))$$ and noting this definition is bilinear and thus extends to $U\\otimes V$ (by the universal property).\nLemma If $\\mathcal{T}\u0026rsquo;\\le \\mathcal{T}$, then $R(\\mathcal{T}\u0026rsquo;)\\le R(\\mathcal{T})$.\nProof. If $\\mathcal{T} = \\sum_{s=1}^r u_s \\otimes v_s \\otimes w_s$, applying the linear maps element-wise yields: $$ \\mathcal{T}\u0026rsquo; = \\sum_{s=1}^r f_U(u_s) \\otimes f_V(v_s) \\otimes f_W(w_s) $$ This is a valid decomposition for $\\mathcal{T}\u0026rsquo;$ of size $r$ (though a better one might exist). $\\blacksquare$\nExample: Consider the tensor over $\\mathbb{R}^{3}\\otimes \\mathbb{R}^{2}\\otimes \\mathbb{R}^{2}$ given by: $$ \\mathcal{T}=\\sum_{i=0}^{2}x_{i}y_{0}z_{0}+\\sum_{j=0}^{1}x_{0}y_{i}z_{i} $$ Let $f_{U}$ be the projection that zeros out $x_0$: $f(\\alpha_{0} x_{0}+\\alpha_{1}x_{1}+\\alpha_{2} x_{2})=\\alpha_{1}x_{1}+\\alpha_{2}x_{2}$. Let $f_{V}, f_W$ be identity maps. Then $(f_{U}\\otimes f_{V} \\otimes f_{W})(\\mathcal{T}) = x_{1}y_{0}z_{0}+x_{2}y_0z_{0}$. This result is isomorphic to a tensor in a smaller space, showing we can \u0026ldquo;restrict\u0026rdquo; tensors to simpler forms by projecting out variables.\nTriple Product Condition We finish this post with a precise characterization of the matrix multiplication tensor using the columns of the algorithm matrices.\nDefinition Let $u,v,w \\in \\mathbb{F}^r$. We define their triple product by: $$ \\langle u,v,w \\rangle = \\sum_{s=1}^{r} u_{s}v_{s}w_{s} $$\nTheorem (Triple Product Condition) Let $(\\mathbf{U},\\mathbf{V},\\mathbf{W})$ be a bilinear algorithm with inner dimension $r$. Let the columns of these matrices be indexed by the basis elements of the matrix spaces (e.g., $\\mathbf{U}_{*, (i,k)}$ is the column corresponding to the matrix entry $A_{i,k}$). The algorithm computes Matrix Multiplication $\\langle n,m,p \\rangle$ if and only if: $$ \\langle \\mathbf{U}_{*,(i,k)},\\mathbf{V}_{*,(k\u0026rsquo;,j)}, \\mathbf{W}_{*,(i\u0026rsquo;,j\u0026rsquo;)} \\rangle = \\begin{cases} 1 \u0026amp; i=i\u0026rsquo; \\land k=k\u0026rsquo; \\land j=j\u0026rsquo; \\\\ 0 \u0026amp; \\text{else} \\end{cases} $$\nProof. Recall that the tensor for the algorithm $(\\mathbf{U},\\mathbf{V},\\mathbf{W})$ is given by: $$ \\mathcal{T}_{alg} = \\sum_{s=1}^r (\\mathbf{u}_s \\otimes \\mathbf{v}_s \\otimes \\mathbf{w}_s) $$ where $\\mathbf{u}_s$ are the rows of $\\mathbf{U}$. The coefficient of this tensor at the index tuple $((i,k), (k\u0026rsquo;,j), (i\u0026rsquo;,j\u0026rsquo;))$ is exactly: $$ \\sum_{s=1}^r (\\mathbf{U})_{s, (i,k)} (\\mathbf{V})_{s, (k\u0026rsquo;,j)} (\\mathbf{W})_{s, (i\u0026rsquo;,j\u0026rsquo;)} = \\langle \\mathbf{U}_{*,(i,k)},\\mathbf{V}_{*,(k\u0026rsquo;,j)}, \\mathbf{W}_{*,(i\u0026rsquo;,j\u0026rsquo;)} \\rangle $$ On the other hand, the definition of the Matrix Multiplication tensor $\\langle n,m,p \\rangle$ is that the coefficient is $1$ if the indices match $A_{ik} B_{kj} = C_{ij}$ (i.e., $k=k\u0026rsquo;$, $i=i\u0026rsquo;$, $j=j\u0026rsquo;$) and $0$ otherwise. Equating the coefficients of $\\mathcal{T}_{alg}$ and $\\langle n,m,p \\rangle$ yields the condition. $\\blacksquare$\nReferences Bürgisser, P., Clausen, M., \u0026amp; Shokrollahi, M. A. (1997). Algebraic Complexity Theory . Bläser, M. (2013). Fast Matrix Multiplication . ","title":"Fast Matrix Multiplication - Part 3"},{"link":"/posts/fmm-2/","text":" Prerequisites Part 1 of this series: Familiarity with standard Matrix Multiplication (MM) and $\\omega$. Linear Algebra: Vector spaces, Bases, Dual spaces, and Tensor Products (basic definition). Abstract Algebra: Fields and Polynomial rings (helpful for the tensor intuition). In the previous post, we discussed Strassen\u0026rsquo;s algorithm and the definition of the exponent $\\omega$. We demonstrated that the complexity of matrix multiplication is dominated by the number of multiplications in the base algorithm, assuming a recursive approach.\nIn this post, we will introduce bilinear algorithms, their tensor representations, and how these concepts fundamentally relate to fast matrix multiplication.\nQuick Recap on Tensor Products Given two vector spaces $V,U$, a standard operation is to consider their direct sum, $V\\oplus U$, which is a vector space structure over the Cartesian product $V\\times U$, where addition and scalar multiplication is done separately in each coordinate (so $(v,u)+(v\u0026rsquo;,u\u0026rsquo;)=(v+v\u0026rsquo;, u+u\u0026rsquo;)$ and similarly for scalar multiplication). This is the simplest way to combine vector spaces. Note that $\\dim (V\\oplus U)=\\dim V+\\dim U$.\nAnother way to combine spaces is via the tensor product, denoted $V\\otimes U$. We will first need the following definition of a bilinear function.\nDefinition Let $U,V,W$ be vector spaces over a field $\\mathbb{F}$. A function $f:U\\times V\\to W$ is called bilinear if it is linear in each argument separately. That is, for all $u,u\u0026rsquo;\\in U$, $v,v\u0026rsquo;\\in V$, and $\\alpha,\\beta\\in \\mathbb{F}$: $$ \\begin{aligned} f(\\alpha u+\\beta u\u0026rsquo;,v) \u0026amp;= \\alpha f(u,v) + \\beta f(u\u0026rsquo;,v) \\\\ f(u,\\alpha v+\\beta v\u0026rsquo;) \u0026amp;= \\alpha f(u,v) + \\beta f(u,v\u0026rsquo;) \\end{aligned} $$\nAn example of a bilinear function is the matrix multiplication function over $U=\\mathbb{F}^{n\\times m},V=\\mathbb{F}^{m\\times p}$ and $W=\\mathbb{F}^{n\\times p}$, given by $f(A,B)=AB$.\nTo define $V\\otimes U$, we consider a new vector space $W$ which is the linear span of elements in $V\\times U$, treating each couple $(v,u)$ as being linearly independent from other couples. We then define the bilinear relations on $W$, which is an equivalence relation on the elements of $W$, defined by: $$(\\alpha v+\\beta v\u0026rsquo;, \\gamma u+ \\delta u\u0026rsquo;)\\sim \\alpha\\cdot \\gamma (v,u)+ \\beta\\cdot \\gamma (v\u0026rsquo;,u)+ \\alpha\\cdot \\delta(v,u\u0026rsquo;) + \\beta\\cdot \\delta(v\u0026rsquo;,u\u0026rsquo;)$$ Now, we define $V\\otimes U$ to be the quotient space of $W$ by the equivalence. This defines a new vector space, where equivalent vectors are treated as identical vectors.\nWe define $\\otimes: V\\times U\\to V\\otimes U$ by setting $(v,u)\\mapsto [(v,u)]_{\\sim}$ and we denote the equivalence class by $v\\otimes u$. Note that by construction, $\\otimes$ is a bilinear map, since the equivalence relation ensures for example $$[(\\alpha v,u)]_{\\sim}=\\alpha \\cdot [(v,u)]_\\sim \\implies (\\alpha v)\\otimes u= \\alpha (v\\otimes u)$$\nIn the case of vector spaces, if $v_1,\\ldots,v_n$ is a basis for $V$ and $u_1,\\ldots,u_m$ for $U$, then we can think of $V\\otimes U$ to be the linear span of the vectors $\\lbrace v_{i}\\otimes u_j\\rbrace_{i\\in [n],j\\in [m]}$. Indeed, for $v\\in V,u\\in U$ we can write $v=\\sum_{i=1}^n \\alpha_i v_i$ and $u=\\sum_{j=1}^m \\beta_j u_j$ to obtain that $$v\\otimes u=\\sum_{i,j}\\alpha_i \\beta_j (v_i\\otimes u_j)$$ It is also easy to see that the vectors $v_i \\otimes u_j$ are linearly independent from each other. Thus $\\dim (V\\otimes U)=\\dim V\\cdot \\dim U$.\nThe Universal Property of the Tensor Product is the following observation: Every bilinear map $f:V\\times U\\to X$ extends in a unique manner to a linear map $\\tilde{f}:V\\otimes U\\to X$, which satisfies $f(v,u)=\\tilde{f}(v\\otimes u)$.\nThis property gives a recipe to construct linear maps from the tensor product space. Start with a bilinear map on $V\\times U$, and extend linearly to $V\\otimes U$. In particular, given a bilinear function $f:V\\times U\\to X$, the extension must be $\\tilde{f}\\left( \\sum_{i,j} \\alpha_{i,j} v_i\\otimes u_j\\right)=\\sum_{i,j}\\alpha_{i,j} f(v_i,u_j)$.\nSpecial Case of Matrix Spaces If $U,V$ are both matrix spaces, i.e. $U=\\mathcal{M}_n(\\mathbb{F})$ and $V=\\mathcal{M}_m(\\mathbb{F})$, then $V\\otimes U$ has a special interpretation. Taking the standard basis matrices $E_{i,j}$ of size $n\\times n$ and $F_{i,j}$ of size $m\\times m$, the basis for $V\\otimes U$ is $F_{k,\\ell}\\otimes E_{i,j}$. We can think of this basis vector as the matrix of size $nm\\times nm$ with $E_{i,j}$ in the $(k,\\ell)$-th block of size $n\\times n$. In other words, it has $1$ in row $k\\cdot n + i$ and column $\\ell\\cdot n +j$, and zeros everywhere else. For general matrices, expanding them in the respective bases, we recover the Kronecker product of matrices, defined by $$\\mathbf{A}\\in V,\\mathbf{B}\\in U:\\quad A\\otimes B = \\begin{pmatrix} A_{0,0} \\mathbf{B} \u0026amp; \\cdots \u0026amp; A_{0,m-1} \\mathbf{B} \\\\ \\vdots \u0026amp;\\ddots \\\\ A_{m-1,0} \\mathbf{B} \u0026amp; \\cdots \u0026amp; A_{m-1,m-1}\\mathbf{B}\\end{pmatrix}$$ It is easy to see that $V\\otimes U$ is therefore just $\\mathcal{M}_{mn}(\\mathbb{F})$.\nBilinear Algorithms Strassen proved that any bilinear function can be computed using a specific structural recipe. This is often called the Strassen Normal Form (or Rank Decomposition).\nTheorem (Strassen Normal Form) Fix bases for $U,V,W$. A function $f:U\\times V\\to W$ is bilinear if and only if there exists a vector space $X$ (of dimension $r$) and linear maps (matrices) $\\mathbf{U}: U \\to X$, $\\mathbf{V}: V \\to X$, and $\\mathbf{W}: W \\to X$ such that: $$f(u,v)=\\mathbf{W}^{\\top}(\\mathbf{U}u\\odot \\mathbf{V}v)$$ where $\\odot$ denotes the element-wise product of vectors in $X$ (Hadamard product) relative to a fixed basis.\nProof. First, we assume $f$ has the form $f(u,v)=\\mathbf{W}^{\\top}(\\mathbf{U}u\\odot \\mathbf{V}v)$ and show it is bilinear. Indeed, $$ \\begin{aligned} f(u+u\u0026rsquo;,v) \u0026amp;= \\mathbf{W}^{\\top}(\\mathbf{U}(u+u\u0026rsquo;) \\odot \\mathbf{V}v) \\\\ \u0026amp;= \\mathbf{W}^{\\top}((\\mathbf{U}u + \\mathbf{U}u\u0026rsquo;) \\odot \\mathbf{V}v) \\\\ \u0026amp;= \\mathbf{W}^{\\top}(\\mathbf{U}u\\odot \\mathbf{V}v) + \\mathbf{W}^{\\top}(\\mathbf{U}u\u0026rsquo;\\odot \\mathbf{V}v) \\\\ \u0026amp;= f(u,v)+f(u\u0026rsquo;,v) \\end{aligned} $$ Here we used the distributivity $(x+x\u0026rsquo;)\\odot y=x\\odot y+ x\u0026rsquo;\\odot y$. Linearity in the second argument and scalar multiplication follows identically.\nConversely, suppose $f$ is bilinear. Let $\\lbrace u_{i}\\rbrace _{i=1}^n$, $\\lbrace v_{j}\\rbrace _{j=1}^m$, and $\\lbrace w_{\\ell}\\rbrace _{\\ell=1}^k$ be bases for $U,V,W$. For every $i,j$ we can expand $f(u_i, v_j)$ in terms of the $w$-basis, thus there are constants $\\alpha_{i,j,\\ell}$ such that: $$ f(u_{i},v_{j})=\\sum_{\\ell=1}^{k}\\alpha_{i,j,\\ell}w_{\\ell}$$ Given arbitrary vectors $u=\\sum_{i}\\beta_{i}u_{i}$ and $v=\\sum_{j}\\gamma_{j}v_{j}$, bilinearity implies: $$ f(u,v)=\\sum_{i,j} \\beta_{i}\\gamma_{j} f(u_{i},v_{j}) = \\sum_{\\ell=1}^k \\left(\\sum_{i,j}\\alpha_{i,j,\\ell}\\beta_{i}\\gamma_{j}\\right)w_{\\ell} $$ We define the intermediate space as the tensor product space $X = U \\otimes V$. The dimension is $r = nm$, and the basis elements are $u_i\\otimes v_j$ for every pair $(i,j)$. We define the maps $\\mathbf{U}$ and $\\mathbf{V}$ to \u0026ldquo;select\u0026rdquo; coordinates: $$ (\\mathbf{U}u)_{(i,j)} = \\beta_i \\quad \\text{and} \\quad (\\mathbf{V}v)_{(i,j)} = \\gamma_j $$ Formally, $\\mathbf{U}_{(i,j), i\u0026rsquo;} = \\delta_{i,i\u0026rsquo;}$ (where the rows are indexed by tuples $(i,j)$, and the $\\delta$ is the Kroncker delta function). Then, the element-wise product corresponds to the cross-terms: $$ (\\mathbf{U}u \\odot \\mathbf{V}v)_{(i,j)} = \\beta_i \\gamma_j $$ Finally, we define $\\mathbf{W}$ to encode the $\\alpha$ constants. Set $\\mathbf{W}_{(i,j), \\ell} = \\alpha_{i,j,\\ell}$. Then: $$ [\\mathbf{W}^{\\top}(\\mathbf{U}u\\odot \\mathbf{V}v)]_{\\ell} = \\sum_{i,j} \\mathbf{W}_{(i,j),\\ell} (\\beta_i \\gamma_j) = \\sum_{i,j} \\alpha_{i,j,\\ell} \\beta_i \\gamma_j $$ This matches the $\\ell$-th coordinate of $f(u,v)$. If we choose the bases to be the standard canonical bases we obtain the equality. Otherwise, all that is left to do is compose with the coordinate transformation for each basis. $\\blacksquare$\nInterpreting the Normal Form Algorithms vs. Functions: The theorem provides a recipe. Given the matrices $\\mathbf{U}, \\mathbf{V}, \\mathbf{W}$, we can compute $f$ by calculating three matrix-vector products and one element-wise product. We call the triple $(\\mathbf{U}, \\mathbf{V}, \\mathbf{W})$ a bilinear algorithm for $f$. Inner Dimension: In the proof, we constructed $X$ with dimension $n \\cdot m$. However, this is rarely optimal. The dimension of $X$, denoted by $r$, is called the Inner Dimension (or rank). Finding the smallest possible $r$ is the key to fast matrix multiplication, as we will soon see. The Inner Dimension \u0026amp; Tensor Powers Suppose we have a bilinear algorithm $(\\mathbf{U},\\mathbf{V},\\mathbf{W})$ for $f$ with inner dimension $r$. We can define the tensor square of the function, $f^{\\otimes 2}: U^{\\otimes 2} \\times V^{\\otimes 2} \\to W^{\\otimes 2}$, via: $$ f^{\\otimes 2}\\left(\\sum_i \\alpha_i u_i \\otimes u_i^{\\prime}\\ ,\\ \\sum_j \\beta_j v_j \\otimes v^{\\prime}_j\\right) =\\sum_{i,j}\\alpha_i\\beta_j f(u,v) \\otimes f(u\u0026rsquo;,v\u0026rsquo;) $$\nExample: If $f$ is $2\\times 2$ matrix multiplication ($U=V=W=\\mathbb{R}^{2\\times 2}$), then $\\otimes$ corresponds to the Kronecker (outer) product of matrices. By the mixed-product property of Kronecker products, we know that $(A \\otimes B)(C \\otimes D) = AC \\otimes BD$. Thus, $f^{\\otimes 2}$ represents $4 \\times 4$ matrix multiplication, because every $4\\times 4$ matrix can be written as the sum of Kronecker products of $2\\times 2$ matrices.\nGeneralizing this, we define $f^{\\otimes k}$ recursively. If $(\\mathbf{U}, \\mathbf{V}, \\mathbf{W})$ computes $f$, does it help us compute $f^{\\otimes k}$? Using the property $(\\mathbf{A} \\otimes \\mathbf{B})(x \\otimes y) = \\mathbf{A}x \\otimes \\mathbf{B}y$, one can show that the algorithm for $f^{\\otimes 2}$ is given by the Kronecker products of the maps: $$ (\\mathbf{U}^{\\otimes 2}, \\mathbf{V}^{\\otimes 2}, \\mathbf{W}^{\\otimes 2}) $$ This algorithm operates with inner dimension $r^2$. For the $k$-th power, the inner dimension is $r^k$.\nTheorem Let $(\\mathbf{U}, \\mathbf{V}, \\mathbf{W})$ be a bilinear algorithm for $f$ with inner dimension $r$. Assuming $$r \u0026gt; \\max(\\dim U, \\dim V, \\dim W)$$ the runtime to compute $f^{\\otimes k}$ using the algorithm $(\\mathbf{U}^{\\otimes k}, \\mathbf{V}^{\\otimes k}, \\mathbf{W}^{\\otimes k})$ is $O(r^k)$.\nProof. The computation of $f^{\\otimes k}(u,v)$ reduces to computing:\n$x = \\mathbf{U}^{\\otimes k} u$ $y = \\mathbf{V}^{\\otimes k} v$ $z = x \\odot y$ $\\text{result} = (\\mathbf{W}^{\\otimes k})^{\\top} z$ We must show that the matrix-vector product $\\mathbf{U}^{\\otimes k} u$ can be computed efficiently. Let $\\mathbf{U}$ be an $r \\times n$ matrix. The vector $u$ has size $n^k$. We decompose $u$ as a concatenation of $n$ vectors $u^1, \\dots, u^n$ of size $n^{k-1}$. $$ \\mathbf{U}^{\\otimes k} u = (\\mathbf{U} \\otimes \\mathbf{U}^{\\otimes k-1}) u = \\begin{pmatrix} U_{1,1} \\mathbf{U}^{\\otimes k-1} \u0026amp; \\cdots \u0026amp; U_{1,n} \\mathbf{U}^{\\otimes k-1} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ U_{r,1} \\mathbf{U}^{\\otimes k-1} \u0026amp; \\cdots \u0026amp; U_{r,n} \\mathbf{U}^{\\otimes k-1} \\end{pmatrix} \\begin{pmatrix} u^1 \\\\ \\vdots \\\\ u^n \\end{pmatrix} $$ The $i$-th block of the output (for $i \\in [r]$) is $\\sum_{j=1}^n U_{i,j} (\\mathbf{U}^{\\otimes k-1} u^j)$. To compute this:\nRecursively compute $z^j = \\mathbf{U}^{\\otimes k-1} u^j$ for all $j \\in [n]$. Compute the linear combinations $\\sum_{j} U_{i,j} z^j$. Let $T(N)$ be the time to compute $\\mathbf{U}^{\\otimes k} u$ where $N=n^k$. The recursive step involves $n$ calls on size $N/n$. The combination step sums $n$ vectors of size $r^{k-1}$ for each of the $r$ output blocks. $$ T(n^k) = n \\cdot T(n^{k-1}) + O(n \\cdot r \\cdot r^{k-1}) = n \\cdot T(n^{k-1}) + O(n r^k) $$ Solving this recurrence (where $r \u0026gt; n$ by assumption) yields $T(n^k) = O(r^k)$. The same logic applies to $\\mathbf{V}$ and $\\mathbf{W}$. The element-wise product takes $O(r^k)$. Thus, total time is $O(r^k)$. $\\blacksquare$\nStrassen\u0026rsquo;s Algorithm Revisited We can now explicitly write down the matrices $(\\mathbf{U}, \\mathbf{V}, \\mathbf{W})$ for Strassen\u0026rsquo;s algorithm. Here, the inner dimension is $r=7$, and the vector spaces have dimension 4 (identified with $\\mathbb{R}^{2\\times 2}$).\n$$ \\mathbf{U}=\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ -1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix}, \\quad \\mathbf{V}=\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; -1 \\\\ -1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 1 \\end{bmatrix} $$ $$ \\mathbf{W}=\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; -1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ -1 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{bmatrix} $$\nIf you vectorize $2 \\times 2$ matrices row-major, meaning $$\\left(\\begin{smallmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{smallmatrix}\\right)\\mapsto \\left(\\begin{smallmatrix} a \\\\ b \\\\ c \\\\ d \\end{smallmatrix}\\right),$$ computing $\\mathbf{W}^\\top (\\mathbf{U} \\mathrm{vec}(A) \\odot \\mathbf{V} \\mathrm{vec}(B))$ exactly reconstructs Strassen\u0026rsquo;s logic. This proves (again) that Strassen runs in $O(7^k)$. In particular, taking tensor powers corresponds to a recursive application of the algorithm.\nAnother View: Tensors Definition A 3-dimensional tensor $\\mathcal{T}$ is an element of the tensor product space $U \\otimes V \\otimes W$. Fixing bases $\\lbrace u_i\\rbrace , \\lbrace v_j\\rbrace , \\lbrace w_{\\ell}\\rbrace $, there is a unique decomposition: $$ \\mathcal{T} = \\sum_{i,j,\\ell} T_{i,j,\\ell} \\cdot (u_i \\otimes v_j \\otimes w_{\\ell}) $$\nThis looks exactly like the constants $\\alpha_{i,j,\\ell}$ in our proof of the Strassen Normal Form. To make this intuitive, we can use the Polynomial View. Let $\\lbrace X_i\\rbrace , \\lbrace Y_j\\rbrace , \\lbrace Z_{\\ell}\\rbrace $ be sets of formal variables. We can represent a bilinear function $f$ as a polynomial: $$ P_f(X,Y,Z) = \\sum_{i,j,\\ell} T_{i,j,\\ell} X_i Y_j Z_{\\ell} $$ Here, $X_i Y_j Z_{\\ell}$ is shorthand for $u_i \\otimes v_j \\otimes w_{\\ell}$. Now, we can think of $P_f$ as a bilinear function by -\nGiven vectors $u\\in U,v\\in V$, Expand the vectors in the bases - $u=\\sum_i \\alpha_i u_i$ and $v=\\sum_j \\beta_j v_j$, Evaluate $P_f$ at $X_i=\\alpha_i$ and $Y_j=\\beta_j$, which produces a polynomial in the $Z$ variables, Let $\\gamma_{\\ell}$ denote the coefficient of $Z_{\\ell}$ in the obtained polynomial, Output $\\sum_{\\ell}\\gamma_{\\ell}w_{\\ell}$. Key Takeaway Every bilinear function corresponds uniquely to a 3-tensor (once bases are fixed).\nThe Matrix Multiplication Tensor Consider $n \\times n$ matrix multiplication. The spaces correspond to matrices, so we index variables by pairs. Variables: $\\lbrace X_{i,j}\\rbrace , \\lbrace Y_{i,j}\\rbrace , \\lbrace Z_{i,j}\\rbrace $ for $i,j \\in [n]$. The condition for the product is: $C_{p,q} = \\sum_k A_{p,k} B_{k,q}$. The corresponding tensor, denoted $\\langle n,n,n \\rangle$, is: $$ \\langle n,n,n \\rangle = \\sum_{i,j,k} X_{i,k} Y_{k,j} Z_{i,j} $$ In particular, we define $$T_{(i,k),(k\u0026rsquo;,j),(p,q)}=\\begin{cases}1 \u0026amp; i=p,j=q,k=k\u0026rsquo;, \\\\ 0 \u0026amp; \\text{else}.\\end{cases}$$\nTensor Ranks and $\\omega$ Note that a vector (called a tensor) in $V\\otimes U$ might have a simple presentation as a tensor product $v\\otimes u$ for $v\\in V,u\\in U$, but most of the vectors do not have such a presentation. For example $v_1 \\otimes u_1 - v_2 \\otimes u_2$ cannot be written as a single tensor product of two vectors. A natural question is - what is the minimal number of simple terms that we need to present some tensor in $V\\otimes U$?\nDefinition The Rank of a tensor $\\mathcal{T}\\in U\\otimes V\\otimes W$, denoted $R(\\mathcal{T})$, is the minimum integer $r$ such that $\\mathcal{T}$ can be written as a sum of $r$ simple tensors: $$ \\mathcal{T} = \\sum_{s=1}^r u_s \\otimes v_s \\otimes w_s $$\nComputing the rank of a specific tensor is NP-hard. However, finding upper bounds on the rank of $\\langle n,n,n \\rangle$ gives us upper bounds on $\\omega$.\nIn particular, suppose we fix some bases $\\set{u_i},\\set{v_j},\\set{w_{\\ell}}$ for $U,V,W$, then any simple tensor can be written as $$\\left(\\sum_{i}\\alpha_i u_i\\right)\\otimes \\left(\\sum_j \\beta_j v_j \\right)\\otimes \\left(\\sum_{\\ell}\\gamma_{\\ell}w_{\\ell}\\right)$$ Hence a tensor $\\mathcal{T}$ can be decomposed as $$\\sum_{s=1}^r \\left(\\sum_{i}\\alpha_{s,i} u_i\\right)\\otimes \\left(\\sum_{s,j} \\beta_j v_j \\right)\\otimes \\left(\\sum_{\\ell}\\gamma_{s,\\ell}w_{\\ell}\\right)$$ and writing $$\\mathbf{U}=(\\alpha_{s,i}),\\quad \\mathbf{V}=(\\beta_{s,j}),\\quad \\mathbf{W}=(\\gamma_{s,\\ell})$$ gives us an algorithm for $\\mathcal{T}$ with inner dimension $r$. To sum this discussion up:\nProposition A decomposition of $\\mathcal{T}$ into $r$ simple tensors is equivalent to a bilinear algorithm $(\\mathbf{U}, \\mathbf{V}, \\mathbf{W})$ with inner dimension $r$. Specifically, there is a correspondence between tensor decompositions and bilinear algorithms for that tensor.\nThis leads to the fundamental connection between tensors and complexity:\nCorollary $$ \\omega \\le \\log_n R(\\langle n,n,n \\rangle) $$\nProof. If $R(\\langle n,n,n \\rangle) = r$, we have an algorithm for $n \\times n$ matrix multiplication with $r$ multiplications. By the recursive theorem from Part 1 (and the tensor power theorem from this post), it follows that $\\omega\\le \\log_n r$. $\\blacksquare$\nReferences Strassen, V. (1969). Gaussian Elimination is not Optimal .. Håstad, J. (1990). Tensor rank is NP-complete . Bürgisser, P., Clausen, M., \u0026amp; Shokrollahi, M. A. (1997). Algebraic Complexity Theory . Bläser, M. (2013). Fast Matrix Multiplication . ","title":"Fast Matrix Multiplication - Part 2"},{"link":"/posts/fmm-1/","text":" Prerequisites Linear Algebra: Basic definitions (matrix multiplication, inner products, block matrices). Asymptotic Notation: Big-O notation and the Master Theorem. No prior knowledge of Group Theory or Representation Theory is required for this post. In this series of posts, we will discuss a fascinating line of work that aims to use group theory and representation theory to design fast algorithms for matrix multiplication. We will start with the basics of matrix multiplication algorithms and build up to the group theoretic approach. This post is intended for readers without any prior knowledge of groups or representation theory, or of matrix multiplication algorithms. My goal is to build an intuitive understanding of the problem space before we dive deeper in future posts.\nMatrix Multiplication Algorithms Recall the standard definition of matrix multiplication. Given a matrix $A$ of size $n\\times m$ and a matrix $B$ of size $m\\times p$, their product $AB$ is an $n\\times p$ matrix given by: $$(AB)_{i,j}=\\sum_{k=1}^{m}A_{i,k}B_{k,j}$$ where $i\\in[n]$ and $j\\in [p]$. It is often helpful to view this geometrically. Let $A_{i}$ denote the $i$-th row of $A$ (transposed to a column vector) and $B^{j}$ denote the $j$-th column of $B$. Then:\n$$(AB)_{i,j} = \\langle A_{i}, B^j \\rangle = A_{i}^{\\top}B^j$$\nThis definition immediately yields the \u0026ldquo;naive\u0026rdquo; algorithm to compute $AB$:\nAlgorithm: Naive Matrix Multiplication For each entry $(i, j)$ in the output matrix ($n \\cdot p$ entries total), compute the inner product between row $i$ of $A$ and column $j$ of $B$.\nEach inner product requires $m$ multiplications and $m-1$ additions. Thus, the overall runtime is approximately $2nmp = O(nmp)$. In the square case where $n=m=p$, we obtain the familiar cubic runtime $O(n^{3})$. Specifically, the number of arithmetic operations is:\nMultiplications: $n^{3}$ Additions: $n^{2}(n-1)$ Strassen\u0026rsquo;s Algorithm In 1969, Volker Strassen made a startling discovery: the product of $2\\times 2$ matrices can be computed using 7 multiplications instead of the usual 8, albeit at the cost of performing more additions.\nAlgorithm: Strassen\u0026rsquo;s $2 \\times 2$ Given matrices: $$A=\\begin{pmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\end{pmatrix},\\quad B=\\begin{pmatrix} e \u0026amp; f \\\\ g \u0026amp; h \\end{pmatrix}$$ First, compute the following 7 intermediate products: $$ \\begin{aligned} M_{1}\u0026amp;= (a+d)(e+h) \u0026amp; M_{2}\u0026amp;= (c+d)e \\\\ M_{3}\u0026amp;= a(f-h) \u0026amp; M_{4}\u0026amp;= d(g-e) \\\\ M_{5}\u0026amp;= (a+b)h \u0026amp; M_{6}\u0026amp;= (c-a)(e+f)\\\\ M_{7}\u0026amp;= (b-d)(g+h) \u0026amp; \u0026amp; \\end{aligned} $$ Then, construct the result matrix $C = AB$ via linear combinations of the $M_i$: $$ C=\\begin{bmatrix} M_{1}+M_{4}-M_{5}+M_{7} \u0026amp; M_{3}+M_{5} \\\\ M_{2}+M_{4} \u0026amp; M_{1}-M_{2}+M_{3}+M_{6} \\end{bmatrix} $$\nWhy is this useful? Why trade one multiplication for several additions? The answer lies in recursion.\nRecursive Application of Matrix Multiplication Algorithms Suppose we have an algorithm for multiplying $n \\times n$ matrices (the \u0026ldquo;base\u0026rdquo; size). Given much larger matrices of size $n^2 \\times n^2$, we can view them as $n \\times n$ block matrices, where each element is itself a smaller sub-matrix of size $n \\times n$.\nLet $\\mathbf{A}, \\mathbf{B}$ be $n^2 \\times n^2$ matrices partitioned into $n \\times n$ blocks. Define the $n\\times n$ blocks by $A^{I,J}=[\\mathbf{A}_{I\\cdot n+i,J\\cdot n+j}]_{i,j\\in [n]}$ for $I,J\\in [n]$. Similarly define $B^{I,J}$. Then $$\\mathbf{A}=\\left(\\begin{smallmatrix} A^{0,0} \u0026amp; \\cdots \u0026amp; A^{0,n-1} \\\\ \\vdots \u0026amp; \\ddots \\\\ A^{n-1,0} \u0026amp; \\cdots \u0026amp; A^{n-1,n-1} \\end{smallmatrix}\\right),\\quad \\mathbf{B}=\\left(\\begin{smallmatrix} B^{0,0} \u0026amp; \\cdots \u0026amp; B^{0,n-1} \\\\ \\vdots \u0026amp; \\ddots \\\\ B^{n-1,0} \u0026amp; \\cdots \u0026amp; B^{n-1,n-1} \\end{smallmatrix}\\right).$$ It is an easy observation that the product block satisfies: $$(\\mathbf{AB})^{I,J}=\\sum_{K=0}^{n-1}A^{I,K}B^{K,J}.$$ Crucially, the product $A^{I,K}B^{K,J}$ represents a matrix multiplication of the smaller blocks, and the summation represents element-wise matrix addition.\nRemark For any ring $R$, one can define the matrix ring $\\mathcal{M}_n(R)$ of square matrices over $R$. In particular, the observations above show that $$\\mathcal{M}_n (\\mathcal{M}_n (\\mathbb{F}))\\cong \\mathcal{M}_{n^2}(\\mathbb{F}).$$ On the left hand side, matrices over the ring $R=\\mathcal{M}_n(\\mathbb{F})$ have the same multiplication law, with scalar multiplication replaced by multiplication in $R$, that is $n\\times n$ block matrix multiplication.\nIf our base algorithm for size $n$ uses $M$ scalar multiplications and $L$ scalar additions, we can lift this to the block setting. The algorithm works if we replace scalar by block matrix operations. Therefore to compute a $n^2\\times n^2$ product, we need to:\nPerform $M$ matrix multiplications of size $n\\times n$. Each product can be computed recursively using the same algorithm, costing $M$ scalar computations and $L$ additions. Perform $L$ matrix additions of size $n\\times n$. Each addition requires computing exactly $n^2$ scalar additions. Therefore the total operation count is $$M\\cdot (M+L) + n^2\\cdot L=M^2 + (M+n^2)\\cdot L.$$ This discussion can be generalized to any power of $n$. To multiply $n^k\\times n^k$ matrices, we apply the algorithm recursively $k$ times (the depth of the recursion). Let $T(k)$ denote the time to multiply $n^k\\times n^k$ matrices using the algorithm. Then the recurrence relation is: $$ T(k) = M \\cdot T(k-1) + L \\cdot (n^{k-1})^2. $$\nSolving this recurrence with the Master Theorem yields a runtime of: $$ T(k)=O(M^{k}) $$ provided $M \u0026gt; n^2$. In other words, the runtime is dominated by the number of multiplications, while the additions only affect the constant factors.\nExample: Lets apply this to Strassen\u0026rsquo;s algorithm, where the base size is $n=2$ and the number of multiplications is $M=7$. We can compute matrices of size $2^{k}\\times 2^{k}$ in time $O(7^{k})$. This is significantly faster than the naive $O(2^{3k})=O(8^{k})$.\nWriting $N=2^{k}$, we have $k = \\log_2 N$, and the complexity to multiply $N\\times N$ matrices becomes: $$ O(7^{\\log_2 N}) = O(N^{\\log_{2}7}) \\approx O(N^{2.807}) $$ This result proved that matrix multiplication is sub-cubic.\nThe Main Question in FMM Given Strassen\u0026rsquo;s breakthrough, we know that the naive cubic algorithm is not optimal. The central pursuit in the field of Fast Matrix Multiplication (FMM) is determining just how fast we can go.\nFormally, we define the exponent of matrix multiplication, $\\omega$:\nDefinition $$ \\omega = \\inf \\lbrace{ \\tau \\le 3 \\mid \\forall \\varepsilon \u0026gt; 0, \\text{ there is an algorithm for } n \\times n \\text{ MM with runtime } O(n^{\\tau+\\varepsilon}) \\rbrace} $$\nThe main open question is: What is the precise value of $\\omega$? (It is conjectured that $\\omega = 2$).\nThe recursive argument we utilized for Strassen\u0026rsquo;s algorithm can be generalized into the following fundamental theorem regarding $\\omega$:\nTheorem If there exists an algorithm for computing an $n_{0}\\times n_{0}$ matrix multiplication using $M$ scalar multiplications, then: $$ \\omega \\le \\log_{n_{0}}M $$\nProof. Let $n \\in \\mathbb{N}$ be arbitrary, and let $A, B$ be $n \\times n$ matrices. We must show we can multiply them in time $O(n^{\\log_{n_0} M+\\varepsilon})$ for every $\\varepsilon\u0026gt;0$. Define $k$ to be the minimal integer such that $n_{0}^{k} \\ge n$, and let $N=n_{0}^{k}$. We construct $N \\times N$ matrices $\\widetilde{A}, \\widetilde{B}$ by embedding $A$ and $B$ into the upper-left $n \\times n$ block and padding with zeros: $$\\widetilde{A}=\\begin{pmatrix}A \u0026amp; 0 \\\\ 0 \u0026amp; 0\\end{pmatrix}.$$\nWe can recursively apply the base algorithm to multiply $\\widetilde{A}$ and $\\widetilde{B}$. Based on our previous analysis, the runtime is: $$ O(M^{k}) = O(N^{\\log_{n_{0}}M}) $$\nThe $n \\times n$ upper-left block of the result $\\widetilde{A}\\widetilde{B}$ is exactly $AB$.\nSince we chose the minimal $k$, we have $n_{0}^{k-1} \u0026lt; n \\le n_{0}^{k}$. Treating $n_0$ as a constant, this implies $\\log_n N =\\frac{k}{\\log_{n_0} n}$ which by definition is between $1$ and $1+\\frac{1}{k-1}$. Taking $k_0$ to be large enough so that $\\frac{1}{k-1}\\cdot \\log_{n_0}M\u0026lt;\\varepsilon$, we see that for $n=\\Omega(n_0^{k_0})$ it holds $O(N^{\\log_{n_0}M})=O(n^{\\log_n N\\cdot \\log_{n_0}M})=O(n^{\\log_{n_0}M+\\varepsilon})$, thus concluding the proof. $\\blacksquare$\nReferences Strassen, V. (1969). Gaussian Elimination is not Optimal . Cohn, H., \u0026amp; Umans, C. (2003). A Group-theoretic Approach to Fast Matrix Multiplication . ","title":"Fast Matrix Multiplication - Part 1"},{"link":"/posts/cut-norm/","text":" Prerequisites Linear Algebra: Eigenvalues, PSD matrices, Tensor products. Convex Optimization: Basic familiarity with Semidefinite Programming (SDP). Probability: Expectations, Markov\u0026rsquo;s inequality. Graph Theory: Basic definitions, Cuts, Regularity. Consider the following problem: Given an undirected graph $G=(V,E)$, let $A,B\\subset V$ denote non-empty disjoint sets. Let $E(A,B)$ denote the set of edges in $E$ that cross from $A$ to $B$. Denote $D(A,B)=\\frac{\\left|E(A,B)\\right|}{\\left|A\\right|\\left|B\\right|}$ to be the density of edges crossing from $A$ to $B$.\nRegularity, in the context of graphs, can describe some notion of pseudo-random structure. The idea of regularity decomposition of graphs is to decompose a graph into sub-graphs which are regular in some sense. This allows us to prove results regarding general graphs by reducing the problem to random-like graphs, which are often easier to analyze and reason about. There is, generally, a tradeoff between the strength of the regularity notion and the efficiency of the decomposition under that notion (efficiency being the number of sub-graphs).\nOne notion of regularity is the following $\\varepsilon$-regularity of sets $(A,B)$ as above: we call $(A,B)$ $\\varepsilon$-regular if for every $X\\subset A,Y\\subset B$ with $\\left|X\\right|\\ge \\varepsilon \\left|A\\right|$ and $\\left|Y\\right|\\ge \\varepsilon \\left|B\\right|$, the density $D(X,Y)$ is $\\varepsilon$-close to the density $D(A,B)$, meaning $$\\forall X\\subset A,Y\\subset B, |X|\\ge \\varepsilon\\cdot |A|,| Y\\ge \\varepsilon\\cdot |B|:\\quad \\left|D(X,Y)-D(A,B)\\right|\\le \\varepsilon$$ In other words, the edges crossing between $A,B$ are not concentrated on a small subset of vertices.\nSuppose we wish to determine if a pair $(A,B)$ is $\\varepsilon$-regular, assuming $\\left|A\\right|=\\left|B\\right|=n$. We could denote $d=D(A,B)$ and define a matrix $F=(f_{ab})_{a\\in A,b\\in B}$ by setting $$f_{ab}=\\begin{cases} 1-d \u0026amp; \\lbrace a,b\\rbrace \\in E \\\\ -d \u0026amp; \\text{else.} \\end{cases}$$ Note that $$\\left|\\sum_{a\\in X,b\\in Y}f_{ab}\\right|=\\left|\\sum_{(a,b)\\in E(X,Y)}1-d\\cdot \\left|X\\right|\\cdot \\left|Y\\right|\\right|=\\left|\\left|E(X,Y)\\right|-d\\cdot |X||Y|\\right|=\\left|X\\right|\\cdot \\left|Y\\right|\\cdot \\left|D(X,Y)-d\\right|.$$ Hence, if the pair is not $\\varepsilon$-regular, then there are $X\\subset A,Y\\subset B$ with $\\left|X\\right|, \\left|Y\\right|\\ge \\varepsilon n$ for which $$\\left|D(X,Y)-D(A,B)\\right|=\\left|D(X,Y)-d\\right|\u0026gt; \\varepsilon\\iff \\left|\\sum_{a\\in X,b\\in Y}f_{ab}\\right|\u0026gt; \\varepsilon \\left|X\\right|\\cdot \\left|Y\\right|\\ge \\varepsilon^{3}n^{2}.$$ This motivates the following definition:\nDefinition: Cut Norm The cut norm of a matrix $A=(a_{ij})_{i\\in R,j\\in S}$ indexed by $R,S$ is $$\\left\\lVert A \\right\\rVert_{C}:=\\max_{I\\subset R,J\\subset S}\\left|\\sum_{i\\in I,j\\in J}a_{ij}\\right|.$$\nSo a pair $(A,B)$ is not $\\varepsilon$-regular if and only if $\\left\\lVert F \\right\\rVert_{C}\\ge \\varepsilon^{3}n^{2}$. Therefore, irregularity in graphs can be detected using the cut-norm. The question then becomes - is there an algorithm that efficiently computes the cut norm?\nApplication: Cut-Decomposition The cut-decomposition of a graph is another motivating example for the need of an efficient algorithm to compute the cut-norm. For many graph problems, it is useful to find a cut-decomposition for the graph, which involves finding matrices $D^{(1)},\\ldots, D^{(k)}$ of a special structure which approximate a matrix $A$ (which is often based on the adjacency matrix of the graph) in the cut-norm. The matrices $D^{(i)}$ are restricted to be cut matrices, which are of the form $d_{i}\\cdot 1_{I}1_{J}^{\\top}$, where $I\\subset[n],J\\subset [m]$ assuming $A$ has size $n\\times m$ and $d_{i}$ is some scalar. Such a computation can be performed rather fast (polynomial time, also in the accuracy) if one has access to a good approximation of the cut norm.\nHardness of Approximation of the Cut-Norm Definition An algorithm is called a $C$-approximation algorithm if it delivers, on expectation, a solution whose objective value is $C$ times the optimum. In particular, this is relevant only for optimization problems.\nA problem is called MAX-SNP hard if there is no polynomial time algorithm that approximates the problem with arbitrary precision. In other words, there exists some constant $\\rho$, so the existence of a $\\rho$-approximation algorithm implies P equals NP.\nOne famous MAX-SNP problem is MAX-CUT, in which we are given a graph $G=(V,E)$ and are asked to find a cut $(A,B)$ (so $A\\cap B=\\emptyset,A\\cup B=V$) for which $\\left|E(A,B)\\right|$ is maximal. The hardness of computing the cut-norm will be proved by a reduction from MAX-CUT.\nMoving to an Equivalent Norm Definition: $\\infty \\mapsto 1$ Norm Let $A$ be a matrix $(a_{ij})_{i\\in [n],j\\in [m]}$, then we define the $\\infty\\mapsto 1$ norm by $$\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}:=\\max_{x\\in \\lbrace \\pm1\\rbrace ^{n},y\\in \\lbrace \\pm1\\rbrace ^{m}}\\sum_{i=1}^{n}\\sum_{j=1}^{m}a_{ij}\\cdot x_{i}y_{j}.$$\nLemma For every real matrix it holds $\\left\\lVert A \\right\\rVert_{C}\\le \\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}\\le 4 \\left\\lVert A \\right\\rVert_{C}$.\nProof. Fix $x,y$, then $$\\sum_{i}\\sum_{j}a_{ij}x_{i}y_{j}=\\sum_{i,j:x_{i}=y_{j}=1}a_{ij}-\\sum_{i,j:x_{i}=-y_{j}=1}a_{ij}-\\sum_{i,j:x_{i}=-y_{j}=-1}a_{ij}+\\sum_{i,j:x_{i}=y_{j}=-1}a_{ij}.$$ Each of the sums corresponds to taking $I=\\lbrace i:x_{i}=\\alpha\\rbrace $ and $J=\\lbrace j:y_{j}=\\beta\\rbrace $ for $(\\alpha,\\beta)\\in \\lbrace \\pm1\\rbrace ^{2}$, and so each of the sums is in absolute value less than or equal to $\\left\\lVert A \\right\\rVert_{C}$, thus giving the upper bound.\nFor the lower bound, let $I,J$ denote the sets achieving the maximum. Suppose $\\left\\lVert A \\right\\rVert_{C}=\\sum_{i\\in I,j\\in J}a_{ij}$. Define $x_{i}=y_{j}=1$ for $i\\in I,j\\in J$ and $-1$ otherwise. Then $\\frac{x_i+1}{2}\\cdot \\frac{y_j+1}{2}=1$ if $i\\in I,j\\in J$ and $0$ otherwise. Therefore: $$\\left\\lVert A \\right\\rVert_{C}=\\sum_{i\\in I,j\\in J}a_{ij}=\\sum_{i,j}a_{ij}\\cdot \\frac{x_{i}+1}{2} \\cdot \\frac{y_{j}+1}{2}=\\frac{1}{4}\\sum_{i,j}a_{ij}+\\frac{1}{4}\\sum_{i,j}a_{ij}x_{i}+\\frac{1}{4}\\sum_{i,j}a_{ij}y_{j}+\\frac{1}{4}\\sum_{i,j}a_{ij}x_{i}y_{j}.$$ Hence $\\left\\lVert A \\right\\rVert_{C}$ is the average of four different assignments of $x,y$, which are all smaller than $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$, thus their average is too, concluding that $\\left\\lVert A \\right\\rVert_{C}\\le \\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$. $\\blacksquare$\nRemark If $A$\u0026rsquo;s rows and columns all sum to zero, then the first three sums above are zero, thus showing that $\\left\\lVert A \\right\\rVert_{C}\\le \\frac{1}{4}\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}\\le \\frac{1}{4}\\cdot 4\\left\\lVert A \\right\\rVert_{C}=\\left\\lVert A \\right\\rVert_{C}$. Hence $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}=4 \\left\\lVert A \\right\\rVert_{C}$.\nReducing from MAX-CUT Proposition Let $G=(V,E)$. Then there exists an efficient construction of a matrix $A$ of size $2\\left|E\\right|$ by $\\left|V\\right|$, such that $\\mathrm{MAXCUT}(G)=\\left\\lVert A \\right\\rVert_{C}=\\frac{1}{4}\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$. This proves that MAX-CUT reduces to computing the cut-norm.\nProof. Define $A$ as follows: for every edge $\\lbrace u,v\\rbrace $, we define two rows in $A$, one for $(u,v)$ and one for $(v,u)$. The first one is defined by $e_{u}-e_{v}$ and the second $e_{v}-e_{u}$, where $e_{i}$ is the $i$-th standard basis vector for $\\mathbb{R}^{\\left|V\\right|}$. Note that rows and columns of $A$ sum to zero. Therefore $\\left\\lVert A \\right\\rVert_{C}= \\frac{1}{4}\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$.\nConsider $$\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}=\\max_{x\\in \\lbrace\\pm 1\\rbrace^{2|E|} ,y\\in \\lbrace \\pm 1\\rbrace^{|V|}}\\sum_{(u,v),w} a_{(u,v),w}x_{(u,v)}y_{w}=\\max_{x,y}\\sum_{(u,v)}(x_{(u,v)}y_{u}-x_{(u,v)}y_{v}).$$ Note that if $y_{u}=y_{v}$ then $x_{(u,v)}y_{u}-x_{(u,v)}y_{v}=0$ and the same goes for $x_{(v,u)}y_{v}-x_{(v,u)}y_{u}=0$. Alternatively, if $y_{u}\\ne y_{v}$, then $$\\left|x_{(u,v)}(y_{u}-y_{v})\\right|=\\left|x_{(v,u)}(y_{v}-y_{u})\\right|=2,$$ and since $x_{(u,v)},x_{(v,u)}$ don\u0026rsquo;t participate in any other elements of the sum, their contribution to the sum is maximized by choosing the signs of the expression $y_u-y_v$ and $y_v-y_u$ respectively. In other words, the sum is maximized by $$\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}=\\max_{y}4\\sum_{\\lbrace u,v\\rbrace \\in E}\\mathbf{1}_{y_{u}\\not=y_{v}},$$ and thus we can treat $y$ as the partition rule setting $A=\\lbrace u:y_u=1\\rbrace $ and $B=\\lbrace u:y_u=-1\\rbrace $, showing that $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}=4\\cdot \\left|E(A,B)\\right|=4\\cdot\\mathrm{MAXCUT}(G)$ by maximality (any valid cut induces a valid assignment for $y$). $\\blacksquare$\nFor MAX-CUT it is known that if P is not equal to NP, there is no polynomial time approximation with ratio exceeding $16/17$, so this is also an upper bound for the ratio of approximation for the Cut Norm problem.\nA Deterministic Approximation Algorithm Casting the Problem as an SDP Reminder A semi-definite program (SDP) is a constrained optimization problem of the following form: $$\\min_{x\\in\\mathbb{R}^{n}}c^{\\top}x\\quad \\text{subject to}\\quad x_{1}A_{1}+\\ldots+x_{n}A_{n}\\preceq B,\\quad Ax=b,$$ where $c\\in\\mathbb{R}^{n}$ and $B,A_{1},\\ldots,A_{n}$ are symmetric matrices of size $k\\times k$ for some $k$, and $b\\in \\mathbb{R}^m,A\\in \\mathbb{R}^{m\\times n}$ are the linear constraints. We denote $A\\preceq B$ when $B-A$ is a positive semi-definite matrix.\nNote that the value $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$ can be written as the solution to the following integer quadratic program: $$\\max \\ \\sum_{i,j}a_{ij}x_{i}y_{j}\\quad \\text{subject to}\\quad x_i,y_{j}\\in \\lbrace \\pm1\\rbrace .$$ As always when dealing with integer programs, we can try to relax the integer constraints to obtain easier problems. Hopefully, if the problem is nice enough, we can then round the solutions of the relaxed problems to integral solutions, without too much error.\nIn this case, one such way to relax the problem is by replacing the scalars $x_i,y_j$ with vectors $u_i,v_j$ (of potentially higher dimension), and so the constraint $x_i,y_j\\in \\lbrace \\pm1 \\rbrace$ becomes $\\| u_i \\|=\\|v_j\\| =1$: $$\\max \\sum_{i,j}a_{ij}\\cdot \\left\\langle u_{i},v_{j} \\right\\rangle \\quad \\text{subject to}\\quad \\left\\lVert u_{i} \\right\\rVert^2=\\left\\lVert v_{j} \\right\\rVert^2=1.$$\nThis type of problem is called a Quadratically Constrained Quadratic Program. In general, it is as hard as integer programming, and cannot be solved efficiently. However, in the special case above, we are not really using the vectors, but only use their inner products (because $\\|u\\|^2= \\langle u,u\\rangle$).\nLemma The vector programming relaxation above can be solved using an SDP.\nProof. Let $p=n+m$ and index the vectors as $w_1,\\ldots,w_p$ where the first $n$ are $u_i$ and the rest are $v_j$. The Gram matrix is defined by $$G=(\\langle w_i, w_j\\rangle)_{i,j\\in [p]}$$ It is a symmetric positive-definite matrix, by properties of the real inner product. Therefore, it is uniquely determined by $p(p+1)/2$ values (the main diagonal + the upper triangle). Note that $$\\sum_{i\\in [n],j\\in [m]} a_{ij}\\cdot \\langle u_i ,v_j\\rangle = \\sum_{i\\in [n],j\\in [m]}a_{ij}\\cdot G_{i,n+j}$$ and the constraints can be written as $\\| u_i\\|^2=G_{i,i}=1$ (similarly for $v_j$). This motivates the following scalar program: for $x\\in \\mathbb{R}^{p(p+1)/2}$ we can index the elements of $x$ by pairs $(i,j)$ where $1\\le i\\le j\\le p$. Let $G(x)$ denote the symmetric matrix determined by $x$. Write: $$\\max_{x\\in \\mathbb{R}^{p(p+1)/2}} \\sum_{i\\in [n], j\\in [m]}a_{ij}\\cdot x_{i,n+j} \\quad \\text{subject to}\\quad x_{i,i}=1,\\quad G(x)\\succeq 0.$$\nGiven a solution to the vector problem, we see that setting $x_{i,j}=G_{i,j}$ gives the same objective value in the scalar program (and is feasible, because $G_{i,i}=1$ and $G\\succeq 0$), hence the vector maximum value (denoted $V^*$) is at most the scalar maximum value (denoted $S^*$), i.e., $V^*\\le S^*$.\nConversely, given a solution $x$ for the scalar problem, the matrix $G(x)$ is positive semi-definite and symmetric, thus by the spectral theorem (from linear algebra) it can be written as $U\\Lambda U^{\\top}$ for an orthogonal matrix $U$ and diagonal matrix $\\Lambda$. Take $w_i=(\\sqrt{\\lambda_j} U_{i,j})_{j\\in [p]}$ where $\\lambda_1,\\ldots,\\lambda_p\\ge 0$ are the eigenvalues on the diagonal of $\\Lambda$. Then $(G(x))_{i,j}=w_i^{\\top}w_j=\\langle w_i,w_j\\rangle$, hence a solution to scalar problem translates to a solution to the vector problem, so $S^{*}\\le V^*$. This concludes the equality $S^*=V^*$.\nTo see the scalar problem is an SDP is easy. The objective is clearly linear in $x$ and the equality constraints are also linear in $x$. We just need to write $G(x)$ as a sum of matrices: $$G(x)=\\sum_{1\\le i\u0026lt; j\\le p} x_{i,j}\\cdot (E_{i,j} +E_{j,i}) +\\sum_{1\\le i\\le p} x_{i,i} E_{i,i}$$ where $E_{i,j}$ is the unit matrix with $1$ in the $(i,j)$-th position and zero everywhere else. $\\blacksquare$\nWhat have we achieved so far: Assuming we solved the relaxed vector problem using the scalar version SDP, we arrive at vectors $u_i,v_j\\in \\mathbb{R}^p$ (see the previous proof), that give a $\\delta$-approximation (for some $\\delta\u0026gt;0$) of the relaxed objective, which is at least $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$. We can choose $\\delta$ to be negligible compared to the entries of $A$. Now we need to round the result to obtain an approximation to the integer program. Note that $\\langle u_i ,v_j\\rangle$ is any real number (by Cauchy-Schwarz it is in $[-1,1]$), not necessarily $\\pm1$ as required.\nOrthogonal Arrays An orthogonal array is just a sample space with limited independence between values.\nDefinition A set of vectors $V$ is called an orthogonal array for $k$ of strength $4$, every vector $\\varepsilon\\in V$ is a sign vector $\\varepsilon\\in \\lbrace \\pm1\\rbrace ^{k}$ in which the values of the vector are $4$-wise independent, and chosen randomly from $\\lbrace \\pm1\\rbrace $. More precisely, for every quadruple of coordinates $1\\le i_{1}\u0026lt;i_{2}\u0026lt;i_{3}\u0026lt;i_{4}$ and every choice of $\\alpha\\in \\lbrace \\pm1\\rbrace ^4$, exactly $1/16$ of the vectors in $V$ have $\\alpha_{j}$ in coordinate $i_{j}$ for $j\\in \\lbrace 1,2,3,4\\rbrace $.\nWe can treat $V$ as a probability space with the uniform probability. For $\\| q\\|=1$ define $h(q)$ as the random variable given by $\\langle \\varepsilon,q\\rangle$ where $\\varepsilon$ is randomly drawn from $V$. Take $V$ to have size $t=O(p^2)$ and $k=p$.\nLemma It holds $\\mathbb{E}[h(q)\\cdot h(q\u0026rsquo;)]=\\left\\langle q,q\u0026rsquo; \\right\\rangle$. In particular, $\\mathbb{E}[h(q)^{2}]=1$ since $\\left\\lVert q \\right\\rVert=1$. Moreover, $\\mathbb{E}[h(q)^{4}]\\le 3$.\nProof. We have $$\\mathbb{E}[h(q)\\cdot h(q\u0026rsquo;)]=\\sum_{i,j}\\mathbb{E}_{\\varepsilon\\in V}[q_{i}\\varepsilon_{i}q_{j}\u0026rsquo;\\varepsilon_{j}]=\\sum_{i,j}q_{i}q_{j}\u0026rsquo;\\cdot \\mathbb{E}_{\\varepsilon\\in V}[\\varepsilon_{i} \\varepsilon_{j}]=\\sum_{i}q_{i}q_{i}\u0026rsquo;=\\left\\langle q,q\u0026rsquo; \\right\\rangle,$$ using the fact $\\varepsilon_{i},\\varepsilon_{j}$ are independent if $i\\not=j$ and their expectation is $0$. Moreover, 4-wise independence also implies $$\\mathbb{E}[h(q)^{4}]=\\sum_{i,j,k,l}q_{i}q_{j}q_{k}q_{l}\\cdot \\mathbb{E}[\\varepsilon_{i} \\varepsilon_{j} \\varepsilon_{k} \\varepsilon_{l}]=\\sum_{i}q_{i}^{4}+\\binom{4}{2}\\sum_{j\u0026lt;k}q_{j}^{2}q_{k}^{2}\\le 3\\cdot\\left(\\sum_{i}q_{i}^{2}\\right)^{2}=3.$$ $\\blacksquare$\nDefine the $M$-truncation $h^{M}(q)$ to be $$h^{M}(q)=\\begin{cases} h(q) \u0026amp; \\left|h(q) \\right|\\le M, \\\\ M \u0026amp; h(q)\u0026gt;M, \\\\ -M \u0026amp; h(q)\u0026lt;-M. \\end{cases}$$ For any $m\\in\\mathbb{R}$, Markov\u0026rsquo;s inequality says $$\\Pr(\\left|h(q)\\right|\\ge m)=\\Pr(\\left|h(q)\\right|^{4}\\ge m^{4})\\le \\frac{\\mathbb{E}[\\left|h(q)\\right|^{4}]}{m^{4}}\\le \\frac{3}{m^{4}}.$$ Note that $$\\mathbb{E}[\\left|h(q)-h^{M}(q)\\right|^{2}]=0\\cdot \\Pr(\\left|h(q)\\right|\\le m)+\\mathbb{E}\\left[\\left|h(q)-h^{M}(q)\\right|^{2}\\mid \\left|h(q)\\right|\u0026gt; M\\right]\\cdot \\Pr(\\left|h(q)\\right|\u0026gt;M).$$ Conditioned on $\\left|h(q)\\right|\u0026gt;M$ we have $$\\mathbb{E}[(h(q)-h^{M}(q))^{2}]=\\mathbb{E}[h(q)^{2}]-2M \\cdot \\mathbb{E}[h(q)]+M^{2}=1+M^{2},$$ and so $$ \\mathbb{E}[\\left|h(q)-h^{M}(q)\\right|^{2}]\\le (1+M^{2})\\cdot \\frac{3}{M^{4}}\\le 1/M.$$ Define $H(q)\\in\\mathbb{R}^{t}$ to be the vector given by $H(q)_{\\varepsilon}=\\frac{1}{\\sqrt{t}}h(q)(\\varepsilon)=\\frac{1}{\\sqrt{t}}\\left\\langle q,\\varepsilon \\right\\rangle$, for every $\\varepsilon\\in V$. Similarly, define the truncation $H^{M}(q)$ to be the scaled truncation of $h^{M}(q)$.\nLemma For every unit vector $q\\in\\mathbb{R}^p$, the vector $H(q)\\in\\mathbb{R}^{t}$ is also a unit vector. The norm of $H^{M}(q)$ is at most $1$, that of $H(q)-H^{M}(q)$ is at most $1/M$. If $q\u0026rsquo;\\in\\mathbb{R}^{p}$ is another vector then $\\left\\langle H(q),H(q\u0026rsquo;) \\right\\rangle=\\left\\langle q,q\u0026rsquo; \\right\\rangle$.\nProof. We have $\\left\\lVert H(q) \\right\\rVert^{2}=\\frac{1}{t}\\sum_{\\varepsilon\\in V}\\left\\langle q,\\varepsilon \\right\\rangle^{2}=\\mathbb{E}[h(q)^{2}]=\\langle q,q\\rangle =1$. Moreover, $\\left|H^{M}(q)_{i}\\right|\\le \\left|H(q)_{i}\\right|$ for every $i$ by definition, so $\\left\\lVert H^{M}(q) \\right\\rVert\\le \\left\\lVert H(q) \\right\\rVert=1$. Similarly, $$\\left\\lVert H(q)-H^{M}(q) \\right\\rVert^{2}=\\frac{1}{t}\\sum_{\\varepsilon\\in V}\\left|\\left\\langle q,\\varepsilon \\right\\rangle-T_{M}(\\left\\langle q,\\varepsilon \\right\\rangle)\\right|^{2}=\\mathbb{E}[\\left|h(q)-h^{M}(q)\\right|^{2}]\\le 1/M.$$ Finally, $$\\left\\langle H(q),H(q\u0026rsquo;) \\right\\rangle=\\frac{1}{t} \\sum_{\\varepsilon\\in V}(\\left\\langle q,\\varepsilon \\right\\rangle \\cdot \\left\\langle q\u0026rsquo;,\\varepsilon \\right\\rangle)=\\mathbb{E}[h(q)\\cdot h(q\u0026rsquo;)]=\\left\\langle q,q\u0026rsquo; \\right\\rangle.$$ $\\blacksquare$\nWhat have we achieved so far: A tool based on $\\pm1$-valued vectors, that allows us to compute inner products of unit vectors using inner products with sign vectors, to a good degree of accuracy (in the truncation functions).\nThe Rounding Procedure Given solutions $u_{i},v_{j}\\in\\mathbb{R}^{p}$ that achieve a $\\delta$ approximation of the SDP, whose optimal value we denote by $B$, it follows that $$B-\\delta\\le \\sum_{ij}a_{ij} \\cdot \\left\\langle u_{i},v_{j} \\right\\rangle=\\sum_{ij}a_{ij}\\left\\langle H(u_{i}),H(v_{j}) \\right\\rangle.$$ Note that $$\\left\\langle H(u_{i}),H(v_{j}) \\right\\rangle=\\left\\langle H^{M}(u_{i}),H^{M}(v_{j}) \\right\\rangle+\\left\\langle (H(u_{i})-H^{M}(u_{i})),H^{M}(v_{j}) \\right\\rangle+\\left\\langle H(u_{i}),(H(v_{j})-H^{M}(v_{j})) \\right\\rangle.$$ By Cauchy-Schwartz, using that fact that the norms of $H^{M}(v_{j}),H(u_{i})$ are at most $1$ and the norm of $H(u_{i})-H^{M}(u_{i}),H(v_{j})-H^{M}(v_{j})$ are at most $1/M$, we obtain $$\\left\\langle H(u_{i}),H(v_{j}) \\right\\rangle\\le \\left\\langle H^{M}(u_{i}),H^{M}(v_{j}) \\right\\rangle+\\frac{2}{M}.$$ Thus $$B-\\delta\\le \\sum_{ij}a_{ij}\\left\\langle H^{M}(u_{i}),H^{M}(v_{j}) \\right\\rangle+\\sum_{ij}a_{ij}\\cdot \\frac{2}{M}\\le \\sum_{ij}a_{ij}\\left\\langle H^{M}(u_{i}),H^{M}(v_{j}) \\right\\rangle+\\frac{2B}{M},$$ where the second inequality follows from the fact $\\sum_{ij}a_{ij}\\le B$ due to the maximality of $B$. This proves $$B\\left(1-\\frac{2}{M}\\right)-\\delta\\le \\sum_{ij}a_{ij}\\left\\langle H^{M} (u_{i}),H^{M}(v_{j}) \\right\\rangle.$$\nLemma There exists a vector $\\varepsilon\\in V$ for which $$\\sum_{ij}a_{ij}H^{M}(u_{i})_{\\varepsilon}\\cdot H^{M}(v_{j})_\\varepsilon \\ge \\frac{1}{t}\\cdot \\left[B\\left(1-\\frac{2}{M}\\right)-\\delta\\right].$$\nProof. Suppose otherwise, then $$\\begin{aligned} \\sum_{ij}a_{ij}\\left\\langle H^{M}(u_{i}),H^{M}(v_{j}) \\right\\rangle \u0026amp;= \\sum_{ij}a_{ij}\\sum_{\\varepsilon\\in V}H^{M}(u_{i})_\\varepsilon\\cdot H^{M}(v_{j})_\\varepsilon\\\\ \u0026amp;= \\sum_{\\varepsilon\\in V}\\sum_{ij}a_{ij}H^{M}(u_{i})_\\varepsilon \\cdot H^{M}(v_{j})_\\varepsilon \\\\ \u0026amp;\u0026lt; \\sum_{\\varepsilon\\in V}\\frac{1}{t}[B(1-2/M)-\\delta]=B(1-2/M)-\\delta,\\end{aligned}$$ contradicting the previous inequality. $\\blacksquare$\nFor such $\\varepsilon$ we have by definition of $H^{M}$ and $h^{M}$ that $$\\sum_{ij}a_{ij}h^{M}(u_{i})(\\varepsilon)\\cdot h^{M}(v_{j})(\\varepsilon)\\ge B\\left(1-\\frac{2}{M}\\right)-\\delta.$$ Thus choosing $M=3$ and letting $x_{i}=\\frac{h^{M}(u_{i})(\\varepsilon)}{M}$ and $y_{j}=\\frac{h^{M}(v_{j})(\\varepsilon)}{M}$, we obtain that $\\left|x_{i}\\right|,\\left|y_{j}\\right|\\le 1$ and $$\\sum_{ij}a_{ij}x_{i}y_{j}\\ge \\frac{B(1-2/M)-\\delta}{M^{2}}=\\frac{B/3-\\delta}{9}=\\frac{B}{27}-\\frac{\\delta}{9}.$$ By keeping the same signs for $x_{i},y_{j}$ but shifting them to $\\pm1$, the objective cannot decrease.\nThis gives a rounded solution in polynomial time, achieving an approximation factor of at least $0.03 \u0026lt; 1/27$.\nConclusion In the next post, we\u0026rsquo;ll see another algorithm, which is Randomized and based on Grothendieck\u0026rsquo;s inequality, that achieves much better approximation of the Cut-Norm. Essentially, this post was about reducing the problem to an SDP, and providing a very smart way to round the solution. These are, of course, not my ideas, and they were proposed by Alon and Naor in their influential paper cited below.\nReferences Alon, N., \u0026amp; Naor, A. (2006). Approximating the Cut-Norm via Grothendieck\u0026rsquo;s Inequality . SIAM Journal on Computing. Grothendieck, A. (1953). Résumé de la théorie métrique des produits tensoriels topologiques . Boletim da Sociedade de Matemática de São Paulo. Håstad, J. (2001). Some optimal inapproximability results . Journal of the ACM. ","title":"Approximating the Cut-Norm - Part 1"},{"link":"/posts/cut-norm-2/","text":" Prerequisites Linear Algebra: Eigenvalues, PSD matrices, Tensor products. Convex Optimization: Basic familiarity with Semidefinite Programming (SDP). Probability: Expectations, Markov\u0026rsquo;s inequality. Graph Theory: Basic definitions, Cuts, Regularity. We are interested in computing the cut norm of a matrix, defined by $$\\|A\\|_C=\\max_{I\\subset R,J\\subset S}\\left|\\sum_{i\\in I,j\\in J}a_{i,j}\\right|$$ where $A=(a_{i,j})_{i\\in R,j\\in S}$. We\u0026rsquo;ve seen this is a hard problem, and it is often equivalent to computing the $\\infty\\mapsto 1$ norm, defined by $$\\|A\\|_{\\infty\\mapsto 1}=\\max_{x\\in \\set{\\pm 1}^R, y\\in \\set{\\pm1}^S} \\sum_{i\\in R,j\\in S} a_{i,j}\\cdot x_i\\cdot y_j$$ We\u0026rsquo;ve already seen that computing the latter norm can be done by solving an integer quadratic program, which has a relaxation to a quadratically constrained quadratic program given by $$\\max \\sum_{i,j}a_{i,j}\\cdot \\langle u_i, v_j\\rangle \\quad\\text{subject to}\\quad \\|u_i \\|^2 = \\|v_j\\|^2=1$$ where the optimization is over vectors $u_1,\\ldots,u_n$ and $v_1,\\ldots ,v_m$. We\u0026rsquo;ve seen that this problem can be solved using semi-definite programming, and we\u0026rsquo;ve seen one method to round the solution, giving an approximation factor of $\\approx 0.03$. In this post, we\u0026rsquo;ll see another method, which is much cleaner, and uses randomized rounding of this SDP.\nRandomized Improvement This method is based on Grothendieck\u0026rsquo;s inequality, which has turned out to be a fundamental result in computer science. Let us first recall some definitions:\nReminder A Hilbert space is a vector space with an inner product, which is also complete. Every finite dimensional inner product space is complete. Completeness means that Cauchy sequences converge (if $\\left\\lVert x_{n}-x_{m} \\right\\rVert\\to 0$ then the sequence $(x_n)$ has a limit point).\nReminder Let $\\mathcal{H}$ denote a Hilbert space and $u,v\\in \\mathcal{H}$, then $\\left\\langle u,v \\right\\rangle^{k}=\\left\\langle u^{\\otimes k},v^{\\otimes k} \\right\\rangle$ where $u^{\\otimes k},v^{\\otimes k}\\in \\mathcal{H}^{\\otimes k}$ are elements in the tensor product space. For more background on tensor products see this post where the tensor product of vector spaces is discussed, and for the proof of the assertion made here see this post where the polynomial kernel is discussed.\nThe main identity we need is:\nGrothendieck\u0026rsquo;s Identity For every two vectors $u,v\\in \\mathcal{H}$ in a Hilbert space, if $z$ is chosen randomly and uniformly at random from the unit sphere of $\\mathcal{H}$, then $$\\frac{\\pi}{2}\\mathbb{E}_{z}[\\mathrm{sign}(\\left\\langle u,z \\right\\rangle)\\cdot \\mathrm{sign}(\\left\\langle v,z \\right\\rangle)]=\\arcsin(\\left\\langle u,v \\right\\rangle).$$\nIn other words, by randomly projecting $u$ and $v$ onto the same unit vector, we are able to estimate their inner product.\nLemma Let $c=\\sinh^{-1}(1)=\\ln (1+\\sqrt{2})$. For any set $\\lbrace u_{i}\\rbrace_{i=1}^{n}$ and $\\lbrace v_{j}\\rbrace_{j=1}^{m}$ of unit vectors in a Hilbert space $\\mathcal{H}$, there is a set $\\lbrace u_{i}\u0026rsquo;\\rbrace_{i=1}^{n}$ and $\\lbrace v_{j}\u0026rsquo;\\rbrace_{j=1}^{m}$ of unit vectors in a Hilbert space $\\mathcal{H}\u0026rsquo;$, such that for $z$ chosen randomly and uniformly from the unit sphere of $\\mathcal{H}\u0026rsquo;$ it holds for every $i,j$: $$\\frac{\\pi}{2}\\mathbb{E}_{z}[\\mathrm{sign}(\\left\\langle u_{i}\u0026rsquo;,z \\right\\rangle)\\cdot \\mathrm{sign}(\\left\\langle v_{j}\u0026rsquo;,z \\right\\rangle)]=c \\cdot \\left\\langle u_{i},v_{j} \\right\\rangle.$$\nProof. By the identity, the expectation is just $\\arcsin(\\left\\langle u_{i}\u0026rsquo;,v_{j}\u0026rsquo; \\right\\rangle)$. So we need to choose $u_{i}\u0026rsquo;,v_{j}\u0026rsquo;$ such that $\\left\\langle u_{i}\u0026rsquo;,v_{j}\u0026rsquo; \\right\\rangle=\\sin(c\\cdot \\left\\langle u_{i},v_{j} \\right\\rangle)$. Using the Taylor expansion of the sine function, $$\\sin(c\\cdot \\left\\langle u,v \\right\\rangle)=\\sum_{k=0}^{\\infty}(-1)^{k}\\frac{c^{2k+1}}{(2k+1)!}\\cdot(\\left\\langle u,v \\right\\rangle)^{2k+1}=\\sum_{k=0}^{\\infty}(-1)^{k}\\frac{c^{2k+1}}{(2k+1)!}\\cdot \\left\\langle u^{\\otimes 2k+1},v^{\\otimes2k+1} \\right\\rangle.$$ Define $\\mathcal{H}\u0026rsquo;=\\bigoplus_{k=0}^{\\infty}\\mathcal{H}^{\\otimes 2k+1}$, and define $u\u0026rsquo;=\\bigoplus_{k=0}^{\\infty}(-1)^{k}\\sqrt{\\frac{c^{2k+1}}{(2k+1)!}}u^{\\otimes2k+1}$ and $v\u0026rsquo;=\\bigoplus_{k=0}^{\\infty}\\sqrt{\\frac{c^{2k+1}}{(2k+1)!}}v^{\\otimes2k+1}$. Then $$\\left\\langle u\u0026rsquo;,v\u0026rsquo; \\right\\rangle=\\sin(c\\cdot \\left\\langle u,v \\right\\rangle),$$ and also $$\\left\\lVert u\u0026rsquo; \\right\\rVert^{2}=\\left\\langle u\u0026rsquo;,u\u0026rsquo; \\right\\rangle=\\sum_{k=0}^{\\infty}(-1)^{2k}\\frac{c^{2k+1}}{(2k+1)!}\\left\\lVert u^{\\otimes2k+1} \\right\\rVert^{2}=\\sum_{k=0}^{\\infty}\\frac{c^{2k+1}(\\left\\lVert u \\right\\rVert^{2})^{2k+1}}{(2k+1)!}=\\sinh(c \\cdot \\left\\lVert u \\right\\rVert^{2}).$$ Similarly, $\\left\\lVert v\u0026rsquo; \\right\\rVert^{2}=\\sinh(c\\cdot \\left\\lVert v \\right\\rVert^{2})$, and since we assumed $\\left\\lVert u \\right\\rVert=\\left\\lVert v \\right\\rVert=1$, we obtain that $\\left\\lVert u\u0026rsquo; \\right\\rVert=\\left\\lVert v\u0026rsquo; \\right\\rVert=\\sinh(c)=\\sinh(\\sinh^{-1}(1))=1$, as required. $\\blacksquare$\nThe existence is proved constructively but gives an infinite dimensional Hilbert space. However, we are essentially interested in a small sub-space of dimension at most $n+m$. Thus, given $\\lbrace u_{i}\\rbrace_{i=1}^{n}$ and $\\lbrace v_{j}\\rbrace_{j=1}^{m}$ we can find $\\lbrace u_{i}\u0026rsquo;\\rbrace $ and $\\lbrace v_{j}\u0026rsquo;\\rbrace $ satisfying the above by solving the following SDP: $$\\max_{\\alpha^{i},\\beta^{j}\\in\\mathbb{R}^{n+m}}0\\quad \\text{subject to}\\quad \\left\\langle \\alpha^{i},\\beta^{j} \\right\\rangle=\\sin (c\\cdot \\left\\langle u_{i},v_{j} \\right\\rangle),\\quad \\|\\alpha^i\\|^2 = \\|\\beta^j\\|^2 = 1.$$ This is essentially a feasibility problem (which we just proved is feasible). Any solution is optimal.\nTheorem There is a randomized algorithm achieving $\\rho=\\frac{2\\ln(1+\\sqrt{2})}{\\pi}\u0026gt;0.56$ approximation in expectation for the problem of computing $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$. The runtime of the algorithm is polynomial.\nProof. Following the discussion above, we can find the vectors $u_{i}\u0026rsquo;,v_{j}\u0026rsquo;$ (at least a small representation of them) by solving the SDP written above. Picking uniformly at random a vector $z\\in\\mathbb{R}^{n+m}$ on the unit sphere, and outputting $x_{i}=\\mathrm{sign}(\\left\\langle u_{i}\u0026rsquo;,z \\right\\rangle)$ and $y_{j}=\\mathrm{sign}(\\left\\langle v_{j}\u0026rsquo;,z \\right\\rangle)$, we know by the previous lemma that $$\\frac{\\pi}{2}\\mathbb{E}\\left[\\sum_{i,j}a_{ij}x_{i}y_{j}\\right]=\\sum_{i,j}a_{i,j}\\cdot c\\cdot \\left\\langle u_{i},v_{j} \\right\\rangle,$$ and therefore on average we obtain a $\\frac{2c}{\\pi}$ approximation of $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}$. $\\blacksquare$\nObtaining an Approximation for the Cut-Norm We\u0026rsquo;re still to obtain an approximation for the cut-norm. Recall that we\u0026rsquo;ve shown above that when the rows and columns of the matrix $A$ sum to zero, it holds $\\left\\lVert A \\right\\rVert_{\\infty\\mapsto 1}=4\\left\\lVert A \\right\\rVert_{C}$. Given a matrix $A$ of size $n\\times m$, we can define $A\u0026rsquo;$ to be a $(n+1)\\times (m+1)$ matrix, by setting $$a_{i,m+1}=-\\sum_{k=1}^{m}a_{i,k}\\quad,\\quad a_{n+1,j}=-\\sum_{k=1}^{n}a_{k,j}\\quad, \\quad a_{n+1,m+1}=0,$$ for every $i\\in [n],j\\in [m]$. This way, the rows and columns of $A\u0026rsquo;$ all sum to zero.\nLemma The cut norms are equal $\\left\\lVert A \\right\\rVert_{C}=\\left\\lVert A\u0026rsquo; \\right\\rVert_{C}$.\nProof. We have a trivial upper bound $\\left\\lVert A \\right\\rVert_{C}\\le \\left\\lVert A\u0026rsquo; \\right\\rVert_{C}$, because $A$ is a sub-matrix of $A\u0026rsquo;$. In the other direction, suppose $I\u0026rsquo;\\subset [n+1]$, $J\u0026rsquo;\\subset [m+1]$ are maximizers for $A\u0026rsquo;$. If $n+1\\in I\u0026rsquo;$, define $I=[n+1]\\setminus I\u0026rsquo;$ to be the complement, and otherwise set $I=I\u0026rsquo;$. Similarly, if $m+1\\in J\u0026rsquo;$ define $J=[m+1]\\setminus J\u0026rsquo;$ and otherwise $J=J\u0026rsquo;$. Note that $I\\subset [n]$ and $J\\subset [m]$. If $I\\not=I\u0026rsquo;$, then $$\\sum_{i\\in I,j\\in J}a_{ij}=\\sum_{i\\in [n],j\\in J}a_{ij}-\\sum_{i\\in [n]\\setminus I,j\\in J}a_{ij}=-\\sum_{j\\in J}a_{n+1,j}-\\sum_{i\\in [n]\\setminus I,j\\in J}a_{ij}=-\\sum_{i\\in I\u0026rsquo;,j\\in J}a_{ij},$$ by definition of $I$. Similarly, if $J\\not=J\u0026rsquo;$, then $$\\sum_{i\\in I\u0026rsquo;,j\\in J}a_{ij}=\\sum_{i\\in I\u0026rsquo;,j\\in [m]}a_{ij}-\\sum_{i\\in I\u0026rsquo;,j\\in [m]\\setminus J}a_{ij}=-\\sum_{i\\in I\u0026rsquo;}a_{i,m+1}-\\sum_{i\\in I\u0026rsquo;,j\\in [m]\\setminus J}a_{ij}=-\\sum_{i\\in I\u0026rsquo;,j\\in J\u0026rsquo;}a_{ij}.$$ Either way, we obtain that $$\\left|\\sum_{i\\in I,j\\in J}a_{ij}\\right|=\\left|\\sum_{i\\in I\u0026rsquo;,j\\in J\u0026rsquo;}a_{ij}\\right|,$$ and therefore $\\left\\lVert A \\right\\rVert_{C}\\ge \\left\\lVert A\u0026rsquo; \\right\\rVert_{C}$. $\\blacksquare$\nThus, given a matrix $A$, we construct the matrix $A\u0026rsquo;$ and compute a $\\rho$-approximation for $\\left\\lVert A\u0026rsquo; \\right\\rVert_{\\infty\\mapsto 1}$, multiply by $4$, to obtain a $\\rho$-approximation for $\\left\\lVert A\u0026rsquo; \\right\\rVert_{C}=\\left\\lVert A \\right\\rVert_{C}$.\nProving the Identity Grothendieck\u0026rsquo;s Identity For every two unit vectors $u,v\\in \\mathcal{H}$ in a Hilbert space, if $z$ is chosen randomly and uniformly at random from the unit sphere of $\\mathcal{H}$, then $$\\frac{\\pi}{2}\\mathbb{E}_{z}[\\mathrm{sign}(\\left\\langle u,z \\right\\rangle)\\cdot \\mathrm{sign}(\\left\\langle v,z \\right\\rangle)]=\\arcsin(\\left\\langle u,v \\right\\rangle).$$\nProof. Let $z$ denote some unit vector, then $\\mathrm{sign}(\\left\\langle u,z \\right\\rangle)$ has the following geometric interpretation - it is $1$ when $u$ is inside the upper half of the sphere, assuming we orient it so that $z$ is the north pole. Therefore, $\\mathrm{sign}(\\left\\langle u,z \\right\\rangle)\\cdot \\mathrm{sign}(\\left\\langle v,z \\right\\rangle)$ is $1$ only when $u,v$ lie on the same half of the sphere and $-1$ otherwise. Thus the expectation becomes $$\\Pr(u,v \\text{ lie on the same half})- \\Pr(u,v \\text{ lie on different halves}).$$ We know that $\\left\\langle u,v \\right\\rangle=\\cos \\theta$ where $\\theta\\in [0,\\pi]$ is the angle between them. The probability that $u,v$ lie on the same half of the sphere boils down to how the equator (the orthogonal hyper-plane to $z$) splits up the unique dimensional circle on which $u,v$ both reside. In other words, we randomly choose a line (passing through the origin) that splits up the circle into two halves, and we ask whether $u,v$ are in the same half. Note that $u,v$ are not in the same half if and only if the line intersects the circle at a point which sits on the arc connecting $u,v$. That arc has length $\\theta$ (because this is the unit circle), and this intersection point uniquely defines the line, so we choose a random point with uniform probability out of the total arc of length $\\pi$. Thus the probability of $u,v$ not being in the same half is exactly $\\theta/\\pi$. Hence $$\\mathbb{E}[\\mathrm{sign}(\\left\\langle u,z \\right\\rangle)\\cdot \\mathrm{sign}(\\left\\langle v,z\\right\\rangle)]=\\left(1-\\frac{\\theta}{\\pi}\\right)-\\frac{\\theta}{\\pi}=1-\\frac{2 \\theta}{\\pi},$$ and the identity follows from the trigonometric identity: $$\\arcsin(\\left\\langle u,v \\right\\rangle)=\\arcsin(\\cos \\theta)=\\arcsin\\left( \\sin\\left(\\frac{\\pi}{2}-\\theta\\right)\\right)=\\frac{\\pi}{2}-\\theta=\\frac{\\pi}{2}\\left(1-\\frac{2\\theta}{\\pi}\\right).$$ $\\blacksquare$\nConclusion We have explored two methods for approximating the cut-norm of a matrix. The deterministic algorithm relied on a vector programming relaxation and a careful derandomization using orthogonal arrays, achieving a modest approximation ratio of $0.03$. The randomized improvement utilized the power of Grothendieck\u0026rsquo;s Identity to lift the problem into an infinite-dimensional Hilbert space and then project it back, yielding a much stronger ratio of $\\approx 0.56$.\nReferences Alon, N., \u0026amp; Naor, A. (2006). Approximating the Cut-Norm via Grothendieck\u0026rsquo;s Inequality . Grothendieck, A. (1953). Résumé de la théorie métrique des produits tensoriels topologiques . Håstad, J. (2001). Some optimal inapproximability results . Journal of the ACM. ","title":"Approximating the Cut-Norm - Part 2"},{"link":"/posts/conj-gradient/","text":" Prerequisites Linear Algebra: Eigenvalues, eigenvectors, positive (semi-)definite (PSD) matrices, and the notion of orthogonality. Calculus: Gradients and basic convexity. Optimization: Gradient Descent (GD) basics. One of the most common tasks in numerical algorithms is to solve a linear equation—that is, find $x$ for which $$Ax=b$$ for a given matrix $A$ and vector $b$. This can be solved via Gaussian elimination, which generally has a high runtime ($O(n^3)$). We will show how to improve upon this using optimization ideas. This is one instance of a problem for which we can find an approximate solution much faster using calculus tools, compared with using a close-form exact algebraic solution.\nWe will make the following assumption: $A$ is symmetric and positive definite. This means $A$ has eigenvalues $0\u0026lt;\\lambda_{1}\\le \\ldots \\le \\lambda_{n}$. In particular, $A$ is square, $A\\in \\mathbb{R}^{n\\times n}$, and $x,b\\in\\mathbb{R}^{n}$.\nAs an Optimization Problem Solving $Ax=b$ is equivalent to minimizing the \u0026ldquo;distance\u0026rdquo; $Ax-b$. However, $Ax-b$ is a vector, so it cannot be minimized directly. To transform $Ax-b$ into a meaningful scalar, several approaches are possible. We could minimize $\\left\\lVert Ax-b \\right\\rVert_{2}$ (the least-squares problem), or any other norm for that matter. Instead, we choose to take the inner product with $x$, leading to the quadratic form: $$f(x):=\\frac{1}{2}x^{\\top}Ax -x^{\\top}b.$$ The gradient of $f$ is $\\nabla f(x)=Ax-b$, while the Hessian matrix is $\\nabla^2 f(x)=A$. It is a general fact that quadratic functions with a positive-definite Hessian matrix are strongly convex. We thus conclude:\nLemma $x$ is a minimizer of $f$ if and only if $Ax=b$. Moreover, $f$ is strongly convex, so it has a unique global minimum, $x^{*}$.\nProof. As noted above, $\\nabla f(x)=Ax-b$; hence $Ax=b$ iff $\\nabla f(x)=0$. We know $\\nabla f(x)=0$ is a necessary condition for $x$ to be a minimizer. However, it is also sufficient because $\\nabla^{2}f=A$ is positive definite, implying $\\nabla f$ can have at most a single root. That root must be a minimizer, as the curvature of $f$ implies there are no saddle points. $\\blacksquare$\nSince $f$ is convex, we can use a general solver, like Gradient Descent (GD), to find a minimizer. GD requires computing gradients, which in our case is just $\\nabla f(x)=Ax-b$. Therefore, each GD iteration requires time $T_{A}$, which is the time needed to compute the matrix-vector product $Ax$ (this dominates the computation). Hopefully, if the number of iterations is small, we need a small number of such products. Put explicitly, each iteration in the GD method is: $$x_{t+1}=x_{t}- \\eta_{t}\\cdot \\nabla f(x_{t})$$ where $\\eta_{t}$ is called the step size at time $t$. The following theorem is a general result about the rate of convergence for gradient descent:\nTheorem With a proper choice of step sizes, for every $\\varepsilon\u0026gt;0$, the GD algorithm finds a vector $x$ such that $$\\sqrt{(x-A^{-1}b)^{\\top}A(x-A^{-1}b)}\\le \\varepsilon \\sqrt{(A^{-1}b)^{\\top}A(A^{-1}b)},$$ in time $O(T_{A}\\cdot \\kappa(A)\\cdot \\log (1/\\varepsilon)))$, where $\\kappa(A)=\\frac{\\lambda_{n}}{\\lambda_{1}}$ is called the condition number of $A$.\nTo understand the meaning of the bounds, note that $A^{-1}b=x^*$, and so $x-A^{-1}b=x-x^*$, i.e., the difference between the solution and the optimal solution. Moreover, note that $$ \\begin{aligned} (x-A^{-1}b)^{\\top}A(x-A^{-1}b)\u0026amp;= x^{\\top} Ax - x^{\\top}AA^{-1}b - b^{\\top}(A^{-1})^{\\top}Ax + b^{\\top}(A^{-1})^{\\top}AA^{-1}b \\\\ \u0026amp;= x^{\\top}Ax- 2x^{\\top} b+ b^{\\top}x^* \\\\ \u0026amp;= 2f(x)-2f(x^*) \\end{aligned} $$ where we used the fact $A^{-1}$ must also by symmetric and that $f(x^*)=-\\frac{1}{2}b^{\\top}x^{*}$. In other words, the error bound in the theorem is just (divided by $2$): $$(f(x)-f(x^*))\\le \\varepsilon^2\\cdot f(x^*)$$ A proof can be found in [5].\nViewing GD with Krylov Subspaces }_\nDefinition Let $M$ denote a matrix of size $n\\times n$ and $y$ a vector of size $n$. Then the Krylov subspace of order $t$ generated by $M,y$ is $$\\mathrm{Span}\\set{y,\\ My,\\ \\ldots ,M^{t-1}y}.$$ In other words, we repeatedly apply $M$ to obtain a sequence of $t$ vectors starting from $y$.\nSuppose we initiate $x_{0}=0$. Then $$\\nabla f(x_{0})=Ax_{0}-b=-b.$$ Consequently, $$x_{1}=x_{0}-\\eta_{0}\\nabla f(x_{0})=\\eta_{0}b,$$ and similarly, $$x_{2}=x_{1}-\\eta_{1}(Ax_{1}-b)=\\eta_{0}\\cdot b-\\eta_{1}\\eta_{0}\\cdot Ab+\\eta_{1}\\cdot b=(\\eta_{0}+\\eta_{1})b-\\eta_{1}\\eta_{0} Ab.$$ Therefore, a simple inductive argument shows that $$x_{t}\\in \\mathrm{Span}\\set{b,Ab,\\ldots ,A^{t-1}b}.$$ Denote the $t$-th order Krylov subspace for $A,b$ by $\\mathcal{K}_{t}$. Then we see GD moves $x_{1},\\ldots,x_{t}$ in $\\mathcal{K}_{1},\\ldots,\\mathcal{K}_{t}$. In other words, $x_i\\in \\mathcal{K}_i$.\nHowever, GD does not guarantee that $x_{t}$ is the minimizer of $f$ in $\\mathcal{K}_{t}$. The idea of the Conjugate Gradient (CG) method is, at each step, to minimize over $\\mathcal{K}_{t}$. In particular, $x^{*}$, the true minimizer, is the minimizer over $\\mathcal{K}_{n}$.\nFinding the Minimizer in a Krylov Subspace Question. How can we quickly find the minimizer of $f$ in $\\mathcal{K}_{t}$?\nTo answer this question, we start with the following observation:\nObservation We can assume without loss of generality that $\\dim \\mathcal{K}_{t}=t$. Otherwise, suppose $\\dim \\mathcal{K}_{t}=t-1$; then $A^{t-1}b$ is linearly dependent on $\\mathcal{K}_{t-1}$, which means $A^{k}b$ is also linearly dependent on $\\mathcal{K}_{t-1}$ for every $k\\ge t$. In particular, $\\mathcal{K}_{t-1}=\\mathcal{K}_{n}$, and so by minimizing over $\\mathcal{K}_{t-1}$ we are done.\nSuppose we were given a basis $\\set{p_{0},\\ldots,p_{t-1}}$ for $\\mathcal{K}_{t}$, with the following separability property: $$f\\left(\\sum_{i=0}^{t-1} \\beta_{i}p_{i}\\right)=\\sum_{i=0}^{t-1}f(\\beta_{i}p_{i}),$$ for every $\\beta_{0},\\ldots,\\beta_{t-1}$. Then, to minimize $f$ over $\\mathcal{K}_{t}$, it suffices to find $\\alpha_{i}$ for all $i$, where $\\alpha_i$ is the scalar minimizing $f(\\alpha p_{i})$. In particular, when moving from $\\mathcal{K}_{t-1}$ to $\\mathcal{K}_{t}$, to minimize $f$ over $\\mathcal{K}_{t}$ we only need to add $\\alpha_{t-1}p_{t-1}$ to the previous minimizer.\nThis separability property clearly holds for linear functions, so for our objective function $f(x)=\\frac{1}{2}x^{\\top}Ax+b^{\\top}x$ (ignoring the constant), only the quadratic part $x^{\\top}Ax$ might be the problem. Note that the left hand side is $$\\left(\\sum_{i=0}^{t-1}\\beta_{i}p_{i}\\right)^{\\top}A\\left(\\sum_{i=0}^{t-1}\\beta_{i}p_{i}\\right)=\\sum_{0\\le i,j\\le t-1}\\beta_{i}\\beta_{j} p_{i}^{\\top}Ap_{j}$$ while the right hand side is $\\sum_{i=0}^{t-1}(\\beta_{i}p_{i})^{\\top}A(\\beta_{i}p_{i})=\\sum_{i} \\beta_{i}^{2}p_{i}^{\\top}Ap_{i}$. Hence, for equality to hold, we need the basis $\\set{p_{0},\\ldots,p_{t-1}}$ to be $A$-orthonormal:\nDefinition Given a symmetric matrix $A$, a set of vectors $v_{1},\\ldots,v_{t}$ is $A$-orthonormal (or $A$-conjugate) if $$v_{i}^{\\top}Av_{j}=0\\quad \\forall i\\not=j.$$\nComputing an $A$-Orthonormal Basis Gram-Schmidt Iterations We wish to iteratively compute an $A$-orthonormal basis $\\set{p_{0},\\ldots,p_{t-1}}$ for $\\mathcal{K}_{t}$. One way to do this is via Gram-Schmidt iterations:\nGiven a new vector $v\\notin \\mathcal{K}_{t}$ (specifically $v=A^{t}b$), Define $$p_{t}=v-\\sum_{i\\le t-1}\\frac{v^{\\top}Ap_{i}}{p_{i}^{\\top}Ap_{i}}p_{i}.$$ Since $p_{i}^{\\top}Ap_{j}=0$ when $i\\not=j$, we obtain that $$p_{t}^{\\top}Ap_{j}=v^{\\top}A p_{j}-\\sum_{i\\le t-1}\\frac{v^{\\top}Ap_{i}}{p_{i}^{\\top}Ap_{i}}p_{i}^{\\top}Ap_{j}=v^{\\top}Ap_{j}-\\frac{v^{\\top}Ap_{j}}{\\cancel{p_{j}^{\\top}Ap_{j}}}\\cdot \\cancel{ p_{j}^{\\top}Ap_{j}}=0$$ for every $j\\le t-1$. However, to compute $p_{t}$ in this manner we need $O(t)$ matrix-vector products. Moreover, we haven\u0026rsquo;t really used the fact that we are computing a basis for the Krylov subspace generated by $A$, which gives us extra structure to work with.\nA Slight Adjustment The ida: If we start with $p_0=b$ and choose $v$ to be $Ap_{t-1}$, the Gram-Schmidt iterations simplify and are faster to compute, involving only a constant number of matrix-vector products.\nFormally, let $p_{0}=b$ be the first vector, and assume we have constructed an $A$-orthonormal basis for $\\mathcal{K}_{t}$, denoted by $\\set{p_{0},\\ldots,p_{t-1}}$, where the induction hypothesis states $$\\mathcal{K}_{i}=\\mathrm{Span}\\set{p_{0},\\ldots,p_{i-1}}$$ for every $i=1,\\ldots,t-1$. In particular, $Ap_{i-1}\\in \\mathcal{K}_{i+1}$. We want to construct a vector $p_{t}$ so that the statement above is true for $i=t+1$.\nAs discussed in the observation above, if $Ap_{t-1}\\in \\mathcal{K}_{t}$, then $\\mathcal{K}_{t}=\\mathcal{K}_{n}$ and so we have reached the end of the iteration. Therefore we may assume without loss of generality that $Ap_{t-1}\\notin \\mathcal{K}_{t-1}$. Define $$p_{t}=A p_{t-1}-\\sum_{i\\le t-1}\\frac{(Ap_{t-1})^{\\top} Ap_{i}}{p_{i}^{\\top}Ap_{i}}p_{i}.$$\nBy definition, $Ap_{t-1}\\in \\mathcal{K}_{t+1}$, and also $\\mathcal{K}_{t+1}=\\mathrm{Span}\\set{p_{0},\\ldots,p_{t}}$ (using the induction hypothesis). By the induction hypothesis, for every $0\\le j\u0026lt;t-1$ it holds that $Ap_{j}\\in \\mathcal{K}_{j+2}$. This implies $Ap_j$ is a linear combination of $p_{0},\\ldots,p_{j+1}$, and in particular there are scalars $c_{0},\\ldots,c_{j+1}$ such that $Ap_{j}=\\sum_{i=0}^{j+1}c_{i}p_{i}$. Hence: $$(Ap_{t-1})^{\\top}Ap_{j}=p_{t-1}A^{\\top}Ap_{j}=p_{t-1}A^{\\top}\\left(\\sum_{i=0}^{j+1}c_{i}p_{i}\\right)=\\sum_{i=0}^{j+1}c_{i}\\cdot p_{t-1}^{\\top}Ap_{i}.$$ Due to $A$-orthogonality, $p_{t-1}^{\\top}Ap_{i} = 0$ unless $i = t-1$. Thus: $$(Ap_{t-1})^{\\top}Ap_{j} = \\begin{cases} 0 \u0026amp; j\u0026lt;t-2, \\\\ c_{t-1}p_{t-1}^{\\top}Ap_{t-1} \u0026amp; j=t-2. \\end{cases}$$ Therefore, the definition of $p_{t}$ is essentially a sum of three elements only. In other words, the definition simplifies to a three-term recurrence: $$p_{t}=Ap_{t-1}-\\frac{p_{t-1}^{\\top}A^{2}p_{t-1}}{p_{t-1}^{\\top}Ap_{t-1}}p_{t-1}-\\frac{p_{t-1}^{\\top}A^{2}p_{t-2}}{p_{t-2}^{\\top}Ap_{t-2}}p_{t-2},$$ which implies we need a constant number of matrix-vector products to compute the next element of the basis.\nThe Conjugate Gradient Algorithm Algorithm: Conjugate Gradient Input: A symmetric, positive definite matrix $A$, vector $b$, and iteration count $T$. Output: A vector $x_{T}$ obtained after $T$ iterations. Set $r_{0}\\gets b$, and $p_{0}\\gets b$. Set $x_0 \\gets 0$. For $t=0,\\ldots,T-1$: Set $\\alpha_{t}=\\frac{r_{t}^{\\top}p_{t}}{p_{t}^{\\top}Ap_{t}}$. Set $x_{t+1}=x_{t}+\\alpha_{t}p_{t}$. Set $r_{t+1}=b-Ax_{t+1}$. Compute $p_{t+1}$ to be $A$-orthogonal to $p_t$ and $p_{t-1}$ using the recurrence derived above. Return $x_{T}$. The choice of $\\alpha_{t}$ is done to minimize $f(\\alpha p_{t})=\\frac{1}{2}\\alpha^{2}p_{t}^{\\top}Ap_{t}- \\alpha b^{\\top}p_{t}$, viewed as a function $\\mathbb{R}\\to \\mathbb{R}$. The number of iterations needed for a certain prescribed precision is therefore dependent on the convergence of the conjugate gradient algorithm (which is always better than the gradient descent algorithm).\nPolynomial Minimization Let $x^{*}$ denote the minimizer, satisfying $Ax^{*}=b$. Then $$f(x^{*})=\\frac{1}{2}(x^{*})^{\\top}Ax^{*}-(x^{*})^{\\top}Ax^{*}=-\\frac{1}{2}(x^{*})^{\\top}Ax^{*}.$$ Thus at iteration $t$ we have $$f(x_{t})-f(x^{*})=\\frac{1}{2}(x_{t}^{\\top}Ax_{t}+(x^{*})^{\\top}Ax^{*}-2x_{t}^{\\top}Ax^{*})=\\frac{1}{2}(x_{t}-x^{*})^{\\top}A(x_{t}-x^{*}),$$ using the symmetry of $A$. This is the derivation as in the error of the gradient descent theorem.\nDefinition Define the $A$-norm as $\\left\\lVert v \\right\\rVert_{A}=\\sqrt{v^{\\top}Av}$. It is a well-defined norm because $A$ is symmetric and positive definite.\nObservation Let $x_t\\in \\mathcal{K}_{t}$; then it can be written as the linear combination $\\gamma_{0},\\ldots,\\gamma_{t-1}$ of $b,Ab,\\ldots,A^{t-1}b$, i.e., $x_{t}=\\sum_{i=0}^{t-1}\\gamma_{i}A^{i}b$. Define $p(X)=\\sum_{i=0}^{t-1}\\gamma_{i}X^{i}$. Then $$x_t=p(A)b=p(A)(Ax^{*}).$$ Hence $$x_t-x^{*}=p(A)(Ax^{*})-x^{*}=(p(A)A-I)x^{*}.$$ This establishes a bijection between points in $\\mathcal{K}_{t}$ and degree $t-1$ polynomials.\nWe note that evaluating a polynomial at a matrix gives a new matrix which is the weighted sum of matrix powers. Here $A^{0}=I$.\nLet $q(X)=p(X)\\cdot X-1$. Then $q(A)=(p(A)A-I)$ is a degree $t$ polynomial. Moreover, there is a bijection between $$\\mathcal{P}_{t} =\\set{P(X)\\mid \\deg P\\le t-1}\\leftrightarrow \\set{Q(X)\\mid \\deg Q\\le t, Q(0)=-1}=\\mathcal{Q}_{t}.$$ This bijection is given by the transformation of $P\\mapsto Q$, i.e., a polynomial $P$ is mapped to $Q(X)=X\\cdot P(X)-1$.\nObservation We know $x_{t}$ is the minimizer of $f(x)$ over $\\mathcal{K}_{t}$, hence it minimizes $f(x)-f(x^{*})$ over $\\mathcal{K}_{t}$. Thus, $$\\frac{1}{2}\\left\\lVert x_{t}-x^{*} \\right\\rVert_{A}^{2}=\\min_{x\\in \\mathcal{K}_{t}}\\frac{1}{2}\\left\\lVert x -x^{*} \\right\\rVert_{A}^{2}=\\min_{p(X)\\in \\mathcal{P}_{t}}\\frac{1}{2}\\left\\lVert (p(A)A-I)x^{*} \\right\\rVert_{A}^{2}=\\min_{q(X)\\in \\mathcal{Q}_{t}}\\frac{1}{2} \\left\\lVert q(A)x^{*} \\right\\rVert_{A}^{2}.$$\nAnother way to view polynomials of matrices is through their diagonalization. Since $A$ is symmetric, it is orthogonally diagonalizable, meaning that $A=U \\Lambda U^{\\top}$ for a diagonal matrix $\\Lambda$ (with the eigenvalues on the diagonal) and $U$ an orthogonal matrix ($UU^{\\top}=U^{\\top}U=I$). In this case, $$A^{2}=(U\\Lambda U^{\\top})^{2}=U \\Lambda U^{\\top}U\\Lambda U^{\\top}=U \\Lambda^{2}U^{\\top}.$$ Since $\\Lambda$ is diagonal with $\\set{\\lambda_{1},\\ldots,\\lambda_{n}}$, $\\Lambda^{2}$ is just the element-wise product of the diagonals. By induction, this is true for every $k$. Therefore, $$p(A)=\\sum_{i}\\gamma_{i}A^{i}=\\sum_{i}\\gamma_{i}U \\mathrm{diag}(\\lambda_{1}^{i},\\ldots,\\lambda_{n}^{i}) U^{\\top}=U\\left(\\sum_{i}\\gamma_{i} \\mathrm{diag}(\\lambda_{1}^{i},\\ldots, \\lambda_{n}^{i})\\right)U^{\\top}$$ which is equal to $U \\mathrm{diag}(p(\\lambda_{1}),\\ldots,p(\\lambda_{n}))U^\\top$. In words, we apply the polynomial on the eigenvalues to obtain a new diagonal matrix.\nLemma For a symmetric matrix $M$ with non-negative eigenvalues $\\lambda_{1},\\ldots,\\lambda_{n}$, and any polynomial $p(X)$ and vector $v$, it holds that $$(p(M)v)^{\\top}M (p(M)v)\\le v^{\\top}Mv\\cdot \\max_{i}\\left|p(\\lambda_{i})\\right|^{2}.$$\nProof. Write $M=U\\Lambda U^{\\top}$ as above. Note that diagonal matrices commute, hence $$v^{\\top}(p(M))^{\\top}Mp(M)v=v^{\\top}Up(\\Lambda)U^{\\top}U\\Lambda U^{\\top}Up(\\Lambda)U^{\\top}v=v^{\\top}U(\\Lambda\\cdot p^{2}(\\Lambda ))U^{\\top}v.$$ Writing $v=\\sum_{i}c_{i}u_{i}$, where $u_{i}$ are the columns of $U$ (the eigenvectors basis), we have $$v^{\\top}p(M)^{\\top}Mp(M)v=\\sum_{i,j}c_{i}c_{j}u_{i}^{\\top}U(\\Lambda\\cdot p^{2}(\\Lambda))U^{\\top}u_{j}=\\sum_{i}c_{i}^{2} \\lambda_{i}p(\\lambda_{i})^{2},$$ where we used the orthogonality. On the other hand, $v^{\\top}Mv=\\sum_{i}c_{i}^{2}\\lambda_{i}$. The claim follows because all the eigenvalues are non-negative. $\\blacksquare$\nTherefore: $$\\left\\lVert x_{t}-x^{*} \\right\\rVert_{A}^{2}=\\min_{q(X)\\in \\mathcal{Q}_{t}}\\left\\lVert q(A)x^{*} \\right\\rVert_{A}^{2}\\le \\min_{q(X)\\in \\mathcal{Q}_{t}}\\max_{i=1}^{n}\\left|q(\\lambda_{i})\\right|^{2}\\cdot \\left\\lVert x^{*} \\right\\rVert_{A}^{2}.$$ More specifically, we have shown that (by extending the set on which we maximize): $$\\left\\lVert x_{t}-x^{*} \\right\\rVert_{A}^{2}\\le \\left\\lVert x^{*} \\right\\rVert_{A}^{2}\\cdot \\min_{q(X)\\in \\mathcal{Q}_{t}}\\max_{\\lambda\\in [\\lambda_{1},\\lambda_{n}]}\\left|q(\\lambda)\\right|^{2}.$$ In particular, every polynomial $q \\in \\mathcal{Q}_t$ gives an upper bound. For example, take $$q(X)=-\\left(1-\\frac{2X}{\\lambda_{1}+\\lambda_{n}}\\right)^{t},$$ and note $q(0)=-1$ so $q(X)\\in \\mathcal{Q}_{t}$, while $|q(X)|^{2}$ is maximized over $[\\lambda_{1},\\lambda_{n}]$ when $\\lambda=\\lambda_{1}$. Thus $$q(\\lambda_{1})^{2}=\\left(1-\\frac{2\\lambda_{1}}{\\lambda_{1}+\\lambda_{n}}\\right)^{2t}=\\left(\\frac{\\lambda_{n}-\\lambda_{1}}{\\lambda_{1}+\\lambda_{n}}\\right)^{2t}=\\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^{2t},$$ which leads to a slightly better convergence rate compared to gradient descent.\nPolynomial Minimax Evaluation We have essentially arrived at the minimax problem of polynomial evaluation: $$\\min_{q(X)\\in \\mathcal{Q}_{t}}\\max_{\\lambda\\in [\\lambda_{1},\\lambda_{n}]}\\left|q(\\lambda)\\right|^{2}.$$ The solution to a more general version of this problem is given by the Chebyshev polynomials, a central result in approximation theory.\nDefinition For an integer $n\\ge 1$ the $n$-th Chebyshev polynomial $T_{n}(x)$ is defined recursively to be a degree $n$ polynomial as follows: $$T_{0}(x)=1,\\quad T_{1}(x)=x,\\quad T_{n}(x)=2x\\cdot T_{n-1}(x)-T_{n-2}(x).$$\nProposition It holds that $T_{n}(\\cos \\theta)=\\cos (n \\theta)$. In particular, $T_{n}(x)\\in [-1,1]$ for every $x\\in [-1,1]$.\nThe minimization property of the Chebyshev polynomials is:\nTheorem For any integer $n\\ge 1$, the normalized $n$-th Chebyshev polynomial $f(x)=\\frac{1}{2^{n-1}}T_{n}(x)$ minimizes the term $$\\max_{x\\in [-1,1]}\\left|g(x)\\right|,$$ over all non-zero polynomials $g$ with degree at most $n$ and leading coefficient $1$. The maximal value for $f$ is $\\frac{1}{2^{n-1}}$.\nFor proof and background, see [6].\nThus, Chebyshev polynomials offer good candidates to obtain the upper bound for the convergence rate. In particular, note that minimizing the square magnitude is equivalent to minimizing the magnitude. All we have to deal with is mapping polynomials evaluated on $[\\lambda_{1},\\lambda_{n}]$ to $[-1,1]$. This is done easily by mapping an interval $$[a,b]\\to [-1,1]\\quad \\text{via}\\quad x\\mapsto \\frac{a+b-2x}{b-a}.$$ Note that $a\\mapsto \\frac{b-a}{b-a}=1$ while $b\\mapsto \\frac{a-b}{b-a}=-1$, and by linearity, every $a\u0026lt;x\u0026lt;b$ is mapped inside the interval. Define $$Q_{a,b,n}(x)=\\frac{T_{n}\\left(\\frac{a+b-2x}{b-a}\\right)}{T_{n}\\left(\\frac{a+b}{b-a}\\right)}.$$ It is a degree $n$ polynomial which satisfies $$Q_{a,b,n}(0)=\\frac{T_{n}(\\frac{a+b}{b-a})}{T_{n}(\\frac{a+b}{b-a})}=1.$$ Therefore $Q_{a,b,t} \\in \\mathcal{Q}_{t}$ (up to a sign flip, which does not affect magnitude). Using the fact the numerator has magnitude $\\le 1$ on the interval gives us the estimate $$|Q_{\\lambda_{1},\\lambda_{n},t}(x)|\\le \\frac{1}{T_{t}(\\frac{\\lambda_{1}+\\lambda_{n}}{\\lambda_{n}-\\lambda_{1}})}=T_{t}\\left(\\frac{\\lambda_{n}/\\lambda_{1}+1}{\\lambda_{n}/\\lambda_{1}-1}\\right)^{-1}.$$ By a few more manipulations, we see that $$T_{t}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)\\ge \\frac{1}{2}\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)^{t},$$ hence $$\\left|Q_{\\lambda_{1},\\lambda_{n},t}(\\lambda)\\right|\\le 2 \\cdot \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^{t}.$$ This proves the following theorem (to obtain the theorem, a few more algebraic steps are required, similar to the omitted proof of the GD theorem), which is essentially a quadratic improvement over gradient descent ($\\sqrt{\\kappa(A)}$ instead of $\\kappa(A)$):\nTheorem The Conjugate Gradient algorithm achieves error $$\\left\\lVert x_{t}-A^{-1}b \\right\\rVert_{A}\\le \\varepsilon \\left\\lVert A^{-1}b \\right\\rVert_{A}$$ after $t=O(\\sqrt{\\kappa(A)}\\cdot \\log (1/\\varepsilon))$ iterations.\nClustered Eigenvalues Consider the following scenario: $\\lambda_{n}$ is very large, $\\lambda_{1}$ is very small, and most of the eigenvalues lie concentrated in the interval $[a,b]$ where $a$ and $b$ are moderately sized. The condition number $\\lambda_n/\\lambda_1$ is enormous, so the convergence bound derived above may be poor. However, it turns out that by a nice algebraic trick, the number of iterations required turns out to be small.\nLemma If all but $c$ eigenvalues lie in are smaller than $b$, then after $t=\\Omega(\\sqrt{b/\\lambda_1}\\cdot \\log (1/\\varepsilon))$ iterations the error is bounded by $\\varepsilon$ (plus a small constant overhead for the outliers).\nProof. Let $a=\\lambda_1$. Using the polynomials $Q_{a,b,t}$ we can define a composite polynomial: $$q(x)=Q_{a,b,t}(x)\\cdot \\prod_{i=1}^{c} \\left(1 - \\frac{x}{\\mu_i}\\right)$$ where $\\mu_{1},\\ldots,\\mu_{c}$ are the \u0026ldquo;outlier\u0026rdquo; eigenvalues outside of $[a,b]$. Note that $q(0)=1 \\cdot \\prod 1 = 1$, so $q \\in \\mathcal{Q}_{t+c}$. Furthermore:\nFor any outlier eigenvalue $\\mu_i$, the term $(1 - \\mu_i/\\mu_i) = 0$, so $q(\\mu_i)=0$. For any $\\lambda \\in [a,b]$, the term $|1 - \\lambda/\\mu_i| \\le 1$. The term $|Q_{a,b,t}(\\lambda)|$ is bounded by the Chebyshev decay rate determined by the ratio $b/a$ (the \u0026ldquo;effective\u0026rdquo; condition number). Thus, the error of the algorithm is at most $\\max_{j}\\left|q(\\lambda_{j})\\right| \\le \\max_{\\lambda\\in [a,b]}\\left|Q_{a,b,t}(\\lambda)\\right|$. Setting $t=O(\\sqrt{b/a}\\cdot \\log(1/\\varepsilon))$, we obtain $\\varepsilon$-error. In particular, $q(x)$ has degree $t+c$, so we need $t$ iterations for the bulk spectrum plus $c$ iterations to \u0026ldquo;kill\u0026rdquo; the outliers. $\\blacksquare$\nThis lemma can be extended to outliers outside of $[a,b]$ for general $a$ (i.e., $a\u0026gt;\\lambda_1$). In this case one has to deal with the fact $(1-x/\\mu)$ may be large in magnitude for small $\\mu$. This might add a logarithmic (in the condition number) number of iterations.\nReferences Hestenes, M. R., \u0026amp; Stiefel, E. (1952), Methods of Conjugate Gradients for Solving Linear Systems . Shewchuk, J. R. (1994), An Introduction to the Conjugate Gradient Method Without the Agonizing Pain . Trefethen, L. N., \u0026amp; Bau III, D. (1997), Numerical Linear Algebra . Nocedal, J., \u0026amp; Wright, S. (2006), Numerical Optimization . Vishnoi, N.K. (2013), $Lx=b$ Laplacian Solvers and Their Algorithmic Applications . Mudde, M.H. (2017), Chebyshev approximation . ","title":"The Conjugate Gradient Method for Linear Equations"},{"link":"/posts/matroid-theory/","text":"Basic Matroid Theory and Infinite Extensions Prerequisites:\nLinear Algebra: Vector spaces, basis, dimension, and linear independence. Graph Theory: Graphs, connected components, cycles, and spanning trees. Set Theory: Basic cardinality and Zorn\u0026rsquo;s Lemma (for the infinite section). In this post, we will discuss the basics of Matroid Theory. While often taught merely as a theoretical framework for Greedy Algorithms, matroids are a rich combinatorial structure in their own right. We will start with the finite foundations and then explore how these definitions behave (and break) when extended to the infinite case.\nMotivation Vector Spaces Many results in Linear Algebra rely on the highly convenient tool of a basis. In every introductory course, one learns that while a vector space may have many bases, they are all of the same size. This fact allows us to define the dimension of vector spaces.\nThe proof of this fact relies on a crucial observation:\nObservation Suppose $A,B$ are linearly independent sets with size $|A|\u0026lt;|B|$. Then there must be some vector $v\\in B$ which is not contained in the span of $A$.\nReasoning: If not, $B\\subseteq \\mathrm{Span}(A)$, which implies that the dimension of the space spanned by $B$ is at most $|A|$. Since $|A| \u0026lt; |B|$, this contradicts the linear independence of $B$. Thus, there must exist $v\\in B$ which is linearly independent from $A$, and we can add it to $A$ to form a larger independent set.\nGraphs Graph algorithms often rely on the existence of a Spanning Tree (or a spanning forest if the graph is disconnected). For a graph $G=(V,E)$ with $n$ vertices and $c$ connected components, any spanning forest must have exactly $n-c$ edges.\nWe can prove this using a similar observation:\nObservation Suppose $S,T \\subset E$ define two forests (acyclic subgraphs) with $|S|\u0026lt; |T|$. Then there must exist an edge $e\\in T$ such that adding $e$ to $S$ doesn\u0026rsquo;t close any cycle.\nReasoning: If no such edge existed, then the vertices touched by edges in $T$ ($V_{T}$) would be \u0026ldquo;covered\u0026rdquo; connectivity-wise by edges in $S$. Since $|S|\u0026lt;|T|$ and both are forests, a counting argument on connected components leads to a contradiction. We conclude that $e$ can be added to $S$ to obtain a bigger forest.\nNotice the pattern: in both cases, there is a fundamental property allowing us to \u0026ldquo;exchange\u0026rdquo; elements from a larger independent set into a smaller one.\nMatroids Matroids are the combinatorial structure that captures this exact notion of \u0026ldquo;independence\u0026rdquo;, abstracting away the specific details of vectors or edges.\nDefinition A matroid is a pair $\\mathcal{M}=\\langle S,\\mathcal{I} \\rangle$, where $S$ is the ground set and $\\mathcal{I}\\subset 2^{S}$ is the collection of independent sets, satisfying:\n$\\mathcal{I}\\neq\\emptyset$. Hereditary: If $A\\in \\mathcal{I}$, then every subset $A\u0026rsquo;\\subseteq A$ satisfies $A\u0026rsquo;\\in \\mathcal{I}$. Exchange: If $A,B\\in \\mathcal{I}$ and $|A|\u0026lt;|B| \u0026lt; \\infty$, then there exists $a\\in B\\setminus A$ such that $A\\cup \\lbrace a\\rbrace \\in \\mathcal{I}$. For now, we assume the ground set and independent sets are finite. We will discuss the infinite case later. First, let us prove that this structure forces all maximal independent sets to have the same size.\nLemma If $A,B\\in \\mathcal{I}$ are maximal (meaning there is no $C\\in \\mathcal{I}$ such that $A\\subsetneq C$), then $|A|=|B|$.\nProof. Suppose otherwise, i.e., that $|A|\u0026gt;|B|$ (we can switch roles if the opposite holds). By the Exchange property, there exists $a\\in A\\setminus B$ such that $B\\cup \\lbrace a\\rbrace \\in \\mathcal{I}$. This contradicts the maximality of $B$. $\\blacksquare$\nThis lemma implies that the following concept is well-defined:\nDefinition A basis of a finite matroid $\\mathcal{M}=\\langle S,\\mathcal{I} \\rangle$ is a maximal element in $\\mathcal{I}$. Every basis has the same size, allowing us to define the rank of $\\mathcal{M}$, denoted $r(\\mathcal{M})$, as the size of any basis.\nExamples The Vector Matroid: For a vector space $V$, let $S\\subset V$ be a finite set of vectors, and let $\\mathcal{I}$ be the collection of linearly independent subsets of $S$. The Graphic Matroid: For a graph $G=(V,E)$, let $S=E$ and define $\\mathcal{I}$ as the collection of edge sets that induce a forest (contain no cycles). Circuits Definition A dependent set in a matroid is a set $A\\subset S$ such that $A\\notin \\mathcal{I}$. A circuit is a minimal dependent set. A set $C$ is a circuit if it is dependent, but every proper subset $C\u0026rsquo;\\subsetneq C$ is independent.\nCircuits act as minimal proofs of dependence:\nIn the vector matroid, if $v_3 = v_1 + v_2$, then $\\lbrace v_1, v_2, v_3\\rbrace $ is a circuit. Note that unlike bases, circuits are not necessarily of the same size. A set $\\lbrace v, 2v\\rbrace $ is a circuit of size 2, while $\\lbrace v_1, v_2, v_1+v_2\\rbrace $ is a circuit of size 3. In the graphic matroid, a circuit corresponds exactly to a simple cycle. The Dual Matroid Definition Given a finite matroid $\\mathcal{M}=(S,\\mathcal{I})$, we define the dual matroid $\\mathcal{M}^{*}=(S,\\mathcal{I}^{*})$ by: $$A\\in \\mathcal{I}^{*} \\iff S\\setminus A \\text{ contains a basis of }\\mathcal{M}.$$\nWe use the prefix \u0026ldquo;co-\u0026rdquo; for dual terms: dual bases are cobases, dual independent sets are coindependent, and dual circuits are cocircuits. Since a basis $A$ of $\\mathcal{M}^{*}$ satisfies $S\\setminus A=B$ for a basis $B$ of $\\mathcal{M}$, the rank is given by $r(\\mathcal{M}^{*})=|S|-r(\\mathcal{M})$.\nLemma If $\\mathcal{M}$ is a finite matroid, then $\\mathcal{M}^{*}$ is also a matroid.\nProof. The fact that $\\mathcal{I}^{*}$ is non-empty and hereditary is straightforward. We focus on the Exchange Property. Let $A,B\\in \\mathcal{I}^{*}$ with $|A|\u0026gt;|B|$. We must show there is $a\\in A\\setminus B$ such that $B\\cup \\lbrace a\\rbrace \\in \\mathcal{I}^{*}$, i.e., that $S\\setminus (B\\cup \\lbrace a\\rbrace )$ contains a basis of $\\mathcal{M}$.\nLet $C,D$ be bases of $\\mathcal{M}$ such that $C\\subseteq S\\setminus B$ and $D\\subseteq S\\setminus A$. Note that $D\\setminus B\\in \\mathcal{I}$ (hereditary), so there exists a basis $E$ such that $D\\setminus B\\subseteq E$. We claim $A\\setminus B \\not\\subset E$. Assume the converse ($A\\setminus B\\subset E$). Then: $$|D|= |D\\cap B|+ |D\\setminus B| \\overset{D\\subseteq S\\setminus A}{\\le} |B\\setminus A|+ |D\\setminus B| \\overset{|A|\u0026gt;|B|}{\u0026lt;} |A\\setminus B|+ |D\\setminus B|\\le |E|$$ (The last inequality holds because $A \\cap D = \\emptyset$ and $A\\setminus B, D\\setminus B\\subseteq E$ by hypothesis). This implies $|D| \u0026lt; |E|$, contradicting that all bases have the same size.\nWe conclude $A\\setminus B \\not\\subset E$, hence there exists $a\\in (A\\setminus B) \\setminus E$. Thus $E \\subseteq S \\setminus (B \\cup \\lbrace a\\rbrace )$, meaning $B \\cup \\lbrace a\\rbrace \\in \\mathcal{I}^*$. $\\blacksquare$\nDual Graph Matroid Recall that in the graphic matroid, independent sets are forests and circuits are simple cycles. What is the dual?\nCoindependent sets are subsets $A\\subset E$ whose removal leaves a spanning forest (does not break connectivity). Cocircuits are minimal subsets $A\\subset E$ whose removal breaks connectivity in some connected component. If $G$ is connected, cocircuits are exactly the minimal cuts of the graph.\nExample: Consider the following graph: A possible basis (Spanning Tree) is: $$\\lbrace (4,3),(5,3),(3,0),(1,0),(2,0)\\rbrace $$ A possible cobasis (edges not in the tree) is: $$\\lbrace (5,4),(2,1)\\rbrace $$ Two possible cocircuits (minimal cuts) are:\n$\\lbrace (5,4),(4,3)\\rbrace $ (Isolating vertex 4) $\\lbrace (3,0)\\rbrace $ (A bridge) The Infinite Case In linear algebra, the standard definition of linear independence applies to finite sets. To handle infinite dimensions, we extend the definition:\nDefinition Let $V$ be a vector space. A set $S\\subset V$ is linearly independent if every finite subset $S\u0026rsquo;\\subset S$ is linearly independent.\nUsing Zorn\u0026rsquo;s lemma, one can show every vector space has a basis. However, this algebraic basis is often less useful than topological bases (like Hilbert bases) which allow for convergence arguments.\nWe can attempt to define infinite matroids similarly:\nDefinition (Infinite Matroid Attempt) Let $\\mathcal{M}=(S,\\mathcal{I})$. We require the standard matroid axioms, plus: $A\\in \\mathcal{I}$ if and only if every finite subset $A\u0026rsquo;\\subset A$ satisfies $A\u0026rsquo;\\in \\mathcal{I}$.\nA consequence of this definition is that circuits must be finite. If a circuit were infinite, every finite subset of it would be independent (by minimality), which would make the whole set independent by our definition—a contradiction.\nI turns out that this definition, while appealing for its simplicity, is not very useful, because certain concepts like that of the dual matroid fail to translate to the infinite case. We will show this with the next canonical example.\nThe Infinite Ladder Failure Let\u0026rsquo;s see where this definition breaks down. Consider the Ladder Graph: $$G=(V,E), \\quad V=\\mathbb{Z}\\times\\lbrace 0,1\\rbrace $$ $$E = E_1 \\cup E_2$$ Where $E_1 = \\lbrace \\lbrace (i,x),(i+1,x)\\rbrace \\mid i\\in\\mathbb{Z}, x\\in \\lbrace 0,1\\rbrace \\rbrace$ are the \u0026ldquo;rails\u0026rdquo; and $E_2 = \\lbrace \\lbrace (i,0),(i,1)\\rbrace \\mid i\\in\\mathbb{Z} \\rbrace $ are the \u0026ldquo;rungs\u0026rdquo;. The level (rung) is determined by $i\\in \\mathbb{Z}$ while the side (rail) is determined by $x\\in \\lbrace 0,1\\rbrace$.\nIndependent Set: $A = E_1 \\cup \\lbrace (0,0),(0,1)\\rbrace $. This is the set of all rails plus one rung. It contains no cycles, and any finite subset is a forest. Coindependent Set: $B = A^c = E_2 \\setminus \\lbrace (0,0),(0,1)\\rbrace $. This is the set of all rungs except one. Removing these leaves $A$ intact, so the graph remains connected. Now consider the set of all rungs, $E_2$. In the dual context, $E_2$ is a cocircuit. Why?\nIt is a cut: Removing all rungs disconnects the two rails. It is minimal: If we keep even one rung $e \\in E_2$, the graph is connected. The Problem: $E_2$ is a cocircuit, but it is infinite. We established that under our definition, circuits must be finite. Thus, the dual of this infinite matroid is not a matroid.\nFinding a definition of infinite matroids that preserves duality was a long-standing open problem, resolved by Bruhn et al., 2013.\nAlgorithmic Applications Matroids are intimately tied to Greedy Algorithms. An optimization algorithm is \u0026ldquo;greedy\u0026rdquo; if it constructs a solution step-by-step, making the locally optimal choice at each step with the hope of finding a global optimum.\nExample (Fractional Knapsack). Given items with values and weights $\\lbrace(v_{i},w_{i})\\rbrace _{i=1}^{N}$ and a capacity $W$, we want to maximize total value. We are allowed to take fractions of items.\nGreedy Strategy: Sort items by value-per-weight ratio. Take as much of the best item as possible, then the next, until the capacity $W$ is full.\nFor matroids, we can solve the following problem: Given a finite matroid $\\mathcal{M}=\\langle S,\\mathcal{I} \\rangle$ and weights $\\mu:S\\to \\mathbb{R}_{+}$, find a basis of maximal weight.\nGreedy Matroid Algorithm Sort $S$ in descending order: $\\mu(s_{1})\\ge \\mu(s_{2})\\ge\\ldots\\ge\\mu(s_{n})$. Initialize $A\\gets \\emptyset$. For $i=1,\\ldots,n$: If $A\\cup \\lbrace s_{i}\\rbrace \\in \\mathcal{I}$, update $A\\gets A\\cup \\lbrace s_{i}\\rbrace $. Return $A$. Proof of Optimality We first note that the algorithm returns a basis (if it returned a smaller independent set, we could extend it by the Exchange property, contradicting the logic of the loop).\nLemma The algorithm returns a basis of maximal weight.\nProof. Let $A$ be the solution returned by the algorithm and $B$ be an optimal basis with $\\mu(B) \u0026gt; \\mu(A)$. Write $A=(a_1, \\dots, a_r)$ and $B=(b_1, \\dots, b_r)$ sorted by weight. Since $\\mu(B) \u0026gt; \\mu(A)$, there must be an index $i$ such that $\\mu(b_i) \u0026gt; \\mu(a_i)$. Let $i$ be the minimal such index.\nConsider the element $b_i$. Since $i$ is minimal, for all $j \u0026lt; i$, we have $\\mu(a_j) \\ge \\mu(b_j) \\ge \\mu(b_i)$. Thus, when the algorithm considered $b_i$, it had already considered and selected $\\lbrace a_1, \\dots, a_{i-1}\\rbrace $ (or elements with even higher weights).\nBy the Exchange property, since $|\\lbrace b_1, \\dots, b_i\\rbrace | \u0026gt; |\\lbrace a_1, \\dots, a_{i-1}\\rbrace |$, we can add an element from the first set to the second. Specifically, we can add $b_i$ to $\\lbrace a_1, \\dots, a_{i-1}\\rbrace $ while maintaining independence (if $b_i$ was dependent on previous elements, it wouldn\u0026rsquo;t be in the independent set $\\lbrace b_1 \\dots b_i\\rbrace $).\nSince $b_i$ could have been added, and it has higher weight than $a_i$ (and subsequent elements), the greedy algorithm would have added it. This contradicts our assumption that the algorithm produced $A$. $\\blacksquare$\nRemark. The runtime is dominated by sorting ($\\Omega(n \\log n)$) and the $n$ calls to the Independence Oracle (checking $A \\cup \\lbrace s_i\\rbrace \\in \\mathcal{I}$).\nKruskal\u0026rsquo;s Algorithm Kruskal\u0026rsquo;s Algorithm for Minimum Spanning Trees is a classic manifestation of the Greedy Matroid Algorithm applied to the Graphic Matroid (sorting edges by weight). The independence oracle checks for cycles, which can be implemented efficiently using a Union-Find data structure.\nReferences Oxley, J. (2011). Matroid Theory . Bruhn, H., Diestel, R., Kriesell, M., Pendavingh, R., \u0026amp; Wollan, P. (2013). Axioms for infinite matroids . ","title":"Basic Matroid Theory"},{"link":"/posts/kernel-2/","text":" Prerequisites Analysis: Basic Fourier Analysis (transforms, exponentials). Probability: Concentration inequalities (Hoeffding), expectation, and Gaussian distributions. Kernel Methods: Familiarity with the basic kernel trick (see previous post). In the previous post, we introduced the idea of kernels as a way to lift a separation problem to a much larger space (potentially infinite-dimensional) while keeping the computation tractable via the \u0026ldquo;Kernel Trick.\u0026rdquo; We also mentioned that when the number of points in the dataset is very large—which is the case in most modern applications—the kernel method is less useful, as it requires computing and storing a huge $n \\times n$ matrix.\nThe idea of the Random Fourier Features (RFF) method is that, in some cases, there is a very clean approximation of the kernel function that works on average for a random sample. Due to the concentration of measure phenomenon, the number of samples needed to ensure a very good approximation is surprisingly low. The idea was first proposed in a seminal paper by Rahimi and Recht [1].\nStationary Kernels Definition A stationary or shift-invariant function over a vector space $X$ is a function $K:X\\times X\\to \\mathbb{R}$ for which $$K(x,y)=K(x+\\Delta,y+ \\Delta)$$ for every $\\Delta\\in X$. Equivalently, there exists a function $f:X\\to \\mathbb{R}$ such that $$K(x,y)=f(x-y),$$ meaning that the value of $K$ is determined solely by the difference of the inputs.\nNote that our definitions of kernels can be generalized to include complex-valued kernels. This will matter for the theorem below, though for applications we ultimately care about the real-valued case.\nWe call a function $f:\\mathbb{R}^{d}\\to \\mathbb{C}$ positive definite if the stationary kernel $K:\\mathbb{R}^{d}\\times \\mathbb{R}^{d}\\to \\mathbb{C}$ defined by $K(x,y)=f(x-y)$ is positive definite. In the complex case, this means that for every choi}_ce of points $x_1 ,\\ldots,x_n$ and scalars $c_1,\\ldots,c_n$, it holds: $$\\sum_{i=1}^n \\sum_{j=1}^n c_i \\cdot \\overline{c_j}\\cdot f(x_i-x_j)\\ge 0$$\nThe Fourier Transform Definition For a function $\\varphi:\\mathbb{R}^{d}\\to \\mathbb{C}$, satisfying $\\int_{\\mathbb{R}^{d}}^{}{|\\varphi(x)|}\\ \\mathrm{d}{x}\u0026lt;\\infty$, we define the Fourier transform as the function $\\widehat{\\varphi}:\\mathbb{R}^{d}\\to \\mathbb{C}$ given by $$\\widehat{\\varphi}(\\xi)=\\int_{\\mathbb{R}^{d}}^{}{\\varphi(x)\\cdot e^{-2 \\pi i\\cdot \\left\\langle x,\\xi \\right\\rangle}}\\ \\mathrm{d}{x}.$$ Here $e^{-2\\pi i \\cdot \\alpha}$ is the complex exponent, which by Euler\u0026rsquo;s formula is equal to $$\\forall \\alpha\\in \\mathbb{R}:\\quad e^{i \\alpha}=\\cos(\\alpha)+i\\sin(\\alpha)\\in \\mathbb{C}.$$\nThe space of functions satisfying $\\int_{\\mathbb{R}^{d}}^{}{|\\varphi(x)|}\\ \\mathrm{d}{x}\u0026lt;\\infty$ is denoted by $L^1(\\mathbb{R}^d)$. The Fourier transform of $f$ is always continuous and vanishes at infinity, meaning $\\hat{f}(\\xi)\\to 0$ when $\\| \\xi \\|\\to \\infty$. We denote the space of continuous and vanishing functions as $C_0(\\mathbb{R}^d)$.\nRecall that a continuous probability measure $\\mu$ on $\\mathbb{R}^{d}$ is defined by its probability density function $p:\\mathbb{R}^{d}\\to \\mathbb{R}_{+}$. The expected value of a random variable $Z:\\mathbb{R}^{d}\\to \\mathbb{C}$ with respect to $\\mu$ is simply: $$\\mathbb{E}[Z]=\\int_{\\mathbb{R}^{d}}^{}{Z(x)\\cdot p(x)}\\ \\mathrm{d}{x}=\\int_{\\mathbb{R}^{d}}^{}{Z(x)}\\ \\mathrm{d}{\\mu(x)}.$$ Therefore, integration against $\\mu$ is the same as integrating with the standard Lebesgue measure weighted by the density function. The Fourier transform of $\\mu$ is defined to be the Fourier transform of $p$: $$\\widehat{\\mu}(\\xi)=\\int_{\\mathbb{R}^{d}}^{}{p(x)\\cdot e^{-2\\pi i\\cdot \\left\\langle x,\\xi \\right\\rangle}}\\ \\mathrm{d}{x}=\\mathbb{E}_{x\\sim \\mu}[\\exp(-2\\pi i \\left\\langle x,\\xi \\right\\rangle)].$$\nThis can be defined also for non-continuous probability measures, using the expectation notation. Without getting too much into measure theory, note that if $\\mu$ is, say a discrete distribution, it has no probability density function. We can still write an integral against $\\mu$, but this is the Lebesgue Integral, and not the classic Riemann integral over $\\mathbb{R}^d$, and it might have a very different meaning (for example in the discrete case, it is a sum).\nBochner\u0026rsquo;s Theorem We are now ready to state the main theorem connecting kernels to probability distributions:\nTheorem: Bochner A continuous function $f:\\mathbb{R}^{d}\\to \\mathbb{C}$ is positive definite (with $f(0)=1$) if and only if there exists a probability distribution $\\mu$ on $\\mathbb{R}^{d}$ such that $$f(x)=\\int_{\\mathbb{R}^{d}}^{}{e^{-2\\pi i \\left\\langle x,\\xi \\right\\rangle}}\\ \\mathrm{d}{\\mu(\\xi)}.$$ In other words, positive definite functions are the Fourier transforms of density functions of probability measures.\nProof Sketch. ($\\Rightarrow$) This direction is complicated, so I\u0026rsquo;ll only mention the key steps. A full proof can be found in [2].\nEstablish that the map $\\widehat{g}\\mapsto \\int_{\\mathbb{R}^{d}}^{}{g\\cdot f}\\ \\mathrm{d}{x}$ is a continuous linear functional on the space $\\mathcal{F}(L^{1}(\\mathbb{R}^{d}))$, i.e., the image of absolutely integrable functions under the Fourier Transform. Use the Gelfand-Naimark theorem, which says the Fourier transform of $L^{1}(\\mathbb{R}^{d})$ is dense in $C_{0}(\\mathbb{R}^{d})$ (continuous functions vanishing at infinity). By density, we extend our functional to a continuous linear functional $\\Phi$ on all of $C_{0}(\\mathbb{R}^{d})$. Apply the Riesz-Markov Representation Theorem, which states that functionals on $C_{0}(\\mathbb{R}^{d})$ correspond to measures. Thus, there exists a measure $\\mu$ such that $\\Phi(h)=\\int_{\\mathbb{R}^{d}}^{}{h}\\ \\mathrm{d}{\\mu}$. This implies the integral representation of $f$, and further analysis shows $\\mu$ is a probability measure. ($\\Leftarrow$) This is the easier direction. Suppose $f(x)=\\int_{\\mathbb{R}^{d}}^{}{e^{-2\\pi i \\left\\langle x,\\xi \\right\\rangle}}\\ \\mathrm{d}{\\mu(\\xi)}$ for a probability measure $\\mu$. Let $x_{1},\\ldots,x_{n}\\in\\mathbb{R}^{d}$ and $c_{1},\\ldots,c_{n}\\in \\mathbb{C}$. We check positive definiteness: $$\\sum_{i,j=1}^{n}c_{i}\\overline{c_{j}}f(x_{i}-x_{j})=\\int_{\\mathbb{R}^{d}}^{}{\\sum_{i,j}c_{i}\\overline{c_{j}} \\cdot \\exp(-2\\pi i \\left\\langle x_{i}-x_{j},\\xi \\right\\rangle)}\\ \\mathrm{d}{\\mu(\\xi)}.$$ Using exponent properties $\\exp(-2\\pi i \\left\\langle x_{i}-x_{j},\\xi \\right\\rangle)=\\exp(-2\\pi i \\left\\langle x_{i},\\xi \\right\\rangle)\\cdot \\overline{\\exp(-2\\pi i \\left\\langle x_{j},\\xi \\right\\rangle)}$, we can rewrite the integral as: $$=\\int_{\\mathbb{R}^{d}}^{}{\\left(\\sum_{i}c_{i}e^{-2\\pi i \\left\\langle x_{i},\\xi \\right\\rangle}\\right)\\cdot \\overline{\\left(\\sum_{j}c_{j}e^{-2\\pi i \\left\\langle x_{j},\\xi \\right\\rangle}\\right)}}\\ \\mathrm{d}{\\mu(\\xi)}=\\int_{\\mathbb{R}^{d}}^{}{\\left|\\sum_{i}c_{i}e^{-2\\pi i \\left\\langle x_{i},\\xi \\right\\rangle}\\right|^{2}}\\ \\mathrm{d}{\\mu(\\xi)}.$$ We have a non-negative integrand integrated against a non-negative measure, so the result is non-negative. Thus, $f$ is positive definite. $\\blacksquare$\nRandom Fourier Features Suppose we have a stationary kernel $K:\\mathbb{R}^{d}\\times \\mathbb{R}^{d}\\to \\mathbb{R}$, defined by a function $f:\\mathbb{R}^{d}\\to \\mathbb{R}$. Without loss of generality, assume $f(0)=1$ and $f$ is continuous. By Bochner\u0026rsquo;s Theorem, there is a probability measure $\\mu$ such that $$f(\\Delta)=\\int_{\\mathbb{R}^{d}}^{}{e^{-2\\pi i \\left\\langle \\Delta,\\xi \\right\\rangle}}\\ \\mathrm{d}{\\mu(\\xi)}=\\mathbb{E}_{\\xi\\sim \\mu}[\\exp(-2\\pi i \\left\\langle \\Delta,\\xi \\right\\rangle)].$$ Writing $\\Delta=x-y$, we have $$K(x,y)=f(x-y)=\\mathbb{E}_{\\xi}[\\exp(-2\\pi i \\left\\langle x-y,\\xi \\right\\rangle)]=\\mathbb{E}_{\\xi}[e^{-2\\pi i \\left\\langle x,\\xi \\right\\rangle}\\cdot \\overline{e^{-2\\pi i \\left\\langle y,\\xi \\right\\rangle}}].$$ Thus, randomly drawing $\\xi\\sim \\mu$ gives an estimator that equals $K(x,y)$ on average. However, there are two problems with this naive approach:\nVariance: Drawing a single sample $\\xi$ will have high variance. Complex Numbers: The computation involves complex variables even though the kernel is real. We want a real-valued feature map. Using Only Real Numbers To deal with the second problem, we use the following trigonometric identity:\nLemma For any real numbers $\\alpha,\\beta\\in\\mathbb{R}$, it holds that $$\\mathbb{E}_{b}[\\cos(\\alpha+b)\\cos(\\beta+b)]=\\frac{1}{2}\\cos(\\alpha - \\beta),$$ where $b$ is uniformly drawn from $[0,2\\pi]$.\nProof. The product-to-sum formula gives $\\cos(\\varphi)\\cos(\\psi)=\\frac{1}{2}[\\cos(\\varphi+\\psi)+\\cos(\\varphi-\\psi)]$. Applying this to $\\varphi=\\alpha+b$ and $\\psi=\\beta+b$: $$2\\cos(\\alpha+b)\\cdot \\cos(\\beta+b)=\\cos((\\alpha+b)+(\\beta+b))+\\cos((\\alpha+b)-(\\beta+b)).$$ The second term is $\\cos(\\alpha-\\beta)$, which is independent of $b$. The first term satisfies: $$\\mathbb{E}_{b}[\\cos(\\alpha+\\beta+2b)]=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}{\\cos(\\alpha+\\beta+2b)}\\ \\mathrm{d}{b}=\\frac{1}{4\\pi}\\left[\\sin(\\alpha+\\beta+2b)\\right]_{0}^{2\\pi}=0.$$ Therefore, $\\mathbb{E}_{b}[\\cos(\\alpha+b)\\cos (\\beta+b)]=\\frac{1}{2}\\cos(\\alpha-\\beta)$. $\\blacksquare$\nCorollary Let $z(x)=\\sqrt{2}\\cos(-2\\pi\\left\\langle x,\\xi \\right\\rangle+b)$ for $\\xi\\sim \\mu$ and $b\\sim [0,2\\pi]$ drawn uniformly. Then $$\\mathbb{E}_{\\xi,b}[z(x)\\cdot z_{}(y)]=K(x,y).$$\nProof. Since $K$ is real-valued, we have $$K(x,y)=\\Re(K(x,y))=\\Re\\left(\\int_{\\mathbb{R}^{d}}^{}{e^{-2\\pi i \\left\\langle x-y,\\xi \\right\\rangle}}\\ \\mathrm{d}{\\mu(\\xi)}\\right)=\\int_{\\mathbb{R}^{d}}^{}{\\cos(-2\\pi \\left\\langle x-y,\\xi \\right\\rangle)}\\ \\mathrm{d}{\\mu(\\xi)}.$$ By the previous lemma, setting $\\alpha=-2\\pi \\left\\langle x,\\xi \\right\\rangle$ and $\\beta=-2\\pi \\left\\langle y,\\xi \\right\\rangle$, we have $$\\mathbb{E}_{b}[z(x)\\cdot z(y)]=\\mathbb{E}_{b}[2\\cos(\\alpha+b)\\cos (\\beta+b)]=\\cos(\\alpha-\\beta)=\\cos (-2\\pi \\left\\langle x-y,\\xi \\right\\rangle).$$ Taking the expectation over $\\xi$, we obtain $\\mathbb{E}_{\\xi,b}[z(x)\\cdot z(y)]=K(x,y)$. $\\blacksquare$\nReducing the Variance Recall that in randomized algorithms, Monte-Carlo algorithms run a fixed number of trials and return an answer that is correct with high probability. Concentration inequalities, like Hoeffding\u0026rsquo;s, Bernstein\u0026rsquo;s, and Chebyshev\u0026rsquo;s, formally answer the question: how likely is a random variable to attain a value far from its mean?\nThe special property of Hoeffding-type inequalities is that for sums of independent random variables, the concentration is exponentially strong. In other words, if a random variable $Y$ is the sum of a number of independent random variables $X_1,\\ldots,X_n$, then $Y$ has very strong concentration properties.\nTheorem: Hoeffding\u0026rsquo;s Inequality Given $X_{1},\\ldots,X_{n}$ independent and identically distributed (i.i.d.) random variables satisfying $\\left|X- \\mathbb{E}[X]\\right|\\le c$ almost surely for some $c\u0026gt;0$, it holds: $$\\Pr\\left(\\left|\\sum_{i=1}^{n}X_{i}-n\\cdot \\mathbb{E}[X]\\right|\\ge a\\right)\\le 2\\exp\\left(\\frac{-a^{2}}{2nc^{2}}\\right),$$ for any $a\u0026gt;0$.\nThis suggests the following Monte-Carlo estimate: $$E(x,y)=\\frac{1}{R}\\sum_{i=1}^{R}z_{i}(x)z_{i}(y),$$ where $z_{i}(x)=\\sqrt{2}\\cos(-2\\pi \\left\\langle x, \\xi_{i} \\right\\rangle +b_{i})$ with $\\xi_{i}\\sim \\mu$ and $b_{i}\\sim [0,2\\pi]$ drawn independently. Let us formalize the quality of this estimator:\nClaim For any $x,y\\in\\mathbb{R}^{d}$ and $\\varepsilon\u0026gt;0$ we have $$\\Pr(\\left|E(x,y)-K(x,y)\\right|\\ge \\varepsilon)\\le 2\\exp(-R \\varepsilon^{2}/8).$$\nProof. First, $\\mathbb{E}[E(x,y)]=K(x,y)$ by linearity of expectation. Second, $E$ is the sum of $R$ i.i.d. random variables $X_{i}=\\frac{1}{R}z_{i}(x)z_{i}(y)$. Note that $z_{i}(x)$ is bounded in $[-\\sqrt{2},\\sqrt{2}]$ because $\\cos$ takes values in $[-1,1]$. Therefore, $|X_{i}|\\le\\frac{2}{R}$ almost surely, which implies the mean is also in this range. Hence $\\left|X_{i}-\\frac{K(x,y)}{R}\\right|\\le \\frac{2}{R}$, and by Hoeffding\u0026rsquo;s inequality, $$\\begin{aligned} \\Pr(\\left| E(x,y)-k(x,y) \\right|)\u0026amp;=\\Pr\\left(\\left|\\sum_{i=1}^{R}X_{i}-R\\cdot \\frac{k(x,y)}{R}\\right| \\ge \\varepsilon\\right) \\\\ \u0026amp; \\le 2\\exp\\left(-\\frac{\\varepsilon^{2}}{2R(2/R)^{2}}\\right)=2\\exp(-\\varepsilon^{2}R/8). \\end{aligned}$$ $\\blacksquare$\nThis result can be upgraded to a uniform bound over a compact subset of $\\mathbb{R}^{d}$. This means the samples $\\xi_{1},\\ldots,\\xi_{R}$ and $b_{1},\\ldots,b_{R}$ can be fixed once for the entire dataset and still approximate the kernel well for all pairs of points simultaneously.\nRecap and Example This procedure produces a random map: $$\\mathbf{z}:\\mathbb{R}^{d}\\to \\mathbb{R}^{R},\\quad \\mathbf{z}(x)=\\frac{1}{\\sqrt{R}}(z_{1}(x),\\ldots,z_{R}(x)).$$ The features in $\\mathbb{R}^{d}$ are replaced by random features in $\\mathbb{R}^{R}$ that capture the same kernel interactions, meaning $$\\left\\langle \\mathbf{z}(x),\\mathbf{z}(y) \\right\\rangle\\approx K(x,y).$$\nRecall the example of the Radial Basis Function kernel from the previous post: $$K(x,y)=\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\lVert x-y \\right\\rVert^{2}\\right).$$ This is a stationary kernel defined by the function $f(x)=\\exp(-\\frac{1}{2\\sigma^{2}}\\left\\lVert x \\right\\rVert^{2})$. To determine the distribution $p(\\xi)$ to sample from, we need to satisfy Bochner\u0026rsquo;s relation: $$f(x) = \\int_{\\mathbb{R}^d} p(\\xi) e^{-2\\pi i \\langle x, \\xi \\rangle} d\\xi = \\hat{p}(x).$$ By the Fourier Inversion Theorem, $p$ must be the inverse Fourier transform of $f$. However, since the Gaussian function $f(x)$ is symmetric ($f(x)=f(-x)$), its inverse transform is identical to its forward transform. Thus, we can compute $p = \\hat{f}$. The Fourier inversion can be applied because we are dealing with Gaussians, which are smooth and rapidly decaying functions (in the Schwartz space).\nWe need the following standard identity, asserting that the Fourier transform of a Gaussian is a Gaussian.\nLemma For $a\u0026gt;0$, the Fourier transform of the Gaussian function $g(x)=e^{-a \\left\\Vert x\\right\\Vert^{2}}$ is: $$\\hat{g}(\\xi ) = \\left(\\frac{\\pi}{a}\\right)^{d/2}\\exp\\left(-\\frac{\\pi^2\\left\\lVert \\xi \\right\\rVert^{2}}{a}\\right).$$\nIn our case, $f(x) = e^{-a|x|^2}$ where $a = \\frac{1}{2\\sigma^2}$. Plugging this into the Lemma: $$\\begin{aligned} p(\\xi) = \\hat{f}(\\xi) \u0026amp;= \\left( \\frac{\\pi}{1/(2\\sigma^2)} \\right)^{d/2} \\exp\\left( -\\frac{\\pi^2 |\\xi|^2}{1/(2\\sigma^2)} \\right) \\ \u0026amp;= (2\\pi \\sigma^2)^{d/2} \\exp\\left( -2\\pi^2 \\sigma^2 |\\xi|^2 \\right). \\end{aligned}$$\nThis looks like the probability density function of another Gaussian. Suppose it has variance $\\tau^2$, then it satisfes $$\\frac{1}{2\\tau^2} = 2\\pi^2 \\sigma^2 \\implies \\tau = \\frac{1}{2\\pi \\sigma}.$$\nThus, to approximate the Gaussian kernel with width $\\sigma$, we must sample frequencies from: $$\\xi \\sim \\mathcal{N}\\left(0, \\frac{1}{(2\\pi \\sigma)^2} I\\right).$$\nLet\u0026rsquo;s see this in action. In the next plot we have $10000$ points sampled from two circles. The linear model doesn\u0026rsquo;t work, and the full RBF kernel perfectly separates the data. However, the RBF kernel does computation based on the number of points, which is huge. Next we have Random Fourier features with $R=10,50,100,1000$ and we can see that even for $50$ the separation is almost perfect! This is a huge reduction in the runtime complexity. The diagrams show the decision boundaries of each classifier. In the next post, we\u0026rsquo;ll discuss another randomized algorithm that approximates a specific family of kernels, called Polynomial kernels.\nReferences Rahimi, A., \u0026amp; Recht, B. (2007), Random Features for Large-Scale Kernel Machines . Bell, J. (2015), Gaussian measures and Bochner\u0026rsquo;s theorem . ","title":"Random Fourier Features"},{"link":"/posts/kernel-3/","text":" Prerequisites Linear Algebra: Inner products, tensor products, and vectorization. Probability: Hash functions, independence, and variance. Algorithms: Fast Fourier Transform (FFT) and basic convolution. Kernel Methods: Familiarity with the Polynomial Kernel. In previous posts, we discussed the Radial Basis Function (RBF) kernel and how to approximate it using Random Fourier Features. Today, we turn our attention to another fundamental kernel—the Polynomial Kernel—and a powerful algebraic technique to approximate it called Tensor Sketching.\nThe Polynomial Kernel For $x,y\\in \\mathbb{R}^{d}$ and $c\\ge 0$, the polynomial kernel of degree $p$ is defined as: $$K(x,y)=(\\left\\langle x,y \\right\\rangle+c)^{p}=\\left(\\sum_{i=1}^{d}x_{i}y_{i}+c\\right)^{p}.$$ If $c=0$, the kernel is called Homogeneous.\nBy applying the binomial formula, we can expand the kernel as: $$K(x,y)=\\sum_{j=0}^{p}\\binom{p}{j}\\cdot(\\left\\langle x,y \\right\\rangle)^{j}c^{p-j}.$$ Further applying the multinomial formula to the term $\\left\\langle x,y \\right\\rangle^{j}$: $$\\left\\langle x,y \\right\\rangle^{j}=\\sum_{n_{1}+\\ldots+n_{d}=j}\\binom{j}{n_{1},\\ldots,n_{d}} \\cdot \\prod_{i=1}^{d}(x_{i}y_{i})^{n_{i}}.$$ Note that for $n_{1}+\\ldots+n_{d}=j$, we have the combinatorial identity: $$\\binom{p}{j}\\cdot \\binom{j}{n_{1},\\ldots,n_{d}}=\\frac{p!}{(p-j)!n_{1}!\\cdots n_{d}!}=\\binom{p}{n_{1},\\ldots,n_{d},p-j}.$$ Letting $n_{d+1}=p-j$, we have $n_{1}+\\ldots+n_{d+1}=p$. Since we sum over all $j$, we can rewrite the kernel as a summation over all non-negative integer partitions of $p$ into $d+1$ parts: $$K(x,y)=\\sum_{n_{1}+\\ldots+n_{d+1}=p}\\binom{p}{n_{1},\\ldots,n_{d+1}}\\prod_{i=1}^{d}(x_{i}y_{i})^{n_{i}}\\cdot c^{n_{d+1}}.$$ The total number of summands is the number of solutions to $n_{1}+\\ldots+n_{d+1}=p$, which is $\\binom{p+d}{d}$.\nWe can explicitly construct a feature map $\\varphi:\\mathbb{R}^{d}\\to \\mathbb{R}^{\\binom{p+d}{d}}$. Enumerate all solutions to the partition equation, and for the $k$-th solution $(n_{1},\\ldots,n_{d+1})$, define the $k$-th feature: $$\\psi_{k}(x)=\\sqrt{\\binom{p}{n_{1},\\ldots,n_{d+1}}}\\cdot x_{1}^{n_{1}}\\cdots x_{d}^{n_{d}}\\cdot c^{n_{d+1}/2}.$$ Defining $\\varphi(x)=(\\psi_{1}(x),\\ldots,\\psi_{\\binom{p+d}{d}}(x))$, we satisfy: $$\\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle = K(x,y).$$ While $\\varphi$ maps to a finite-dimensional space, the dimension $\\binom{p+d}{d} \\approx p^d$ grows exponentially with $d$. For high-dimensional data, computing explicit features is intractable.\nNote: The polynomial kernel is not stationary. For example, if $p=2$ and $c=0$, $K(x,x) = |x|^4$. If we take two vectors with different norms, $K(x,x) \\neq K(y,y)$, even if $x-x=y-y=0$. Therefore, the Random Fourier Features method (which relies on Bochner\u0026rsquo;s theorem for shift-invariant kernels) is not applicable.\nNote: we can always reduce the general case to the homogeneous case by appending a coordinate with value $\\sqrt{c}$ to each vector. Thus, we will focus on the homogeneous case where $K(x,y) = \\langle x, y \\rangle^p$.\nAs Tensor Products Definition The tensor product (or Kronecker product) of two vectors $a\\in\\mathbb{R}^{n},b\\in\\mathbb{R}^{m}$ is the vector $a \\otimes b\\in\\mathbb{R}^{n\\cdot m}$, whose $(i,j)$-th coordinate is $a_{i}b_{j}$ (using double indexing).\nFor a vector $a$, let $a^{(p)}$ denote the $p$-th tensor power: $$a^{(p)} = \\underbrace{a \\otimes \\ldots \\otimes a}_{p \\text{ times}} \\in \\mathbb{R}^{d^p}.$$\nLemma It holds that $\\left\\langle x^{(p)},y^{(p)} \\right\\rangle=\\left\\langle x,y \\right\\rangle^{p}$.\nProof. We proceed by induction on $p$.\nBase case: $p=1$ is trivial ($\\langle x, y \\rangle = \\langle x, y \\rangle$). Step: Suppose the claim holds for $p$. Let $a=x^{(p)}$ and $b=y^{(p)}$. Then: $$\\left\\langle x\\otimes a,y\\otimes b \\right\\rangle=\\sum_{i\\in [d],j\\in [d^p]}(x\\otimes a)_{(i,j)}\\cdot (y\\otimes b)_{(i,j)}=\\sum_{i,j}x_{i}y_{i}a_{j}b_{j}=\\left(\\sum_{i}x_{i}y_{i}\\right)\\cdot \\left(\\sum_{j}a_{j}b_{j}\\right).$$ This equals $\\langle x,y \\rangle \\cdot \\langle a,b \\rangle$. By the induction hypothesis, $\\langle a,b \\rangle = \\langle x,y \\rangle^p$, so the result is $\\langle x,y \\rangle^{p+1}$. $\\blacksquare$ This suggests that if we can approximate $x^{(p)}$ and $y^{(p)}$ with low-dimensional vectors, we can approximate the polynomial kernel efficiently.\nSketching Linear sketches are random linear dimension-reducing maps that preserve the norm of vectors with high probability. The most famous examples include the Johnson-Lindenstrauss Transform and the Count-Sketch. We will focus on the Count-Sketch because of its unique interaction with tensor products.\nHash Functions Recall the definition of $k$-wise independent hash families.\nDefinition A set $\\mathcal{H}$ of functions $[d]\\to [R]$ is a $k$-wise independent family if for every distinct set of indices $I\\subset [d]$ of size $|I| \\le k$, and any vector of values $\\mathbf{v}\\in [R]^{|I|}$, it holds: $$\\Pr_{f\\in \\mathcal{H}}\\left((f(i))_{i\\in I}=\\mathbf{v}\\right)=\\frac{1}{R^{|I|}}.$$ In simple terms, any $k$ inputs are mapped to independent uniform random outputs.\nNote that if $\\mathcal{H}$ is $k$-wise independent it is also $k\u0026rsquo;$-wise independent for $k\u0026rsquo;\u0026lt;k$.\nCount Sketch Definition: Count Sketch Given a hash function $h:[d]\\to [R]$ drawn from a $2$-wise independent family, and a sign function $s:[d]\\to \\set{\\pm 1}$ drawn from a $4$-wise independent family, define the Count-Sketch with respect to $h,s$ to be the matrix in $\\mathbb{R}^{R\\times d}$ denoted by $C$, defined by: $$C_{k,i}:=\\begin{cases}s(i) \u0026amp; h(i)=k, \\\\ 0 \u0026amp; \\text{else},\\end{cases}$$ for every $i\\in [d]$ and $k\\in [R]$. Note that every column $i$ has a single non-zero element, at row $h(i)$. Explicitly, given a vector $x\\in\\mathbb{R}^{d}$, its Count-Sketch is: $$(Cx)_{k}=\\sum_{i:h(i)=k}s(i)\\cdot x_{i},$$ for every $k\\in [R]$.\nOne way to think of count-sketch is as a random binning operator. We send each element in the vector to a different bin, with a total of $R$ bins, while multiplying it by a random sign. Note that since $C$ has at $d$ non-zero values, computing $Cx$ can be done in $O(d)$ operations.\nAn important property of Count-Sketch is that it preserves inner products in expectation, and the variance is controllable:\nLemma Let $C$ be a random Count-Sketch matrix. Then for any $x,y\\in\\mathbb{R}^{d}$: $$\\mathbb{E}[\\left\\langle Cx,Cy \\right\\rangle]=\\left\\langle x,y \\right\\rangle, \\quad \\text{and} \\quad \\mathbf{Var}(\\left\\langle Cx,Cy \\right\\rangle)\\le \\frac{2}{R}\\left\\lVert x \\right\\rVert^{2} \\left\\lVert y \\right\\rVert^{2}.$$\nProof. The inner product of the sketches is: $$\\left\\langle Cx,Cy \\right\\rangle=\\sum_{k=1}^{R}(Cx)_{k}(Cy)_{k}=\\sum_{k=1}^{R}\\sum_{i,j:h(i)=h(j)=k} s(i)s(j)x_{i}y_{j}.$$ Taking the expectation over the sign functions $s$:\nIf $i \\neq j$, $\\mathbb{E}[s(i)s(j)] = \\mathbb{E}[s(i)]\\mathbb{E}[s(j)] = 0$. If $i = j$, $s(i)^2 = 1$, implying $\\mathbb{E}[s(i)^2]=1$. Thus, the only surviving terms are where $i=j$: $$\\mathbb{E}[\\left\\langle Cx,Cy \\right\\rangle] = \\sum_{k=1}^R \\sum_{i:h(i)=k} x_i y_i = \\sum_{i=1}^d x_i y_i = \\langle x, y \\rangle.$$ Note that since $\\sum_k \\mathbb{1}_{h(i)=k} = 1$ always, the randomness of $h$ does not affect the mean.\nFor the variance, let $\\delta_{i,j}$ be the indicator that $h(i)=h(j)$. Note $$\\begin{aligned} \\left\\langle Cx,Cy \\right\\rangle\u0026amp;=\\sum_{i,j}\\delta_{i,j}s(i)s(j)\\cdot x_{i}y_{j} \\\\ \u0026amp;=\\sum_{i}\\delta_{i,i}s(i)^{2} x_{i}y_{i}+\\sum_{i\\not=j}\\delta_{i,j}s(i)s(j)x_{i}y_{j} \\\\ \u0026amp;=\\left\\langle x,y \\right\\rangle+\\sum_{i\\not=j}\\delta_{i,j}s(i)s(j)x_{i}y_{j} \\end{aligned}$$ Thus: $$\\left\\langle Cx,Cy \\right\\rangle^2 = \\left( \\langle x,y \\rangle + \\sum_{i \\neq j} \\delta_{i,j} s(i)s(j) x_i y_j \\right)^2.$$ The cross-terms ($\\langle x,y\\rangle\\cdot \\sum_{i\\not=j}\\delta_{i,j}s(i)s(j)x_i y_j$) vanish in expectation due to $\\mathbb{E}[s(i)]=0$. The expectation of the square of the sum involves terms like $\\delta_{i,j}\\delta_{i\u0026rsquo;,j\u0026rsquo;}s(i)s(j)s(i\u0026rsquo;)s(j\u0026rsquo;)$. Due to 4-wise independence of $s$, the expectation is non-zero only if the indices ${i,j,i\u0026rsquo;,j\u0026rsquo;}$ match in pairs. Specifically, either $(i,j)=(i\u0026rsquo;,j\u0026rsquo;)$ or $(i,j)=(j\u0026rsquo;,i\u0026rsquo;)$. Using $\\mathbb{E}[\\delta_{i,j}] = 1/R$ (for $i \\neq j$), we obtain $$\\mathbb{E}\\left[\\left(\\sum_{i\\not=j}\\delta_{i,j}s(i)s(j)x_{i}y_{j}\\right)^{2}\\right]= \\frac{1}{R}\\sum_{i\\not=j}(x_{i}^{2}y_{j}^{2}+x_{i}y_{i}x_{j}y_{j})$$ Note that $$\\sum_{i\\not=j}x_{i}^{2}y_{j}^{2}\\le \\sum_{i,j}x_{i}^{2}y_{j}^{2}=\\left(\\sum_{i}x_{i}^{2}\\right)\\left(\\sum_{j}y_{j}^{2}\\right)= \\left\\lVert x \\right\\rVert^{2}\\cdot \\left\\lVert y \\right\\rVert^{2},$$and by Cauchy-Schwarz $$\\sum_{i\\not=j}x_{i}y_{i}x_{j}y_{j}\\le \\sum_{i,j}x_{i}y_{i}x_{j}y_{j}=\\left(\\sum_{i}x_{i}y_{i}\\right)^{2}=\\left\\langle x,y \\right\\rangle^{2}\\le \\left\\lVert x \\right\\rVert^{2}\\cdot \\left\\lVert y \\right\\rVert^{2},$$ and so we conclude that $$\\mathbf{Var}(\\left\\langle Cx,Cy \\right\\rangle) = \\mathbb{E}[\\langle Cx, Cy\\rangle^2]- \\mathbb{E}[\\langle Cx,Cy\\rangle]^2= \\frac{1}{R}\\sum_{i\\not=j}(x_i ^2 y_j^2 + x_iy_i x_j y_j)\\le \\frac{2}{R}\\left\\lVert x \\right\\rVert^{2}\\left\\lVert y \\right\\rVert^{2}$$ $\\blacksquare$\nTensor Sketch Suppose we have a $2$-wise independent family $[d]\\to [R]$. How can we construct a $2$-wise family $[d^{2}]\\to [R]$?\nOne way is to take $h_{1},h_{2}:[d]\\to [R]$ (sampled randomly) and define $$H(i_{1},i_{2})=h_{1}(i_{1})+ h_{2}(i_{2})\\pmod{R}.$$ The function $H$ sampled this way is $2$-wise independent too. For sign functions, we can take $$S(i_{1},i_{2})=s_{1}(i_{1})\\cdot s_{2}(i_{2}),$$ giving a new $2$-wise independent sign function on $[d^{2}]$. Of course we can take $h_{1},h_{2}$ to be from different families and universe sizes, say $h_1:[d_1]\\to [R],h_2:[d_2]\\to [R]$ and obtain $H:[d_1 d_2]\\to [R]$.\nThe main observation is the following lemma.\nLemma Let $h_{1},h_{2}:[d]\\to [R]$ be drawn from a $2$-wise independent family and $s_{1},s_{2}$ also drawn from a $2$-wise independent family of sign functions. Define $H=h_{1}+h_{2}\\pmod{R}$ and $S=s_{1}\\cdot s_{2}$ and let $C$ denote the Count-Sketch with respect to $H,S$, sketching vectors of size $d^{2}$. Moreover, let $C^{1},C^{2}$ denote the Count-Sketch with respect to $h_{1},s_{1}$ and $h_{2},s_{2}$. Then for all $x,y\\in\\mathbb{R}^{d}$, it holds $$C(x \\otimes y)=C^{1}x* C^{2}y,$$ where $*$ denotes $R$-cyclic convolution.\nLet us define cyclic convolution:\nDefinition Given two vectors $u,v\\in\\mathbb{R}^m$, the $m$-cyclic convolution denoted $u*v\\in\\mathbb{R}^{m}$ is given by $$(u*v)_{k}=\\sum_{i=1}^{m}u_{i}\\cdot v_{(k-i)\\mod m},$$ where we interpret $v_{0}=v_{m}$. We can also write it as $$(u*v)_{k}=\\sum_{i,j:i+j\\equiv k\\pmod{m}} u_{i}\\cdot v_{j}.$$\nProof. Let $\\equiv$ denote congruence modulo $R$. For $k\\in [R]$ we have $$\\begin{aligned}(C(x \\otimes y))_{k} \u0026amp;= \\sum_{i_{1},i_{2}:H(i_{1},i_{2})=k}S(i_{1},i_{2})\\cdot (x \\otimes y)_{i_{1}i_{2}} \\\\ \u0026amp;=\\sum_{i_{1},i_{2}:h_{1}(i_{1})+h_{2}(i_{2})\\equiv k}s_{1}(i_{1})s_{2}(i_{2})x_{i_{1}}y_{i_{2}} \\\\ \u0026amp;= \\sum_{j_{1},j_{2}\\in [R]: j_{1}+j_{2}\\equiv k}\\sum_{i_{1},i_{2}:h_{1}(i_{1})=j_{1},h_{2}(i_{2})=j_{2}}(s_{1}(i_{1})x_{i_{1}})\\cdot (s_{2}(i_{2})y_{i_{2}}) \\\\ \u0026amp;= \\sum_{j_{1},j_{2}\\in [R]: j_{1}+j_{2}\\equiv k}\\left(\\sum_{i:h_{1}(i)=j_{1}} s_{1}(i)x_{i} \\right)\\cdot \\left(\\sum_{i: h_{2}(i)=j_{2}}s_{2}(i)y_{i}\\right) \\\\ \u0026amp;= \\sum_{j_{1},j_{2}\\in[R]:j_{1}+j_{2}\\equiv k}(C^{1}x)_{j_{1}}\\cdot (C^{2}y)_{j_{2}} =(C^{1}x * C^{2}y)_{k}.\\end{aligned}$$ $\\blacksquare$\nWe thus obtain the corollary:\nCorollary Suppose $C$ is the count-sketch for dimension $d^{p}$ with respect to $$H(i_{1},\\ldots,i_{p})=h_{1}(i_{1})+\\ldots + h_{p}(i_{p})\\pmod{R},$$ and $S(i_{1},\\ldots,i_{p})=s_{1}(i_{1})\\cdots s_{p}(i_{p})$, where all the hash functions are drawn independently from respective $2$-wise independent families. Let $C^{j}$ be the count-sketch for dimension $d$ with respect to $h_{j},s_{j}$. Then $$C(x_{1}\\otimes \\ldots \\otimes x_{p})=C^{1}x_{1}* \\ldots * C^{p}x_{p}.$$ In particular $C(x^{(p)})$ is the convolution of $p$-different count sketches of $x$.\nComputing Convolutions The nice thing about cyclic convolution is that it can be computed fast via the fast Fourier transform.\nTheorem: The Convolution Theorem The $m$-cyclic convolution of two vectors $u,v$ is given by $$u*v=\\mathrm{DFT}^{-1}(\\mathrm{DFT}(u)\\odot \\mathrm{DFT}(v)),$$ where $\\odot$ denotes element-wise multiplication of vectors.\nBy induction, we see that (note convolution is associative) $$u*v*w=(u*v)*w=\\mathrm{DFT}^{-1}(\\mathrm{DFT}(u*v)\\odot \\mathrm{DFT}(w))=\\mathrm{DFT}^{-1}(\\mathrm{DFT}(u)\\odot \\mathrm{DFT}(v)\\odot \\mathrm{DFT}(w))$$ and in general to compute the convolution of $p$ vectors, we need to compute the DFT of each vector, take the element-wise products, and compute a single inverse DFT.\nTheorem: The FFT Theorem The DFT of size $m$ can be computed in time $O(m\\log m)$, and so can the inverse DFT.\nTherefore computing the convolution of $p$ vectors requires time $O(pm\\log m)$.\nThus, to compute $Cx^{(p)}$, one needs to:\ndraw $p$ different hash functions $h_{1},\\ldots,h_{p}$ and sign functions $s_{1},\\ldots,s_{p}$, compute $p$ different count sketches $C^{1}x,\\ldots,C^{p}x$, compute the DFT (of size $R$) for each sketch, denoted $\\widehat{C^{1}x},\\ldots,\\widehat{C^{p}x}$, compute the element-wise product $u=\\widehat{C^{1}x}\\odot \\ldots\\odot \\widehat{C^{p}x}$, return the inverse DFT of $u$. Recap To conclude, the tensor sketch produces a random feature vector for an input $x\\in\\mathbb{R}^{d}$, in $\\mathbb{R}^{R}$, which is given as the count-sketch of $x^{(p)}$. Computing this random feature vector requires $$O(p(d+R\\log R)),$$ since we have to perform $p$ sketches and compute the convolution of $p$ vectors of size $R$. Using the statistical properties of count-sketch, we have $$\\mathbb{E}\\left[\\left\\langle Cx^{(p)},Cy^{(p)} \\right\\rangle\\right]=\\left\\langle x^{(p)},y^{(p)} \\right\\rangle=\\left\\langle x,y \\right\\rangle^{p}=K(x,y),$$ with considerably good variance (which by Chebyshev\u0026rsquo;s inequality implies decent concentration around the mean).\nThis is much better than computing the inner product of $x^{(p)}$ and $y^{(p)}$ in time $O(d^{p})$. The ideas behind tensor sketch and count-sketch are relevant to many other fields, especially numerical linear algebra, dimensionality reduction and even neural network acceleration.\nReferences Pagh, R. (2013), Compressed Matrix Multiplication . Pham, N., \u0026amp; Pagh, R. (2013), Fast and Scalable Polynomial Kernels via Explicit Feature Maps . Ahle, T. D., et al. (2020), Oblivious Sketching of High-Degree Polynomial Kernels . Charikar, M., et al. (2002), Finding Frequent Items in Data Streams . Woodruff, D. P. (2014), Sketching as a Tool for Numerical Linear Algebra . ","title":"Tensor Sketch: Polynomial Kernels"},{"link":"/posts/kernel-1/","text":" Prerequisites Linear Algebra: Inner product spaces, positive definite matrices, spectral decomposition, projections. In this post, we will explore the idea of kernels in machine learning. In future posts, we will explore different ways to approximate specific kernel computations. Approximation is useful for big data applications due to the prohibitively high cost of exact kernel computations.\nHigh Level Idea Suppose we wish to find a linear separation rule $$f(x)=w^{\\top }x$$ that minimizes some objective function. Since many datasets are not linearly separable (or even close to being so), a natural idea is to embed the dataset in a higher dimensional vector space in a specific way that ensures better separation. A classic example is separating circular data, as depicted below:\nWe map $\\varphi:\\mathbb{R}^{2}\\to \\mathbb{R}^{3}$ by setting $\\varphi(x,y)=(x,y,\\sqrt{x^{2}+y^{2}})$. Under this map, the datasets can be linearly separated by a plane.\nDefinition The Kernel Method is taking a map $\\varphi:X\\to \\mathcal{H}$, where $X$ is the vector space in which the original dataset resides, and $\\mathcal{H}$ is some high-dimensional Hilbert space. The separation is then done in $\\mathcal{H}$, i.e., by finding a vector $h\\in \\mathcal{H}$ for which the function $$f(x)=\\left\\langle h,\\varphi(x) \\right\\rangle_\\mathcal{H}$$ minimizes some objective function.\nRemark. For those unfamiliar with Hilbert spaces, these are just vector spaces with an inner product that are also complete (every Cauchy sequence converges). Finite-dimensional spaces are always Hilbert spaces, but we can also choose to work with infinite-dimensional spaces.\nThere are multiple problems we must solve for this approach to be useful:\nFinding $h\\in \\mathcal{H}$ might be much harder than finding $w\\in X$, since $\\mathcal{H}$ might have a larger dimension (potentially infinite). Computing $f(x)$ requires computing an inner product in $\\mathcal{H}$, which is not necessarily efficient (or even computable). Representer Theorem To solve problem (1)—finding $h$—we will observe that under certain natural assumptions regarding the objective function, $h$ takes on a simple form. This reduces the problem to a finite-dimensional case.\nTheorem: Representer Theorem Let $\\varphi:X\\to \\mathcal{H}$ be a feature map, and suppose we are given a dataset $\\set{(x_{i},y_{i})}_{i=1}^{n}$ (where $x_{i}\\in X$). Let $F$ be an objective function depending on the predicted values, and let $R:\\mathbb{R}_{\\ge 0}\\to \\mathbb{R}$ be a monotone non-decreasing regularizer.\nThe optimal vector $h^{*}\\in \\mathcal{H}$ which minimizes the objective $$h^{*}=\\arg\\min_{h\\in \\mathcal{H}}\\set{F(\\left\\langle h,\\varphi(x_{1}) \\right\\rangle_{\\mathcal{H}},\\ldots , \\left\\langle h,\\varphi(x_{n}) \\right\\rangle_{\\mathcal{H}})+R(\\left\\lVert h \\right\\rVert^{2})},$$ can be written as a finite linear combination of the data points: $$h^{*}=\\sum_{i=1}^{n}\\alpha_{i}\\cdot \\varphi(x_{i})$$ for some $\\alpha\\in\\mathbb{R}^{n}$.\nProof. Let $h^{*}$ be an optimizer of the objective. Let $U = \\mathrm{Span}(\\set{\\varphi(x_{i})}_{i=1}^n)$. We can decompose $h^{*}$ as $h^{*} = h_{1} + h_{2}$, where $h_{1} = P_U(h^{*})$ is the orthogonal projection onto $U$, and $h_{2} = h^{*} - h_{1}$. Note that such a projection exists because $\\mathcal{H}$ is a Hilbert space and $U$ is a finite-dimensional subspace (hence closed).\nBy definition, $h_{2}\\perp U$, which implies $$\\forall i=1,\\ldots ,n:\\quad\\left\\langle h_{2},\\varphi(x_{i}) \\right\\rangle=0.$$ Consequently, $\\left\\langle h^{*},\\varphi(x_{i}) \\right\\rangle = \\left\\langle h_{1},\\varphi(x_{i}) \\right\\rangle$. The value of the loss function $F$ remains unchanged if we replace $h^{*}$ with $h_{1}$. However, by the Pythagorean theorem: $$\\left\\lVert h^{*} \\right\\rVert^{2}=\\left\\lVert h_{1} \\right\\rVert^{2}+ \\left\\lVert h_{2} \\right\\rVert^{2}.$$ Since $R$ is non-decreasing, $$R(\\left\\lVert h^{*} \\right\\rVert^{2}) \\ge R(\\left\\lVert h_{1} \\right\\rVert^{2}).$$ Therefore, $h_{1}\\in U$ achieves a loss less than or equal to $h^{*}$. Without loss of generality, we can choose the optimizer from $U$. (If $R$ is strictly monotone and $h_2 \\neq 0$, $h^*$ would be strictly suboptimal, forcing $h^* \\in U$). Since $U$ is spanned by $\\varphi(x_{1}),\\ldots,\\varphi(x_{n})$, the claim follows. $\\blacksquare$\nThe Representer Theorem also hints towards a solution to problem (2). The separation function becomes $$f(x) = \\left\\langle h,\\varphi(x) \\right\\rangle_{\\mathcal{H}}=\\sum_{i=1}^{n}\\alpha_{i} \\left\\langle \\varphi(x_{i}),\\varphi(x) \\right\\rangle_{\\mathcal{H}}.$$ This means that in order to compute the separation rule, it suffices to find a function $k:X\\times X\\to \\mathbb{R}$ such that $k(x,x\u0026rsquo;)=\\left\\langle \\varphi(x),\\varphi(x\u0026rsquo;) \\right\\rangle_{\\mathcal{H}}$.\nPositive-Definite Kernels Remark: Kernels as Integral Transforms Kernel functions in mathematics are often used to define integral transforms, which take in functions and produce new functions (functional operators). The general form is to integrate against the kernel function, so given a function $f$ on some space $(X,\\mu)$ where $\\mu$ is the measure, we define a new function $Tf$ by $Tf(x)=\\int_{X}f(y)K(x,y) \\ dy$. The most well-known example is the Fourier transform, where the kernel is $K(x,\\xi)=e^{-ix\\cdot \\xi}$.\nAnother well known example hiding in plain sight is the following. Consider the space $([n],\\mu_{\\mathrm{Count}})$, where the counting measure assigns each element $x\\in [n]$ with \u0026ldquo;weight\u0026rdquo; $1$. Functions from $[n]$ to $\\mathbb{C}$ are just vectors (the $k$-th coordinate is the value at $k$). So an integral transforms takes a vector $f=(f_1,\\ldots,f_n)$ and outputs a new vector $Tf$, given by $$Tf(i)=\\int_{[n]}{f(j)\\cdot K(i,j)}\\ \\mathrm{d}{\\mu_{\\mathrm{Count}}(j)}=\\sum_{j=1}^{n}f(j)K(i,j)=(\\mathbf{K}f)_{i}.$$ Here $\\mathbf{K}$ is simply the matrix $(K(i,j))_{i,j}$. So kernel based integral transforms are just matrix-vector multiplication.\nDefinition A Positive Definite (PD) kernel is a real-valued kernel function $K:X^{2}\\to \\mathbb{R}$ such that for every $n\\in\\mathbb{N}$, choice of $n$ points $x_{1},\\ldots,x_{n}\\in X$, and $n$ scalars $\\gamma_{1},\\ldots,\\gamma_{n}\\in\\mathbb{R}$, it holds: $$\\sum_{i,j=1}^{n}\\gamma_{i}\\gamma_{j}K(x_{i},x_{j})\\ge 0.$$ In other words, for every choice of $n$ points, the Gram matrix $\\mathbf{K}_{i,j}=K(x_{i},x_{j})$ is positive semi-definite (PSD).\nRecall that a matrix is called positive-semi-definite if it is symmetric and all of its eigenvalues are non-negative.\nIf there exists a map $\\varphi:X\\to \\mathcal{H}$ such that $K(x,y)=\\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle_\\mathcal{H}$, then $K$ is clearly symmetric (because real inner products are symmetric) and positive definite, because: $$\\sum_{i,j}\\gamma_{i}\\gamma_{j}\\left\\langle \\varphi(x_{i}),\\varphi(x_{j}) \\right\\rangle=\\left\\langle \\sum_{i}\\gamma_{i}\\varphi(x_{i}),\\sum_{j}\\gamma_{j}\\varphi(x_{j}) \\right\\rangle=\\left\\lVert \\sum_{i}\\gamma_{i}\\varphi(x_{i}) \\right\\rVert^{2}\\ge 0.$$ Thus being positive definite is necessary for a kernel $K$ to describe some inner product.\nMoore–Aronszajn Theorem It turns out that being positive definite is also sufficient.\nTheorem: Moore–Aronszajn Let $K:X\\times X\\to \\mathbb{R}$ be symmetric and positive definite. Then there exists a Hilbert space $\\mathcal{H}$ and a map $\\varphi:X\\to \\mathcal{H}$ (called the Feature Map) such that $$K(x,y)=\\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle_\\mathcal{H}$$ for every $x,y\\in X$.\nProof. Note that $K$ can be used to define functionals $X\\to \\mathbb{R}$, simply by fixing one of the coordinates. For $x\\in X$, define $f_{x}:X\\to \\mathbb{R}$ by $f_{x}(y)=K(x,y)$. Define $$\\mathcal{H}_{0}=\\mathrm{Span}_{\\mathbb{R}}\\set{f_{x}:x\\in X}.$$In other words, $\\mathcal{H}_{0}$ is a vector space whose elements are finite linear combinations of functionals of the form $f_{x}$. We can endow this vector space with an inner product, by defining $$\\left\\langle \\sum_{i=1}^{n}a_{i}f_{x_{i}}\\ ,\\ \\sum_{j=1}^{m}b_{j}f_{y_{j}} \\right\\rangle:=\\sum_{i=1}^{n}\\sum_{j=1}^{m}a_{i}b_{j} K(x_{i},y_{j}).$$Since $K$ is symmetric and positive definite, it is easy to see that the above is indeed an inner product (i.e., satisfies symmetry, positive-definiteness, bi-linearity). From here, all that remains is to take the completion of $\\mathcal{H}_{0}$, which we denote by $\\mathcal{H}$. Here we use a fundamental theorem, that every inner product space has a (unique) completion. Moreover, $\\mathcal{H}_{0}$ embeds into $\\mathcal{H}$, via an isometric embedding. In other words, we can think of $\\mathcal{H}_{0}$ as a subset $\\subset \\mathcal{H}$, and for $F,G\\in \\mathcal{H}_{0}$ it holds $\\left\\langle F,G \\right\\rangle_{\\mathcal{H}_{0}}=\\left\\langle F,G \\right\\rangle_{\\mathcal{H}}$. The proof is complete by taking $$\\varphi:X\\to \\mathcal{H}_{0}\\subset \\mathcal{H}\\quad ,\\quad \\varphi(x)=f_{x},$$and noting that under our definition of the inner product, it holds $$\\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle_{\\mathcal{H}}=\\left\\langle f_{x},f_{y} \\right\\rangle_{\\mathcal{H}_{0}}=K(x,y).$$ $\\blacksquare$\nPutting Everything Together Fix a feature map $\\varphi:X\\to \\mathcal{H}$ implicitly by choosing its corresponding Kernel $K:X^{2}\\to \\mathbb{R}$. Compute the kernel matrix for the training data: $$\\forall i,j\\in [n]:\\quad \\mathbf{K}_{i,j}=K(x_{i},x_{j}).$$ Use a quadratic solver to find $\\alpha^{*}$ (based on the specific loss $F$ and regularizer $R$): $$\\alpha^{*}=\\arg\\min_{\\alpha\\in\\mathbb{R}^{n}}\\set{F(\\mathbf{K}\\alpha)+ R(\\alpha^{\\top}\\mathbf{K}\\alpha)}.$$ The reason a quadratic solver is needed, is because of the expression $\\alpha^{\\top}\\mathbf{K}\\alpha$. Obtain the separation rule for a new point $x$: $$f(x)=\\sum_{i=1}^{n}\\alpha^{*}_{i}K(x_{i}, x).$$ Example: The Radial Basis Function Kernel Definition Define the vector space $\\ell^{2}(\\mathbb{N})$, as the set of all real-valued sequences $(a_{n})_{n=0}^{\\infty}$ which are absolutely square summable, meaning $$\\sum_{n=0}^{\\infty} |a_n|^2 \u0026lt; \\infty$$ Addition and scalar multiplication is done element-by-element (meaning $(a_n)_{n=0}^{\\infty} + (b_n)_{n=0}^{\\infty}= (a_n+ b_n)_{n=0}^{\\infty}$ and $\\lambda \\cdot (a_n)_{n=0}^{\\infty} =(\\lambda\\cdot a_n)_{n=0}^{\\infty}$). This is an infinite-dimensional Hilbert space, with the inner product $$\\langle (a_n)_{n=0}^{\\infty} , (b_n)_{n=0}^{\\infty} \\rangle := \\sum_{n=0}^{\\infty} a_n \\cdot b_n.$$\nIn the next section, we\u0026rsquo;ll construct a feature map $\\varphi: \\mathbb{R}^d\\to \\ell^2(\\mathbb{N})$ such that $$\\langle \\varphi(x),\\varphi(y)\\rangle = \\exp(-(1/2\\sigma^2)\\| x-y\\|^2)$$ This kernel function is called the Gaussian Kernel (note $\\sigma\u0026gt;0$).\nFor this we must first fix an enumeration:\nFor every $j\\in \\mathbb{N}$, there are a finite ways to partition $j$ to $d$ natural numbers. In other words, the number of non-negative integral solutions to the equation $$n_1 +\\ldots + n_d =j$$ is finite. In particular, it is the binomial coefficient $\\binom{j+d-1}{d-1}$. Thus, for every $j\\in \\mathbb{N}$, we can fix an order (enumeration) on the set $$S_j=\\left\\lbrace(n_1,\\ldots,n_d): n_i\\ge 0, \\sum_{i=1}^d n_i=j\\right\\rbrace$$ Define the enumeration $$\\mathbb{N}\\to (S_0 ,S_1 , S_2 ,\\ldots )$$ over all partitions of natural numbers to $d$ natural numbers. For $k\\in \\mathbb{N}$, let $(n_1(k),\\ldots,n_d(k))$ denote the $k$-th partition in the order, summing up to $j(k)$. Define (we drop the $k$-notation here): $$\\psi_{k}(x)=\\underbrace{\\frac{x_{1}^{n_{1}}\\cdots x_{d}^{n_{d}}}{\\sigma^{j}\\cdot \\sqrt{n_{1}!\\cdots n_{d}!}}\\exp\\left(\\frac{-1}{2\\sigma^{2}}\\left\\Vert x \\right\\Vert^{2}\\right)}_{\\beta_{k}}.$$ Define the full feature map $\\varphi(x) = (\\psi_0(x),\\psi_1(x), \\psi_2(x), \\dots)$. Now, note: $$\\begin{aligned} \\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle \u0026amp;= \\sum_{k=0}^{\\infty} \\psi_k(x)\\psi_k(y) \\\\ \u0026amp;= \\sum_{j=0}^{\\infty}\\sum_{(n_{1},\\ldots,n_{d})\\in S_j}\\frac{x_{1}^{n_{1}}\\cdots x_{d}^{n_{d}}y_{1}^{n_{1}}\\cdots y_{d}^{n_{d}}}{\\sigma^{2j}\\cdot n_{1}!\\cdots n_{d}!}\\exp\\left(\\frac{-1}{2\\sigma^{2}}(\\left\\lVert x \\right\\rVert^{2}+\\left\\lVert y \\right\\rVert^{2})\\right) \\end{aligned}$$\nBy the multinomial formula we know that: $$(x^{\\top}y)^j=\\left(\\sum_{i=1}^n x_i y_i \\right)^j= \\sum_{(n_1,\\ldots,n_d)\\in S_j} \\binom{j}{n_{1},\\ldots,n_{d}}(x_{1}y_{1})^{n_{1}}\\cdots (x_{d}y_{d})^{n_{d}}$$\nNote that $$\\frac{x_{1}^{n_{1}}\\cdots x_{d}^{n_{d}}y_{1}^{n_{1}}\\cdots y_{d}^{n_{d}}}{\\sigma^{2j}\\cdot n_{1}!\\cdots n_{d}!}=\\frac{1}{\\sigma^{2j}}\\cdot \\frac{1}{j!}\\binom{j}{n_1,\\ldots,n_d}\\cdot (x_1 y_1)^{n_1}\\cdots (x_dy_d)^{n_d}$$ Therefore $$\\begin{aligned} \\left\\langle \\varphi(x),\\varphi(y) \\right\\rangle \u0026amp;= \\exp\\left(\\frac{-1}{2\\sigma^{2}}(\\left\\lVert x \\right\\rVert^{2}+\\left\\lVert y \\right\\rVert^{2})\\right)\\cdot \\sum_{j=0}^{\\infty}\\frac{1}{j!}\\cdot \\frac{1}{\\sigma^{2j}} (x^{\\top}y)^j \\\\ \u0026amp;= \\exp\\left(\\frac{-1}{2\\sigma^{2}}(\\left\\lVert x \\right\\rVert^{2}+\\left\\lVert y \\right\\rVert^{2})\\right)\\cdot \\exp\\left(\\frac{1}{\\sigma^{2}}x^{\\top}y\\right)\\\\ \u0026amp;= \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\left( \\left\\lVert x \\right\\rVert^{2} + \\left\\lVert y \\right\\rVert^{2} - 2x^\\top y \\right) \\right) \\\\ \u0026amp;= \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\left\\lVert x-y \\right\\rVert^{2}\\right) \\end{aligned}$$ Where we used the identity $$\\| x-y\\|^2= (x-y)^{\\top}(x-y)=x^{\\top}x + y^{\\top}y -x^{\\top}y - y^{\\top}x=\\| x\\|^2 +\\| y\\|^2 -2x^{\\top}y$$\nThis gives us the RBF (Radial Basis Function) kernel, also known as the Gaussian kernel: $$K(x,y)=\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\lVert x-y \\right\\rVert^{2}\\right).$$ Note that $K(x,y)$ depends only on the distance $x-y$, making it a stationary kernel.\nConclusion The kernel method allows us to perform regression and optimization in a much \u0026ldquo;richer\u0026rdquo; and more \u0026ldquo;expressive\u0026rdquo; space, while keeping the computation tractable. That said, using the Kernel method generally requires computing and storing the $n \\times n$ Gram matrix $\\mathbf{K}$, and when $n$ is very large—typical in big data applications—this becomes prohibitive ($O(n^2)$ space, $O(n^3)$ solve time).\nThis requires other techniques, where the goal is to approximate the kernel computation, or find more computational shortcuts. These often use randomness and ideas from numerical linear algebra or analysis. In the next post we will see one such method, which works for certain types of kernels.\nReferences Schölkopf, B., \u0026amp; Smola, A. J. (2002), Learning with Kernels . Shaham, U. (2025). Reproducing Kernel Hilbert Spaces . Berlinet, A., \u0026amp; Thomas-Agnan, C. (2004), Reproducing Kernel Hilbert Spaces in Probability and Statistics . ","title":"The Kernel Method"}],"tags":[{"link":"/tags/algebra/","name":"Algebra","slug":"Algebra"},{"link":"/tags/algebraic-algorithms/","name":"Algebraic-Algorithms","slug":"Algebraic-Algorithms"},{"link":"/tags/algorithms/","name":"Algorithms","slug":"Algorithms"},{"link":"/tags/analysis/","name":"Analysis","slug":"Analysis"},{"link":"/tags/fft/","name":"Fft","slug":"Fft"},{"link":"/tags/gradient-descent/","name":"Gradient-Descent","slug":"Gradient-Descent"},{"link":"/tags/linear-algebra/","name":"Linear-Algebra","slug":"Linear-Algebra"},{"link":"/tags/linear-equations/","name":"Linear-Equations","slug":"Linear-Equations"},{"link":"/tags/machine-learning/","name":"Machine-Learning","slug":"Machine-Learning"},{"link":"/tags/math/","name":"Math","slug":"Math"},{"link":"/tags/matrix-multiplication/","name":"Matrix-Multiplication","slug":"Matrix-Multiplication"},{"link":"/tags/matroids/","name":"Matroids","slug":"Matroids"},{"link":"/tags/optimization/","name":"Optimization","slug":"Optimization"},{"link":"/tags/semi-definite-programming/","name":"Semi-Definite-Programming","slug":"Semi-Definite-Programming"},{"link":"/tags/tensors/","name":"Tensors","slug":"Tensors"},{"link":"/tags/theory/","name":"Theory","slug":"Theory"}]}